#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CCUSçŸ¥è¯†å›¾è°±å¯è§†åŒ–æ–‡ä»¶ç”Ÿæˆè„šæœ¬
å°†UIEæŠ½å–çš„CCUSå…³ç³»æ•°æ®è½¬æ¢ä¸ºå‰ç«¯å¯è§†åŒ–æ ¼å¼
"""

import json
import os
from collections import defaultdict

def load_ccus_data(file_path):
    """åŠ è½½CCUSçŸ¥è¯†å›¾è°±æ•°æ®"""
    data = []
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    data.append(json.loads(line))
    return data

def extract_entities_and_relations(ccus_data):
    """ä»CCUSæ•°æ®ä¸­æå–å®ä½“å’Œå…³ç³»"""
    entities = {}  # entity_name -> {id, category, frequency}
    relations = []  # {source, target, relation_type, sentence_id}
    entity_sentences = defaultdict(list)  # entity -> [sentence_ids]

    entity_id = 0

    # å®šä¹‰å®ä½“ç±»å‹åˆ°ç±»åˆ«IDçš„æ˜ å°„
    entity_type_mapping = {
        "CCUSæŠ€æœ¯": 0,
        "è¡Œä¸š": 1,
        "åœ°åŒº": 2,
        "æ”¿ç­–": 3,
        "é¡¹ç›®": 4,
        "æˆæœ¬": 5,
        "æ•ˆç›Š": 6,
        "é£é™©": 7,
        "æ ‡å‡†": 8,
        "å…¶ä»–": 9
    }

    for item in ccus_data:
        sent_id = item.get('id', 0)
        sent_text = item.get('sentText', '')
        relation_mentions = item.get('relationMentions', [])

        for relation_mention in relation_mentions:
            for entity_type, entities_list in relation_mention.items():
                if not isinstance(entities_list, list):
                    continue

                for entity_data in entities_list:
                    if not isinstance(entity_data, dict):
                        continue

                    # æå–ä¸»å®ä½“
                    entity_text = entity_data.get('text', '').strip()
                    if not entity_text or len(entity_text) < 2:
                        continue

                    # æ·»åŠ ä¸»å®ä½“
                    if entity_text not in entities:
                        entities[entity_text] = {
                            'id': entity_id,
                            'name': entity_text,
                            'category': entity_type_mapping.get(entity_type, 9),
                            'frequency': 1,
                            'symbolSize': 10
                        }
                        entity_id += 1
                    else:
                        entities[entity_text]['frequency'] += 1

                    entity_sentences[entity_text].append(sent_id)

                    # æå–å…³ç³»
                    relations_dict = entity_data.get('relations', {})
                    for rel_type, rel_entities in relations_dict.items():
                        if not isinstance(rel_entities, list):
                            continue

                        for rel_entity_data in rel_entities:
                            if not isinstance(rel_entity_data, dict):
                                continue

                            rel_entity_text = rel_entity_data.get('text', '').strip()
                            if not rel_entity_text or len(rel_entity_text) < 2:
                                continue

                            # æ·»åŠ å…³ç³»å®ä½“
                            if rel_entity_text not in entities:
                                entities[rel_entity_text] = {
                                    'id': entity_id,
                                    'name': rel_entity_text,
                                    'category': 9,  # é»˜è®¤ç±»åˆ«
                                    'frequency': 1,
                                    'symbolSize': 8
                                }
                                entity_id += 1
                            else:
                                entities[rel_entity_text]['frequency'] += 1

                            entity_sentences[rel_entity_text].append(sent_id)

                            # æ·»åŠ å…³ç³»
                            relations.append({
                                'source': entities[entity_text]['id'],
                                'target': entities[rel_entity_text]['id'],
                                'name': rel_type,
                                'sent': sent_id
                            })

    return entities, relations, entity_sentences

def create_visualization_data(entities, relations, entity_sentences, ccus_data):
    """åˆ›å»ºå¯è§†åŒ–æ•°æ®æ ¼å¼"""

    # åˆ›å»ºå¥å­æ–‡æœ¬æ˜ å°„
    sentences = {}
    for item in ccus_data:
        sentences[item.get('id', 0)] = item.get('sentText', '')

    # è½¬æ¢å®ä½“ä¸ºèŠ‚ç‚¹æ ¼å¼
    nodes = []
    for entity_name, entity_info in entities.items():
        # è®¡ç®—èŠ‚ç‚¹å¤§å°ï¼ˆåŸºäºé¢‘ç‡ï¼‰
        symbol_size = max(8, min(50, entity_info['frequency'] * 3))

        node = {
            'id': entity_info['id'],
            'name': entity_name,
            'category': entity_info['category'],
            'symbolSize': symbol_size,
            'label': {
                'show': symbol_size > 20
            },
            'lines': entity_sentences[entity_name][:10]  # æœ€å¤šä¿ç•™10ä¸ªå¥å­ID
        }
        nodes.append(node)

    # åˆ›å»ºç±»åˆ«
    categories = [
        {'name': 'CCUSæŠ€æœ¯'},
        {'name': 'è¡Œä¸š'},
        {'name': 'åœ°åŒº'},
        {'name': 'æ”¿ç­–'},
        {'name': 'é¡¹ç›®'},
        {'name': 'æˆæœ¬'},
        {'name': 'æ•ˆç›Š'},
        {'name': 'é£é™©'},
        {'name': 'æ ‡å‡†'},
        {'name': 'å…¶ä»–'}
    ]

    # åˆ›å»ºå¯è§†åŒ–æ•°æ®ç»“æ„
    visualization_data = {
        'nodes': nodes,
        'links': relations,
        'categories': categories,
        'sents': sentences
    }

    return visualization_data

def main():
    """ä¸»å‡½æ•°"""
    input_file = "data/ccus_v1/base.json"
    output_file = "data/ccus_data.json"

    print("ğŸš€ å¼€å§‹ç”ŸæˆCCUSçŸ¥è¯†å›¾è°±å¯è§†åŒ–æ–‡ä»¶...")

    # åŠ è½½CCUSæ•°æ®
    ccus_data = load_ccus_data(input_file)
    if not ccus_data:
        print(f"âŒ æ— æ³•åŠ è½½CCUSæ•°æ®: {input_file}")
        return

    print(f"ğŸ“Š åŠ è½½äº† {len(ccus_data)} æ¡CCUSæ•°æ®è®°å½•")

    # æå–å®ä½“å’Œå…³ç³»
    entities, relations, entity_sentences = extract_entities_and_relations(ccus_data)

    print(f"ğŸ” æå–äº† {len(entities)} ä¸ªå®ä½“")
    print(f"ğŸ”— æå–äº† {len(relations)} ä¸ªå…³ç³»")

    # ç”Ÿæˆå¯è§†åŒ–æ•°æ®
    visualization_data = create_visualization_data(entities, relations, entity_sentences, ccus_data)

    # ä¿å­˜å¯è§†åŒ–æ•°æ®
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(visualization_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… CCUSçŸ¥è¯†å›¾è°±å¯è§†åŒ–æ–‡ä»¶å·²ç”Ÿæˆ: {output_file}")
    print(f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - èŠ‚ç‚¹æ•°é‡: {len(visualization_data['nodes'])}")
    print(f"   - è¾¹æ•°é‡: {len(visualization_data['links'])}")
    print(f"   - ç±»åˆ«æ•°é‡: {len(visualization_data['categories'])}")
    print(f"   - å¥å­æ•°é‡: {len(visualization_data['sents'])}")

    # æ˜¾ç¤ºå®ä½“åˆ†å¸ƒ
    category_counts = defaultdict(int)
    for node in visualization_data['nodes']:
        category_counts[visualization_data['categories'][node['category']]['name']] += 1

    print(f"\nğŸ“Š å®ä½“åˆ†å¸ƒ:")
    for category, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):
        print(f"   - {category}: {count} ä¸ª")

if __name__ == "__main__":
    main()