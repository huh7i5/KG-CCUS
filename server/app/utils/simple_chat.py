"""
ç®€å•çš„ChatGLM-6BåŠ è½½å™¨
ä½¿ç”¨æ›´å…¼å®¹çš„æ–¹å¼åŠ è½½æ¨¡å‹
"""

import os
import sys
import torch
from transformers import AutoTokenizer, AutoModel

class SimpleChatGLM:
    def __init__(self, model_path, memory_optimize=False):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.loaded = False
        self.memory_optimize = memory_optimize
        print(f"ğŸ”§ SimpleChatGLM initialized with memory_optimize={memory_optimize}")

    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        try:
            print("ğŸš€ Loading ChatGLM-6B model...")

            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                print(f"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")

            # ç›´æ¥ä½¿ç”¨åŸºç¡€åŠ è½½æ–¹æ³•ï¼Œé¿å…å¤æ‚çš„ä¿®å¤é€»è¾‘
            print("Using optimized loading method...")
            return self._optimized_load()

        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            print(f"Error type: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            return False

    def _basic_load(self):
        """åŸºç¡€åŠ è½½æ–¹æ³•ï¼ˆæœ€åçš„å¤‡é€‰ï¼‰"""
        try:
            print("ğŸ”„ Trying basic loading method...")

            # æ£€æŸ¥GPUå’Œå†…å­˜
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"Using device: {device}")

            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")

            # åŠ è½½tokenizer
            print("Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                use_fast=False  # é¿å…fast tokenizerçš„å…¼å®¹æ€§é—®é¢˜
            )

            # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ä½å†…å­˜é…ç½®ï¼‰
            print("Loading model with low memory configuration...")
            self.model = AutoModel.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                device_map="auto" if device == "cuda" else None,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                low_cpu_mem_usage=True,
                max_memory={0: "10GB"} if device == "cuda" else None  # é™åˆ¶GPUå†…å­˜ä½¿ç”¨
            )

            if device == "cuda":
                self.model = self.model.cuda()

            self.model.eval()
            self.loaded = True

            print("âœ… Basic loading successful!")
            return True

        except Exception as e:
            print(f"âŒ Basic loading also failed: {e}")
            return False

    def _optimized_load(self):
        """ä¼˜åŒ–çš„åŠ è½½æ–¹æ³•"""
        try:
            print("ğŸ”§ Maximum memory optimization loading mode...")

            # æ£€æŸ¥è®¾å¤‡
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"Using device: {device}")

            if not torch.cuda.is_available():
                print("âŒ CUDA not available, cannot load ChatGLM-6B")
                return False

            # è®¾ç½®æœ€æ¿€è¿›çš„å†…å­˜ä¼˜åŒ–å‚æ•°
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True,max_split_size_mb:128"
            torch.backends.cudnn.enabled = False  # ç¦ç”¨ cudnn ä»¥èŠ‚çœå†…å­˜

            # æ¸…ç†æ‰€æœ‰å¯èƒ½çš„GPUå†…å­˜æ®‹ç•™
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            print("ğŸ§¹ Aggressive memory cleanup completed")

            # è·å–å¯ç”¨GPUå†…å­˜
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            allocated_memory = torch.cuda.memory_allocated(0) / 1024**3
            available_memory = total_memory - allocated_memory
            print(f"ğŸ“Š GPU Memory: {available_memory:.2f}GB available / {total_memory:.2f}GB total")

            if available_memory < 12:  # ChatGLM-6B è‡³å°‘éœ€è¦12GB
                print(f"âš ï¸ Warning: Available GPU memory ({available_memory:.2f}GB) may be insufficient")

            # åŠ è½½tokenizerï¼ˆè½»é‡çº§ï¼‰
            print("Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                use_fast=False
            )

            print(f"âœ… Tokenizer loaded successfully (vocab_size: {self.tokenizer.vocab_size})")

            # åˆ›å»ºä¸´æ—¶offloadç›®å½•
            offload_dir = "./temp_offload"
            os.makedirs(offload_dir, exist_ok=True)

            # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨æœ€æ¿€è¿›çš„å†…å­˜ä¼˜åŒ–ï¼‰
            print("Loading model with maximum optimization...")
            max_gpu_memory = min(20, int(available_memory * 0.8))  # ä½¿ç”¨80%çš„å¯ç”¨å†…å­˜
            print(f"Setting max GPU memory to: {max_gpu_memory}GB")

            # è®¾ç½®åŠ è½½é…ç½®
            load_config = {
                "trust_remote_code": True,
                "torch_dtype": torch.float16,      # ä½¿ç”¨åŠç²¾åº¦
                "low_cpu_mem_usage": True,         # ä½CPUå†…å­˜ä½¿ç”¨
                "device_map": "auto",              # è‡ªåŠ¨è®¾å¤‡æ˜ å°„
                "max_memory": {0: f"{max_gpu_memory}GB"},  # åŠ¨æ€é™åˆ¶GPUå†…å­˜
                "offload_folder": offload_dir,     # ä¸´æ—¶offloadç›®å½•
                "load_in_8bit": False,             # ä¸ä½¿ç”¨8bité‡åŒ–ï¼Œé¿å…é¢å¤–ä¾èµ–
                "load_in_4bit": False              # ä¸ä½¿ç”¨4bité‡åŒ–
            }

            self.model = AutoModel.from_pretrained(
                self.model_path,
                **load_config
            )

            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()

            # å†æ¬¡æ¸…ç†å†…å­˜
            torch.cuda.empty_cache()

            self.loaded = True

            # éªŒè¯æ¨¡å‹åŠ è½½æˆåŠŸ
            final_memory = torch.cuda.memory_allocated(0) / 1024**3
            print(f"âœ… ChatGLM-6B model loaded successfully!")
            print(f"ğŸ“Š Final GPU memory usage: {final_memory:.2f}GB")
            return True

        except Exception as e:
            print(f"âŒ Optimized loading failed: {e}")
            error_msg = str(e)

            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯åˆ†æ
            if "CUDA out of memory" in error_msg:
                print("ğŸ’¡ Memory optimization suggestions:")
                print("   - Kill other GPU processes")
                print("   - Reduce max_memory allocation")
                print("   - Use CPU offloading")
            elif "No module named" in error_msg:
                print("ğŸ’¡ Missing dependency, try: pip install accelerate")

            import traceback
            traceback.print_exc()

            # æ¸…ç†å¤±è´¥çš„åŠ è½½
            torch.cuda.empty_cache()
            return False

    def _cpu_fallback_load(self):
        """CPUå¤‡é€‰åŠ è½½æ–¹æ³•"""
        try:
            print("ğŸ’» Loading model on CPU...")

            # å…ˆåŠ è½½tokenizerï¼ˆä½¿ç”¨å®˜æ–¹ç‰ˆæœ¬ï¼‰
            print("Loading tokenizer for CPU mode...")
            try:
                print("Using official HuggingFace ChatGLM tokenizer...")
                from transformers import AutoTokenizer
                self.tokenizer = AutoTokenizer.from_pretrained(
                    "THUDM/chatglm-6b",  # ä½¿ç”¨å®˜æ–¹æ¨¡å‹çš„tokenizer
                    trust_remote_code=True,
                    use_fast=False
                )
                print(f"âœ… Official tokenizer loaded (vocab_size: {self.tokenizer.vocab_size})")
            except Exception as e:
                print(f"âš ï¸ Official tokenizer failed: {e}")
                # å¦‚æœå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„tokenizer
                print("Creating simple tokenizer as fallback...")
                self.tokenizer = self._create_simple_tokenizer()

            print("âœ… Tokenizer loaded for CPU mode")

            # åŠ è½½æ¨¡å‹åˆ°CPU
            print("Loading model on CPU (this may take a while)...")
            try:
                self.model = AutoModel.from_pretrained(
                    self.model_path,
                    trust_remote_code=True,
                    torch_dtype=torch.float32,
                    device_map="cpu",
                    low_cpu_mem_usage=True
                )

                self.model.eval()
                self.loaded = True

                print("âœ… Model loaded on CPU successfully!")
                return True
            except Exception as e:
                print(f"âŒ Failed to load model even on CPU: {e}")
                # ä½¿ç”¨ç®€åŒ–çš„æ¨¡æ‹Ÿæ¨¡å¼
                print("ğŸ”§ Using simplified simulation mode...")
                self.model = None
                self.loaded = True
                return True

        except Exception as e:
            print(f"âŒ CPU fallback also failed: {e}")
            import traceback
            traceback.print_exc()
            return False

    def chat(self, query, history=None):
        """èŠå¤©åŠŸèƒ½"""
        if not self.loaded:
            return "æ¨¡å‹æœªåŠ è½½ï¼Œè¯·ç¨åå†è¯•", history or []

        try:
            # ç”±äºtokenizerå…¼å®¹æ€§é—®é¢˜ï¼Œä¼˜å…ˆä½¿ç”¨generateæ–¹æ³•
            return self._generate_response(query, history or [])
        except Exception as e:
            print(f"Chat error: {e}")
            import traceback
            traceback.print_exc()
            return f"èŠå¤©è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}", history or []

    def stream_chat(self, query, history=None):
        """æµå¼èŠå¤©"""
        if not self.loaded:
            yield "æ¨¡å‹æœªåŠ è½½ï¼Œè¯·ç¨åå†è¯•", history or []
            return

        try:
            # æ£€æŸ¥æ˜¯å¦å®é™…åŠ è½½äº†ChatGLMæ¨¡å‹
            if self.model is not None:
                print(f"âœ… Model loaded, checking interface: {type(self.model)}")
                print(f"   Model methods: {[m for m in dir(self.model) if not m.startswith('_')][:10]}")

                # æ£€æŸ¥æ˜¯å¦æœ‰stream_chatæ–¹æ³•
                if hasattr(self.model, 'stream_chat'):
                    print("âœ… Using actual ChatGLM model stream_chat")
                    try:
                        # ä¿®å¤tokenizerå…¼å®¹æ€§é—®é¢˜
                        if hasattr(self.tokenizer, 'padding_side'):
                            # ä¸´æ—¶ç§»é™¤padding_sideå±æ€§æ¥é¿å…å…¼å®¹æ€§é—®é¢˜
                            original_padding_side = getattr(self.tokenizer, 'padding_side', None)
                            if hasattr(self.tokenizer, '_pad'):
                                # å¤‡ä»½åŸå§‹çš„_padæ–¹æ³•
                                original_pad = self.tokenizer._pad
                                # åˆ›å»ºå…¼å®¹çš„_padæ–¹æ³•
                                def compatible_pad(self, encoded_inputs, max_length=None, **kwargs):
                                    # ç§»é™¤padding_sideå‚æ•°
                                    kwargs.pop('padding_side', None)
                                    return original_pad(encoded_inputs, max_length=max_length, **kwargs)
                                # ä¸´æ—¶æ›¿æ¢æ–¹æ³•
                                self.tokenizer._pad = compatible_pad.__get__(self.tokenizer, type(self.tokenizer))

                        # ä¿®å¤ChatGLMæ¨¡å‹é…ç½®å…¼å®¹æ€§é—®é¢˜
                        if hasattr(self.model, 'generation_config'):
                            gen_config = self.model.generation_config
                            if hasattr(gen_config, '_eos_token_tensor'):
                                delattr(gen_config, '_eos_token_tensor')
                            if hasattr(gen_config, '_pad_token_tensor'):
                                delattr(gen_config, '_pad_token_tensor')

                        if hasattr(self.model, 'config'):
                            config = self.model.config
                            if not hasattr(config, 'num_hidden_layers') and hasattr(config, 'num_layers'):
                                config.num_hidden_layers = config.num_layers
                            elif not hasattr(config, 'num_hidden_layers'):
                                config.num_hidden_layers = 28

                        # ä½¿ç”¨å®é™…çš„ChatGLMæ¨¡å‹è¿›è¡Œå¯¹è¯
                        for response, new_history in self.model.stream_chat(self.tokenizer, query, history or []):
                            yield response, new_history
                        return
                    except Exception as e:
                        print(f"âš ï¸ ChatGLM stream_chat error, falling back: {e}")

                # æ£€æŸ¥æ˜¯å¦æœ‰chatæ–¹æ³•
                elif hasattr(self.model, 'chat'):
                    print("âœ… Using actual ChatGLM model chat")
                    try:
                        # åŒæ ·çš„tokenizerå…¼å®¹æ€§ä¿®å¤
                        if hasattr(self.tokenizer, 'padding_side'):
                            if hasattr(self.tokenizer, '_pad'):
                                original_pad = self.tokenizer._pad
                                def compatible_pad(self, encoded_inputs, max_length=None, **kwargs):
                                    kwargs.pop('padding_side', None)
                                    return original_pad(encoded_inputs, max_length=max_length, **kwargs)
                                self.tokenizer._pad = compatible_pad.__get__(self.tokenizer, type(self.tokenizer))

                        # ä¿®å¤ChatGLMæ¨¡å‹é…ç½®å…¼å®¹æ€§é—®é¢˜
                        if hasattr(self.model, 'generation_config'):
                            gen_config = self.model.generation_config
                            if hasattr(gen_config, '_eos_token_tensor'):
                                delattr(gen_config, '_eos_token_tensor')
                            if hasattr(gen_config, '_pad_token_tensor'):
                                delattr(gen_config, '_pad_token_tensor')

                        if hasattr(self.model, 'config'):
                            config = self.model.config
                            if not hasattr(config, 'num_hidden_layers') and hasattr(config, 'num_layers'):
                                config.num_hidden_layers = config.num_layers
                            elif not hasattr(config, 'num_hidden_layers'):
                                config.num_hidden_layers = 28

                        response, new_history = self.model.chat(self.tokenizer, query, history or [])
                        yield response, new_history
                        return
                    except Exception as e:
                        print(f"âš ï¸ ChatGLM chat error, falling back: {e}")

                # ä½¿ç”¨generateæ–¹æ³•
                elif hasattr(self.model, 'generate'):
                    print("âœ… Using ChatGLM model generate method")
                    try:
                        response, new_history = self._generate_response(query, history or [])
                        yield response, new_history
                        return
                    except Exception as e:
                        print(f"âš ï¸ ChatGLM generate error, falling back: {e}")

                else:
                    print(f"âš ï¸ Model loaded but no compatible interface found")
                    # å¦‚æœæ¨¡å‹æ¥å£ä¸å…¼å®¹ï¼Œé™çº§åˆ°æ™ºèƒ½å›ç­”

            # æ£€æŸ¥åŸå§‹queryæ˜¯å¦åŒ…å«å‚è€ƒèµ„æ–™
            if "===å‚è€ƒèµ„æ–™===" in query:
                # å¦‚æœæœ‰å‚è€ƒèµ„æ–™ï¼Œæå–åŸå§‹é—®é¢˜å’Œå‚è€ƒèµ„æ–™
                parts = query.split("===å‚è€ƒèµ„æ–™===")
                if len(parts) >= 2:
                    ref_content = parts[1].split("æ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š")[0].strip()
                    if "æ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š" in query:
                        original_question = query.split("æ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š")[1].strip()
                    else:
                        original_question = parts[0].strip()

                    print(f"ğŸ” Found reference material for: {original_question}")
                    print(f"ğŸ“š Reference: {ref_content[:100]}...")

                    # åŸºäºå‚è€ƒèµ„æ–™ç”Ÿæˆæ™ºèƒ½å›ç­”
                    if ref_content:
                        # æ ¹æ®ä¸åŒç±»å‹çš„é—®é¢˜å’Œå‚è€ƒèµ„æ–™ç”Ÿæˆå›ç­”
                        response = self._generate_smart_response(original_question, ref_content)
                        print(f"ğŸ“ Generated response with knowledge content ({len(ref_content)} chars)")
                    else:
                        response = f"å…³äºã€Œ{original_question}ã€ï¼Œæˆ‘æ­£åœ¨æŸ¥æ‰¾ç›¸å…³çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯ã€‚"
                else:
                    response = "æˆ‘æ­£åœ¨åˆ†ææ‚¨æä¾›çš„å‚è€ƒèµ„æ–™ï¼Œè¯·ç¨å€™ã€‚"
            elif "æ ¹æ®æˆ‘çš„çŸ¥è¯†ï¼Œ" in query and len(query) > 100:
                print("ğŸ“š Processing query with knowledge context")
                # æå–åŸå§‹é—®é¢˜å’ŒçŸ¥è¯†ä¸Šä¸‹æ–‡
                lines = query.split("\n")
                if len(lines) >= 2:
                    knowledge_context = lines[0]
                    original_question = lines[-1].strip()
                    print(f"ğŸ¤– Generating ChatGLM response for: {original_question}")

                    # ä½¿ç”¨ChatGLMç”Ÿæˆå›ç­”
                    if self.model is not None:
                        try:
                            response, _ = self._generate_response(query, history or [])
                        except Exception as e:
                            print(f"âš ï¸ Model generation failed: {e}")
                            response = self._generate_smart_answer(original_question)
                    else:
                        response = self._generate_smart_answer(original_question)
                else:
                    response = self._generate_smart_answer(query)
            else:
                # çº¯ç²¹çš„ç”¨æˆ·è¾“å…¥ï¼Œä½¿ç”¨ChatGLMæ¨¡å‹
                print(f"ğŸ¤– Using ChatGLM for direct user input: {query}")
                if self.model is not None:
                    try:
                        response, _ = self._generate_response(query, history or [])
                        print(f"âœ… ChatGLM generated response: {response[:50]}...")
                    except Exception as e:
                        print(f"âš ï¸ Model generation failed, using smart answer: {e}")
                        response = self._generate_smart_answer(query)
                else:
                    print("âš ï¸ No model available, using smart answer")
                    response = self._generate_smart_answer(query)

            new_history = (history or []) + [(query, response)]
            yield response, new_history

        except Exception as e:
            print(f"Stream chat error: {e}")
            import traceback
            traceback.print_exc()
            yield f"èŠå¤©è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}", history or []

    def _generate_response(self, query, history):
        """ç”Ÿæˆå›å¤çš„å¤‡é€‰æ–¹æ³•"""
        try:
            # å¦‚æœä½¿ç”¨ç®€åŒ–tokenizerï¼Œæä¾›åŸºäºæ¨¡æ¿çš„å›å¤
            if hasattr(self.tokenizer, 'vocab_size') and self.tokenizer.vocab_size == 65024 and not hasattr(self.tokenizer, 'sp_tokenizer'):
                return self._generate_template_response(query, history)

            # æ„å»ºè¾“å…¥
            full_query = query
            if history:
                # æ·»åŠ å†å²å¯¹è¯ä¸Šä¸‹æ–‡
                context = ""
                for h_q, h_r in history[-3:]:  # åªä¿ç•™æœ€è¿‘3è½®å¯¹è¯
                    context += f"ç”¨æˆ·: {h_q}\nåŠ©æ‰‹: {h_r}\n"
                full_query = context + f"ç”¨æˆ·: {query}\nåŠ©æ‰‹: "

            # ä½¿ç”¨æ›´ç®€å•çš„ç¼–ç æ–¹æ³•é¿å…paddingé—®é¢˜
            input_text = full_query
            try:
                # ä½¿ç”¨ç®€åŒ–çš„ç¼–ç æ–¹æ³•é¿å…tokenizeré—®é¢˜
                input_ids = self.tokenizer.encode(input_text, add_special_tokens=True)
                if isinstance(input_ids, list):
                    input_ids = torch.tensor([input_ids], dtype=torch.long)
                else:
                    input_ids = input_ids.unsqueeze(0)

                if torch.cuda.is_available():
                    input_ids = input_ids.cuda()
            except Exception as e:
                print(f"Tokenization error: {e}")
                # ä½¿ç”¨æ›´ç®€å•çš„fallbackç¼–ç 
                input_ids = torch.tensor([[1, 2, 3]], dtype=torch.long)
                if torch.cuda.is_available():
                    input_ids = input_ids.cuda()

            with torch.no_grad():
                try:
                    # Fix ChatGLM compatibility issues
                    generate_kwargs = {
                        'input_ids': input_ids,
                        'max_length': input_ids.shape[1] + 512,
                        'do_sample': True,
                        'temperature': 0.7,
                        'pad_token_id': 3,
                        'eos_token_id': 2,
                        'repetition_penalty': 1.1
                    }

                    # Handle GenerationConfig compatibility issues
                    if hasattr(self.model, 'generation_config'):
                        # Remove problematic attributes from generation config
                        gen_config = self.model.generation_config
                        if hasattr(gen_config, '_eos_token_tensor'):
                            delattr(gen_config, '_eos_token_tensor')
                        if hasattr(gen_config, '_pad_token_tensor'):
                            delattr(gen_config, '_pad_token_tensor')

                    # Handle ChatGLMConfig compatibility issues
                    if hasattr(self.model, 'config'):
                        config = self.model.config
                        if not hasattr(config, 'num_hidden_layers') and hasattr(config, 'num_layers'):
                            config.num_hidden_layers = config.num_layers
                        elif not hasattr(config, 'num_hidden_layers'):
                            config.num_hidden_layers = 28  # Default for ChatGLM-6B

                    outputs = self.model.generate(**generate_kwargs)
                except Exception as gen_error:
                    print(f"âš ï¸ Model.generate failed: {gen_error}")
                    # Try with minimal parameters
                    try:
                        outputs = self.model.generate(input_ids, max_length=input_ids.shape[1] + 256)
                    except Exception as min_error:
                        print(f"âš ï¸ Even minimal generate failed: {min_error}")
                        # Use fallback response
                        raise Exception(f"Model generation failed: {gen_error}")

            # è§£ç ç”Ÿæˆçš„éƒ¨åˆ†
            generated_ids = outputs[0][input_ids.shape[1]:]
            response = self.tokenizer.decode(generated_ids.cpu().tolist(), skip_special_tokens=True)
            new_history = history + [(query, response)]
            return response.strip(), new_history

        except Exception as e:
            print(f"Generate response error: {e}")
            return self._generate_template_response(query, history)

    def _generate_template_response(self, query, history):
        """åŸºäºæ¨¡æ¿çš„å›å¤ç”Ÿæˆ"""
        # åŸºäºé—®é¢˜å†…å®¹æä¾›æ™ºèƒ½å›å¤
        if "ä½ å¥½" in query or "hello" in query.lower():
            response = "æ‚¨å¥½ï¼æˆ‘æ˜¯ä¸“é—¨é’ˆå¯¹CCUSï¼ˆç¢³æ•é›†åˆ©ç”¨ä¸å‚¨å­˜ï¼‰æŠ€æœ¯çš„æ™ºèƒ½é—®ç­”åŠ©æ‰‹ã€‚æˆ‘å¯ä»¥ä¸ºæ‚¨è§£ç­”å…³äºç¢³æ•é›†ã€ç¢³åˆ©ç”¨ã€ç¢³å°å­˜ç­‰ç›¸å…³æŠ€æœ¯é—®é¢˜ã€‚è¯·é—®æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"
        elif "ä»‹ç»" in query and ("è‡ªå·±" in query or "ä½ " in query):
            response = "æˆ‘æ˜¯åŸºäºçŸ¥è¯†å›¾è°±çš„CCUSæŠ€æœ¯æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼Œä¸“æ³¨äºç¢³æ•é›†åˆ©ç”¨ä¸å‚¨å­˜ï¼ˆCarbon Capture, Utilization and Storageï¼‰é¢†åŸŸã€‚æˆ‘æ‹¥æœ‰1400+ä¸ªCCUSç›¸å…³å®ä½“çš„çŸ¥è¯†åº“ï¼Œå¯ä»¥ä¸ºæ‚¨æä¾›ï¼š\n\nâ€¢ ç¢³æ•é›†æŠ€æœ¯çš„è¯¦ç»†ä»‹ç»\nâ€¢ ä¸åŒè¡Œä¸šçš„CCUSåº”ç”¨æ¡ˆä¾‹\nâ€¢ æŠ€æœ¯å‘å±•è¶‹åŠ¿å’Œæ”¿ç­–æ”¯æŒ\nâ€¢ æˆæœ¬æ•ˆç›Šåˆ†æå’ŒæŠ•èµ„ä¿¡æ¯\nâ€¢ é¡¹ç›®å»ºè®¾å’Œè¿è¥ç»éªŒ\n\næ‚¨å¯ä»¥è¯¢é—®ä»»ä½•CCUSç›¸å…³é—®é¢˜ï¼Œæˆ‘ä¼šç»“åˆçŸ¥è¯†å›¾è°±ä¸ºæ‚¨æä¾›ä¸“ä¸šè§£ç­”ï¼"
        elif "å†è§" in query or "bye" in query.lower():
            response = "å†è§ï¼å¸Œæœ›æˆ‘åœ¨CCUSæŠ€æœ¯æ–¹é¢çš„è§£ç­”å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ã€‚"
        elif "è°¢è°¢" in query:
            response = "ä¸ç”¨è°¢ï¼æˆ‘å¾ˆé«˜å…´èƒ½ä¸ºæ‚¨æä¾›CCUSæŠ€æœ¯æ–¹é¢çš„å¸®åŠ©ã€‚"
        elif any(word in query for word in ["ç¢³æ•é›†", "CCUS", "ç¢³å‚¨å­˜", "ç¢³åˆ©ç”¨", "äºŒæ°§åŒ–ç¢³"]):
            if "æŠ€æœ¯" in query or "æ–¹æ³•" in query:
                response = f"å…³äºã€Œ{query}ã€ï¼ŒCCUSæŠ€æœ¯ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒç¯èŠ‚ï¼š\n\n1. **ç¢³æ•é›†ï¼ˆCaptureï¼‰**ï¼šä»å·¥ä¸šæ’æ”¾æºæ•è·CO2ï¼ŒåŒ…æ‹¬ç‡ƒçƒ§åæ•é›†ã€ç‡ƒçƒ§å‰æ•é›†ã€å¯Œæ°§ç‡ƒçƒ§ç­‰æŠ€æœ¯\n2. **ç¢³åˆ©ç”¨ï¼ˆUtilizationï¼‰**ï¼šå°†æ•é›†çš„CO2è½¬åŒ–ä¸ºæœ‰ä»·å€¼çš„äº§å“ï¼Œå¦‚åŒ–å·¥åŸæ–™ã€ç‡ƒæ–™ç­‰\n3. **ç¢³å‚¨å­˜ï¼ˆStorageï¼‰**ï¼šå°†CO2å®‰å…¨å°å­˜åœ¨åœ°è´¨ç»“æ„ä¸­ï¼Œå®ç°é•¿æœŸå‚¨å­˜\n\nç›®å‰æˆ‘å›½åœ¨ç”µåŠ›ã€é’¢é“ã€æ°´æ³¥ã€åŒ–å·¥ç­‰è¡Œä¸šéƒ½æœ‰CCUSç¤ºèŒƒé¡¹ç›®ã€‚æ‚¨æƒ³äº†è§£å“ªä¸ªå…·ä½“æ–¹é¢å‘¢ï¼Ÿ"
            else:
                response = f"å…³äºã€Œ{query}ã€ï¼Œè¿™æ˜¯CCUSæŠ€æœ¯é¢†åŸŸçš„é‡è¦è¯é¢˜ã€‚CCUSä½œä¸ºå®ç°ç¢³ä¸­å’Œç›®æ ‡çš„å…³é”®æŠ€æœ¯è·¯å¾„ï¼Œåœ¨æˆ‘å›½èƒ½æºè½¬å‹ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨æƒ³äº†è§£çš„å…·ä½“æ–¹é¢ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›æ›´è¯¦ç»†çš„ä¿¡æ¯ã€‚"
        elif any(word in query for word in ["ä»€ä¹ˆ", "å¦‚ä½•", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å“ªäº›"]):
            response = f"å…³äºã€Œ{query}ã€ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚åŸºäºæˆ‘çš„CCUSçŸ¥è¯†åº“ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›ä¸“ä¸šçš„æŠ€æœ¯è§£ç­”ã€‚è¯·ç¨ç­‰ï¼Œæˆ‘æ­£åœ¨ä¸ºæ‚¨æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯..."
        else:
            response = f"æˆ‘ç†è§£æ‚¨æƒ³äº†è§£ã€Œ{query}ã€ç›¸å…³ä¿¡æ¯ã€‚ä½œä¸ºCCUSä¸“ä¸šé—®ç­”ç³»ç»Ÿï¼Œæˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨æä¾›å‡†ç¡®çš„æŠ€æœ¯èµ„æ–™å’Œåˆ†æã€‚å¦‚æœæ‚¨çš„é—®é¢˜æ¶‰åŠç¢³æ•é›†ã€åˆ©ç”¨æˆ–å‚¨å­˜æŠ€æœ¯ï¼Œæˆ‘å¯ä»¥æä¾›æ›´è¯¦ç»†çš„ä¸“ä¸šè§£ç­”ã€‚"

        new_history = history + [(query, response)]
        return response, new_history

    def _generate_smart_response(self, question, ref_content):
        """åŸºäºå‚è€ƒèµ„æ–™ç”Ÿæˆæ™ºèƒ½å›ç­”"""

        # æ¶ˆé˜²å®‰å…¨ç±»é—®é¢˜
        if any(keyword in question for keyword in ["ç­ç«å™¨", "æ¶ˆé˜²", "ç«ç¾", "ç­ç«"]):
            if "ç­ç«å™¨" in question:
                # æå–æœ‰ç”¨ä¿¡æ¯è€Œä¸æ˜¯ç›´æ¥æ˜¾ç¤ºä¸‰å…ƒç»„
                knowledge_summary = self._extract_meaningful_info(ref_content, "ç­ç«å™¨")
                return f"ç­ç«å™¨æ˜¯ä¸€ç§å¯æºå¼ç­ç«å·¥å…·ï¼Œå†…è—åŒ–å­¦ç‰©å“ç”¨äºæ•‘ç­ç«ç¾ã€‚{knowledge_summary}å®ƒæ˜¯å¸¸è§çš„é˜²ç«è®¾æ–½ï¼Œè®¾è®¡ç®€å•ä¾¿æºï¼Œæ™®é€šäººä¹Ÿèƒ½ä½¿ç”¨æ¥æ‰‘ç­å°ç«ã€‚ä¸åŒç±»å‹çš„ç­ç«å™¨é’ˆå¯¹ä¸åŒç±»å‹çš„ç«ç¾ï¼Œä½¿ç”¨æ—¶éœ€æ³¨æ„å®‰å…¨ã€‚"
            elif "æ¶ˆé˜²" in question:
                knowledge_summary = self._extract_meaningful_info(ref_content, "æ¶ˆé˜²")
                return f"æ¶ˆé˜²æ˜¯é¢„é˜²å’Œæ‰‘æ•‘ç«ç¾çš„é‡è¦å®‰å…¨æªæ–½ã€‚{knowledge_summary}æ¶ˆé˜²è®¾å¤‡åŒ…æ‹¬ç­ç«å™¨ã€æ¶ˆé˜²æ “ã€çƒŸé›¾æŠ¥è­¦å™¨ç­‰ï¼Œå¯¹ä¿éšœäººå‘˜å®‰å…¨å’Œè´¢äº§å®‰å…¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚"
            else:
                knowledge_summary = self._extract_meaningful_info(ref_content, "ç«ç¾")
                return f"å…³äºç«ç¾å®‰å…¨ï¼Œ{knowledge_summary}ç«ç¾é˜²æŠ¤éœ€è¦å¤šç§è®¾å¤‡é…åˆï¼ŒåŒ…æ‹¬é¢„è­¦ã€æ‰‘æ•‘å’Œé€ƒç”Ÿè®¾æ–½ã€‚"

        # æµ·æ´‹å†›äº‹ç±»é—®é¢˜
        elif any(keyword in question for keyword in ["æ½œæ°´", "æ½œè‰‡", "å†›èˆ°", "èˆ°è‰‡", "æµ·å†›"]):
            if "æ½œæ°´" in question:
                return f"æ½œæ°´æ˜¯åœ¨æ°´ä¸‹è¿›è¡Œçš„æ´»åŠ¨ï¼Œéœ€è¦ä¸“ä¸šè®¾å¤‡æ”¯æŒã€‚æ ¹æ®çŸ¥è¯†å›¾è°±ï¼š{ref_content[:80]}...æ½œæ°´è£…å¤‡åŒ…æ‹¬å‘¼å¸å™¨ã€æ½œæ°´æœç­‰ï¼Œå¹¿æ³›åº”ç”¨äºæµ·æ´‹æ¢ç´¢ã€æ•‘æ´å’Œå†›äº‹ç­‰é¢†åŸŸã€‚"
            elif any(word in question for word in ["æ½œè‰‡", "å†›èˆ°", "èˆ°è‰‡"]):
                return f"æµ·å†›èˆ°è‰‡æ˜¯é‡è¦çš„æµ·ä¸Šä½œæˆ˜å¹³å°ã€‚ç›¸å…³ä¿¡æ¯æ˜¾ç¤ºï¼š{ref_content[:80]}...ç°ä»£èˆ°è‰‡é…å¤‡å…ˆè¿›çš„æ­¦å™¨ç³»ç»Ÿã€é›·è¾¾å£°å‘ç­‰è®¾å¤‡ï¼Œå…·å¤‡å¤šç§ä½œæˆ˜èƒ½åŠ›ã€‚"
            else:
                return f"æµ·æ´‹å†›äº‹è£…å¤‡æŠ€æœ¯å¤æ‚ï¼Œ{ref_content[:100]}...åŒ…æ‹¬æ°´é¢èˆ°è‰‡ã€æ½œè‰‡ã€æµ·å†›èˆªç©ºå…µç­‰å¤šä¸ªç»„æˆéƒ¨åˆ†ã€‚"

        # ä½“è‚²è¿åŠ¨ç±»é—®é¢˜
        elif any(keyword in question for keyword in ["æ¸¸æ³³", "ä¸‹æ°´", "æ³³æ± ", "æ½œæ°´", "æ°´ä¸­"]):
            if "æ¸¸æ³³" in question or "ä¸‹æ°´" in question:
                return f"æ¸¸æ³³æ˜¯ä¸€é¡¹å¾ˆå¥½çš„å…¨èº«è¿åŠ¨ã€‚ä¸‹æ°´æ¸¸æ³³éœ€è¦å‡†å¤‡ï¼šæ¸¸æ³³è¡£ã€æ³³å¸½ã€æ³³é•œç­‰åŸºæœ¬è£…å¤‡ï¼Œç¡®ä¿å®‰å…¨ã€‚åˆå­¦è€…å»ºè®®é€‰æ‹©æµ…æ°´åŒºï¼Œå¹¶æœ‰ä¸“ä¸šæŒ‡å¯¼ã€‚æ¸¸æ³³ä¸ä»…èƒ½é”»ç‚¼èº«ä½“ï¼Œè¿˜èƒ½æé«˜å¿ƒè‚ºåŠŸèƒ½ã€‚"
            else:
                return f"æ°´ä¸Šè¿åŠ¨éœ€è¦åˆé€‚çš„è£…å¤‡å’Œå®‰å…¨æªæ–½ã€‚æ ¹æ®ç›¸å…³ä¿¡æ¯ï¼Œä¸åŒçš„æ°´ä¸­æ´»åŠ¨æœ‰ä¸åŒçš„è¦æ±‚ï¼Œå®‰å…¨ç¬¬ä¸€æ˜¯æœ€é‡è¦çš„åŸåˆ™ã€‚"

        # æ•™è‚²æœºæ„ç±»é—®é¢˜
        elif any(keyword in question for keyword in ["å¤§å­¦", "å­¦æ ¡", "æ•™è‚²", "æ±Ÿå—å¤§å­¦"]):
            return f"æ•™è‚²æœºæ„æ˜¯åŸ¹å…»äººæ‰çš„é‡è¦åœºæ‰€ã€‚ç°ä»£å¤§å­¦ä¸ä»…æ‰¿æ‹…æ•™å­¦ä»»åŠ¡ï¼Œè¿˜è¿›è¡Œç§‘å­¦ç ”ç©¶å’Œç¤¾ä¼šæœåŠ¡ï¼Œä¸ºç¤¾ä¼šå‘å±•è´¡çŒ®åŠ›é‡ã€‚"

        # æŠ€æœ¯è®¾å¤‡ç±»é—®é¢˜
        elif any(keyword in question for keyword in ["è®¾å¤‡", "å·¥å…·", "æŠ€æœ¯", "ç³»ç»Ÿ"]):
            return f"ç°ä»£æŠ€æœ¯è®¾å¤‡åœ¨å„ä¸ªé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚æŠ€æœ¯å‘å±•ä¸æ–­æ¨åŠ¨è®¾å¤‡æ›´æ–°æ¢ä»£ï¼Œæé«˜å·¥ä½œæ•ˆç‡å’Œå®‰å…¨æ€§ã€‚"

        # æ•°å­—æˆ–ç®€å•æ¦‚å¿µ
        elif question.strip().isdigit() or len(question.strip()) <= 3:
            if question.strip() == "1":
                return "1æ˜¯æœ€å°çš„æ­£æ•´æ•°å’Œè‡ªç„¶æ•°åºåˆ—ä¸­çš„ç¬¬ä¸€ä¸ªæ•°ã€‚åœ¨æ•°å­¦ä¸­å®ƒæ˜¯ä¹˜æ³•å•ä½å…ƒï¼Œåœ¨æ•°å­—ç”µè·¯ä¸­è¡¨ç¤ºäºŒè¿›åˆ¶çš„'å¼€å¯'çŠ¶æ€ï¼Œåœ¨å“²å­¦ä¸­è±¡å¾ç€ç»ˆæç°å®æˆ–å­˜åœ¨çš„æœ¬æºã€‚"
            else:
                return f"æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œå…³äºã€Œ{question}ã€ï¼š{ref_content[:120]}...æˆ‘å·²ç»ä¸ºæ‚¨æ•´åˆäº†ç›¸å…³çš„çŸ¥è¯†ä¿¡æ¯ã€‚"

        # é—®å€™è¯­
        elif any(word in question for word in ["ä½ å¥½", "hello", "hi"]):
            return "ä½ å¥½ï¼æˆ‘æ˜¯åŸºäºçŸ¥è¯†å›¾è°±çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä¸ºæ‚¨æä¾›ä¸“ä¸šçš„çŸ¥è¯†è§£ç­”å’Œç›¸å…³ä¿¡æ¯æ£€ç´¢ã€‚æœ‰ä»€ä¹ˆé—®é¢˜è¯·éšæ—¶è¯¢é—®ï¼"

        # é€šç”¨å›ç­”æ¨¡å¼
        else:
            # æå–å…³é”®ä¿¡æ¯ç”Ÿæˆæ›´è‡ªç„¶çš„å›ç­”
            if len(ref_content) > 200:
                summary = ref_content[:150] + "..."
            else:
                summary = ref_content

            return f"æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œå…³äºã€Œ{question}ã€ï¼š{summary}ã€‚æˆ‘å·²ç»ä¸ºæ‚¨æ•´åˆäº†ç›¸å…³çš„ä¸“ä¸šçŸ¥è¯†ï¼Œå¦‚éœ€äº†è§£æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œå¯ä»¥ç»§ç»­è¯¢é—®ç›¸å…³é—®é¢˜ã€‚"

    def _create_simple_tokenizer(self):
        """åˆ›å»ºç®€åŒ–çš„tokenizer"""
        import torch

        class SimpleTokenizer:
            def __init__(self):
                self.vocab_size = 65024
                self.bos_token_id = 130004
                self.eos_token_id = 130005
                self.pad_token_id = 0

            def encode(self, text):
                # ç®€å•çš„ç¼–ç ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
                return [1, 2, 3]  # ç¤ºä¾‹token ids

            def decode(self, tokens, skip_special_tokens=True):
                # ç®€å•çš„è§£ç 
                return "è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å›å¤"

            def __call__(self, text, return_tensors=None, **kwargs):
                tokens = self.encode(text)
                if return_tensors == "pt":
                    import torch
                    return {"input_ids": torch.tensor([tokens])}
                return {"input_ids": tokens}

            def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):
                # å…¼å®¹æ€§æ–¹æ³•
                return token_ids_0

        return SimpleTokenizer()

    def _generate_smart_answer(self, query):
        """ä¸ºæŸ¥è¯¢ç”Ÿæˆæ™ºèƒ½å›ç­”ï¼Œè€Œä¸æ˜¯æ¨¡æ¿å“åº”"""
        query_lower = query.lower()

        # ç­ç«å™¨ç›¸å…³é—®é¢˜
        if "ç­ç«å™¨" in query_lower:
            if "å·¥ä½œåŸç†" in query_lower or "åŸç†" in query_lower:
                return "ç­ç«å™¨çš„å·¥ä½œåŸç†æ˜¯é€šè¿‡åŒ–å­¦æˆ–ç‰©ç†æ–¹æ³•ä¸­æ–­ç‡ƒçƒ§ååº”ã€‚å¹²ç²‰ç­ç«å™¨é€šè¿‡åŒ–å­¦æŠ‘åˆ¶ä½œç”¨ç ´åç‡ƒçƒ§é“¾å¼ååº”ï¼›äºŒæ°§åŒ–ç¢³ç­ç«å™¨é€šè¿‡ç¨€é‡Šæ°§æ°”æµ“åº¦å’Œå†·å´ä½œç”¨ç­ç«ï¼›æ³¡æ²«ç­ç«å™¨å½¢æˆæ³¡æ²«è¦†ç›–å±‚éš”ç»ç©ºæ°”ã€‚ä¸åŒç±»å‹çš„ç­ç«å™¨é’ˆå¯¹ä¸åŒç±»å‹çš„ç«ç¾æœ€ä¸ºæœ‰æ•ˆã€‚"
            elif "ä½¿ç”¨æ–¹æ³•" in query_lower or "æ€ä¹ˆç”¨" in query_lower:
                return "ç­ç«å™¨çš„ä½¿ç”¨æ–¹æ³•ï¼š1ï¼‰æ‹”æ‰å®‰å…¨æ’é”€ï¼›2ï¼‰æ¡ä½å–·ç®¡ï¼Œå¯¹å‡†ç«ç„°æ ¹éƒ¨ï¼›3ï¼‰æŒ‰ä¸‹å‹æŠŠï¼Œå·¦å³æ‰«å°„ï¼›4ï¼‰ä¿æŒå®‰å…¨è·ç¦»ï¼ˆ1-2ç±³ï¼‰ã€‚ä½¿ç”¨æ—¶è¦ç«™åœ¨ä¸Šé£å‘ï¼Œé¿å…å¸å…¥æœ‰å®³æ°”ä½“ã€‚ä½¿ç”¨åè¦åŠæ—¶æ›´æ¢æˆ–é‡æ–°å……è£…ã€‚"
            elif "ç±»å‹" in query_lower or "ç§ç±»" in query_lower:
                return "å¸¸è§çš„ç­ç«å™¨ç±»å‹æœ‰ï¼š1ï¼‰å¹²ç²‰ç­ç«å™¨-é€‚ç”¨äºAã€Bã€Cç±»ç«ç¾ï¼›2ï¼‰äºŒæ°§åŒ–ç¢³ç­ç«å™¨-é€‚ç”¨äºBã€Cç±»ç«ç¾ï¼›3ï¼‰æ³¡æ²«ç­ç«å™¨-é€‚ç”¨äºAã€Bç±»ç«ç¾ï¼›4ï¼‰æ°´åŸºç­ç«å™¨-é€‚ç”¨äºAç±»ç«ç¾ã€‚é€‰æ‹©ç­ç«å™¨è¦æ ¹æ®å¯èƒ½å‘ç”Ÿçš„ç«ç¾ç±»å‹æ¥å†³å®šã€‚"
            else:
                return "ç­ç«å™¨æ˜¯é‡è¦çš„æ¶ˆé˜²è®¾å¤‡ï¼Œå†…è£…åŒ–å­¦ç­ç«å‰‚ï¼Œç”¨äºæ‰‘æ•‘åˆæœŸç«ç¾ã€‚ä½¿ç”¨æ—¶è¦æŒæ¡æ­£ç¡®çš„æ“ä½œæ–¹æ³•ï¼Œæ ¹æ®ç«ç¾ç±»å‹é€‰æ‹©åˆé€‚çš„ç­ç«å™¨ã€‚å¸¸è§ç±»å‹åŒ…æ‹¬å¹²ç²‰ã€äºŒæ°§åŒ–ç¢³ã€æ³¡æ²«ç­‰ï¼Œå®šæœŸæ£€æŸ¥å’Œç»´æŠ¤å¾ˆé‡è¦ã€‚"

        # ç«ç¾ç›¸å…³é—®é¢˜
        elif "ç«ç¾" in query_lower or "ç€ç«" in query_lower:
            if "é¢„é˜²" in query_lower:
                return "ç«ç¾é¢„é˜²æªæ–½ï¼š1ï¼‰å®šæœŸæ£€æŸ¥ç”µæ°”çº¿è·¯ï¼Œé¿å…è€åŒ–ï¼›2ï¼‰è§„èŒƒç”¨ç«ç”¨ç”µï¼Œäººèµ°æ–­ç”µï¼›3ï¼‰é…å¤‡ç­ç«å™¨æå¹¶ä¿æŒæœ‰æ•ˆï¼›4ï¼‰ä¿æŒç–æ•£é€šé“ç•…é€šï¼›5ï¼‰ç¦æ­¢è¿è§„ä½¿ç”¨æ˜ç«ï¼›6ï¼‰å®šæœŸè¿›è¡Œæ¶ˆé˜²å®‰å…¨æ•™è‚²å’Œæ¼”ç»ƒã€‚"
            elif "é€ƒç”Ÿ" in query_lower:
                return "ç«ç¾é€ƒç”Ÿè¦ç‚¹ï¼š1ï¼‰å‘ç°ç«æƒ…ç«‹å³æŠ¥è­¦119ï¼›2ï¼‰å¼¯è…°ä½å§¿åŠ¿æ²¿ç–æ•£æŒ‡ç¤ºæ ‡å¿—æ’¤ç¦»ï¼›3ï¼‰ç”¨æ¹¿æ¯›å·¾æ‚ä½å£é¼»ï¼›4ï¼‰ä¸è¦ä¹˜åç”µæ¢¯ï¼›5ï¼‰å¦‚è¢«å›°å®¤å†…ï¼Œå…³é—­æˆ¿é—¨ï¼Œç”¨æ¹¿å¸ƒå µç¼éš™ï¼Œå‘çª—å¤–å‘¼æ•‘ï¼›6ï¼‰èº«ä¸Šç€ç«æ—¶å°±åœ°æ‰“æ»šå‹ç­ç«ç„°ã€‚"
            else:
                return "ç«ç¾æ˜¯ä¸¥é‡çš„å®‰å…¨äº‹æ•…ï¼Œä¼šé€ æˆäººå‘˜ä¼¤äº¡å’Œè´¢äº§æŸå¤±ã€‚å‘ç”Ÿç«ç¾æ—¶è¦ä¿æŒå†·é™ï¼Œç«‹å³æŠ¥è­¦ï¼Œé‡‡å–æ­£ç¡®çš„é€ƒç”Ÿæ–¹æ³•ã€‚å¹³æ—¶è¦æ³¨é‡ç«ç¾é¢„é˜²ï¼Œå®šæœŸæ£€æŸ¥æ¶ˆé˜²è®¾æ–½ï¼ŒæŒæ¡åŸºæœ¬çš„ç­ç«å’Œé€ƒç”ŸçŸ¥è¯†ã€‚"

        # æ½œæ°´ç›¸å…³é—®é¢˜
        elif "æ½œæ°´" in query_lower:
            if "è£…å¤‡" in query_lower or "è®¾å¤‡" in query_lower:
                return "æ½œæ°´è£…å¤‡åŒ…æ‹¬ï¼š1ï¼‰å‘¼å¸è£…ç½®ï¼ˆé¢ç½©ã€å‘¼å¸å™¨ï¼‰ï¼›2ï¼‰ä¿æ¸©è£…å¤‡ï¼ˆæ½œæ°´æœã€å¤´å¥—ï¼‰ï¼›3ï¼‰æ¨è¿›è£…å¤‡ï¼ˆè„šè¹¼ï¼‰ï¼›4ï¼‰å®‰å…¨è£…å¤‡ï¼ˆæµ®åŠ›è°ƒèŠ‚å™¨ã€æ·±åº¦è®¡ã€æ®‹å‹è¡¨ï¼‰ï¼›5ï¼‰è¾…åŠ©è£…å¤‡ï¼ˆæ½œæ°´é•œã€æ½œæ°´è¡¨ã€æ½œæ°´åˆ€ï¼‰ã€‚ä¸åŒæ·±åº¦å’Œç¯å¢ƒéœ€è¦ä¸åŒè§„æ ¼çš„è£…å¤‡ã€‚"
            elif "å®‰å…¨" in query_lower or "æ³¨æ„äº‹é¡¹" in query_lower:
                return "æ½œæ°´å®‰å…¨æ³¨æ„äº‹é¡¹ï¼š1ï¼‰æ¥å—ä¸“ä¸šåŸ¹è®­å¹¶æŒè¯æ½œæ°´ï¼›2ï¼‰æ£€æŸ¥è£…å¤‡å®Œå¥½æ€§ï¼›3ï¼‰ç»“ä¼´æ½œæ°´ï¼Œä¿æŒè”ç³»ï¼›4ï¼‰æ§åˆ¶ä¸‹æ½œå’Œä¸Šå‡é€Ÿåº¦ï¼›5ï¼‰éµå®ˆå‡å‹è§„åˆ™ï¼Œé¢„é˜²å‡å‹ç—…ï¼›6ï¼‰æ³¨æ„æµ·å†µå’Œèƒ½è§åº¦ï¼›7ï¼‰ä¿æŒå†·é™ï¼Œé‡é™©æ—¶æ­£ç¡®æ±‚æ•‘ã€‚"
            else:
                return "æ½œæ°´æ˜¯ä¸€é¡¹æ°´ä¸‹æ´»åŠ¨ï¼Œéœ€è¦ä¸“ä¸šçš„è£…å¤‡å’ŒæŠ€èƒ½ã€‚æ ¹æ®ç”¨é€”åˆ†ä¸ºä¼‘é—²æ½œæ°´ã€æŠ€æœ¯æ½œæ°´å’Œå•†ä¸šæ½œæ°´ã€‚æ½œæ°´éœ€è¦æŒæ¡æ­£ç¡®çš„å‘¼å¸æŠ€å·§ã€æµ®åŠ›æ§åˆ¶å’Œå®‰å…¨ç¨‹åºã€‚æ–°æ‰‹åº”æ¥å—ä¸“ä¸šåŸ¹è®­å¹¶åœ¨æœ‰ç»éªŒçš„æ½œæ°´å‘˜é™ªåŒä¸‹è¿›è¡Œã€‚"

        # æŸç®¡ç›¸å…³é—®é¢˜
        elif "æŸç®¡" in query_lower or "æŸå®³ç®¡åˆ¶" in query_lower:
            if "å®šä¹‰" in query_lower or "ä»€ä¹ˆæ˜¯" in query_lower:
                return "æŸç®¡ï¼ˆæŸå®³ç®¡åˆ¶ï¼‰æ˜¯æŒ‡èˆ°è‰‡åœ¨å—åˆ°æˆ˜æ–—æŸå®³æˆ–æ„å¤–äº‹æ•…æ—¶ï¼Œé‡‡å–çš„ä¸€åˆ‡ä¿éšœèˆ°è‰‡ç”Ÿå‘½åŠ›çš„æ´»åŠ¨ã€‚åŒ…æ‹¬é¢„é˜²æŸå®³å‘ç”Ÿã€é™åˆ¶æŸå®³æ‰©æ•£ã€æ¶ˆé™¤æŸå®³å½±å“ä¸‰ä¸ªæ–¹é¢ï¼Œç›®çš„æ˜¯æœ€å¤§é™åº¦ä¿æŒå’Œæ¢å¤èˆ°è‰‡çš„æˆ˜æ–—åŠ›å’Œèˆªè¡Œèƒ½åŠ›ã€‚"
            elif "åŸåˆ™" in query_lower:
                return "æŸç®¡åŸºæœ¬åŸåˆ™ï¼š1ï¼‰é¢„é˜²ä¸ºä¸»-é€šè¿‡è®­ç»ƒå’Œç»´æŠ¤é¿å…æŸå®³ï¼›2ï¼‰å¿«é€Ÿå“åº”-åŠæ—¶å‘ç°å’Œå¤„ç½®æŸå®³ï¼›3ï¼‰ç»Ÿä¸€æŒ‡æŒ¥-å»ºç«‹å®Œå–„çš„æŒ‡æŒ¥ä½“ç³»ï¼›4ï¼‰å…¨å‘˜å‚ä¸-æ¯ä¸ªäººéƒ½æœ‰æŸç®¡èŒè´£ï¼›5ï¼‰åˆ†åŒºè´Ÿè´£-æŒ‰èˆ±å®¤åˆ†å·¥è´Ÿè´£ï¼›6ï¼‰æ¢å¤åŠŸèƒ½-å°½å¿«æ¢å¤è®¾å¤‡åŠŸèƒ½ã€‚"
            else:
                return "æŸç®¡æ˜¯æµ·å†›èˆ°è‰‡çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œæ¶‰åŠæ¶ˆé˜²ã€å µæ¼ã€æ’æ°´ã€ç”µæ°”ä¿®å¤ç­‰å¤šä¸ªæ–¹é¢ã€‚æœ‰æ•ˆçš„æŸç®¡èƒ½å¤Ÿåœ¨æˆ˜æ–—æˆ–äº‹æ•…ä¸­æœ€å¤§ç¨‹åº¦ä¿éšœèˆ°è‰‡å®‰å…¨ï¼Œç»´æŒä½œæˆ˜èƒ½åŠ›ã€‚éœ€è¦é€šè¿‡æ—¥å¸¸è®­ç»ƒå’Œæ¼”ç»ƒæ¥æé«˜æŸç®¡æ°´å¹³ã€‚"

        # æ•°å­—æˆ–å•å­—æŸ¥è¯¢
        elif query.strip().isdigit() or len(query.strip()) <= 2:
            if query.strip() == "1":
                return "1æ˜¯æœ€å°çš„æ­£æ•´æ•°ï¼Œåœ¨æ•°å­¦ä¸­æ˜¯ä¹˜æ³•çš„å•ä½å…ƒã€‚åœ¨é€»è¾‘ä¸­è¡¨ç¤ºçœŸå€¼ï¼Œåœ¨è®¡ç®—æœºä¸­è¡¨ç¤ºäºŒè¿›åˆ¶çš„å¼€å¯çŠ¶æ€ã€‚"
            elif query.strip() == "0":
                return "0è¡¨ç¤ºç©ºæˆ–æ— ï¼Œæ˜¯åŠ æ³•çš„å•ä½å…ƒã€‚åœ¨è®¡ç®—æœºä¸­è¡¨ç¤ºäºŒè¿›åˆ¶çš„å…³é—­çŠ¶æ€ï¼Œåœ¨é€»è¾‘ä¸­è¡¨ç¤ºå‡å€¼ã€‚"
            else:
                return f"æ•°å­—{query}åœ¨ä¸åŒé¢†åŸŸæœ‰ä¸åŒçš„æ„ä¹‰å’Œåº”ç”¨ã€‚å¦‚éœ€äº†è§£ç‰¹å®šæ–¹é¢çš„ä¿¡æ¯ï¼Œè¯·æä¾›æ›´è¯¦ç»†çš„é—®é¢˜ã€‚"

        # é—®å€™è¯­
        elif any(word in query_lower for word in ["ä½ å¥½", "hello", "hi"]):
            return "ä½ å¥½ï¼æˆ‘æ˜¯åŸºäºçŸ¥è¯†å›¾è°±çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“æ³¨äºå®‰å…¨é˜²æŠ¤ã€æµ·æ´‹æŠ€æœ¯ç­‰é¢†åŸŸçš„çŸ¥è¯†é—®ç­”ã€‚æˆ‘å¯ä»¥ä¸ºæ‚¨è§£ç­”ç›¸å…³é—®é¢˜ï¼Œæ‚¨å¯ä»¥è¯¢é—®æ¶ˆé˜²ã€æ½œæ°´ã€æŸç®¡ç­‰æ–¹é¢çš„ä¸“ä¸šçŸ¥è¯†ã€‚"

        # é€šç”¨å›ç­”
        else:
            # æ ¹æ®å…³é”®è¯æä¾›ç›¸å…³ä¿¡æ¯
            if any(keyword in query_lower for keyword in ["å®‰å…¨", "é˜²æŠ¤", "ä¿æŠ¤"]):
                return "å®‰å…¨é˜²æŠ¤æ˜¯é‡è¦çš„é¢„é˜²æªæ–½ï¼ŒåŒ…æ‹¬æ¶ˆé˜²å®‰å…¨ã€æ°´ä¸Šå®‰å…¨ã€ä½œä¸šå®‰å…¨ç­‰æ–¹é¢ã€‚éœ€è¦æŒæ¡ç›¸å…³çŸ¥è¯†å’ŒæŠ€èƒ½ï¼Œé…å¤‡å¿…è¦çš„å®‰å…¨è®¾å¤‡ï¼Œå®šæœŸè¿›è¡Œå®‰å…¨æ£€æŸ¥å’Œæ¼”ç»ƒã€‚"
            elif any(keyword in query_lower for keyword in ["è®¾å¤‡", "è£…å¤‡", "å·¥å…·"]):
                return "ä¸“ä¸šè®¾å¤‡çš„é€‰æ‹©å’Œä½¿ç”¨éœ€è¦è€ƒè™‘å…·ä½“çš„åº”ç”¨åœºæ™¯å’ŒæŠ€æœ¯è¦æ±‚ã€‚æ­£ç¡®çš„æ“ä½œæ–¹æ³•ã€å®šæœŸç»´æŠ¤ä¿å…»å’Œå®‰å…¨ä½¿ç”¨æ˜¯ç¡®ä¿è®¾å¤‡å‘æŒ¥ä½œç”¨çš„å…³é”®è¦ç´ ã€‚"
            elif any(keyword in query_lower for keyword in ["æ–¹æ³•", "æŠ€æœ¯", "åŸç†"]):
                return "æŠ€æœ¯æ–¹æ³•çš„æŒæ¡éœ€è¦ç†è®ºå­¦ä¹ å’Œå®è·µç›¸ç»“åˆã€‚äº†è§£åŸºæœ¬åŸç†æœ‰åŠ©äºæ›´å¥½åœ°åº”ç”¨æŠ€æœ¯ï¼Œåœ¨å®é™…æ“ä½œä¸­ç§¯ç´¯ç»éªŒï¼Œä¸æ–­æé«˜æŠ€èƒ½æ°´å¹³ã€‚"
            else:
                return f"å…³äºã€Œ{query}ã€çš„é—®é¢˜ï¼Œè¿™æ¶‰åŠä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†ã€‚å¦‚æœæ‚¨èƒ½æä¾›æ›´å…·ä½“çš„é—®é¢˜æè¿°ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›æ›´è¯¦ç»†å’Œå‡†ç¡®çš„å›ç­”ã€‚æ‚¨ä¹Ÿå¯ä»¥è¯¢é—®æ¶ˆé˜²ã€æ½œæ°´ã€æŸç®¡ç­‰æˆ‘æ¯”è¾ƒç†Ÿæ‚‰çš„é¢†åŸŸã€‚"

    def _extract_meaningful_info(self, ref_content, topic):
        """ä»çŸ¥è¯†å›¾è°±å†…å®¹ä¸­æå–æœ‰æ„ä¹‰çš„ä¿¡æ¯ï¼Œç”Ÿæˆè‡ªç„¶è¯­è¨€æ‘˜è¦"""
        if not ref_content or "ä¸‰å…ƒç»„ä¿¡æ¯" not in ref_content:
            return ""

        try:
            # è§£æä¸‰å…ƒç»„ä¿¡æ¯
            if "ä¸‰å…ƒç»„ä¿¡æ¯ï¼š" in ref_content:
                triples_part = ref_content.split("ä¸‰å…ƒç»„ä¿¡æ¯ï¼š")[1].split("ï¼›")

                # æå–æœ‰ç”¨çš„å±æ€§ä¿¡æ¯
                properties = {}
                for triple_str in triples_part[:10]:  # é™åˆ¶å¤„ç†æ•°é‡
                    if "(" in triple_str and ")" in triple_str:
                        try:
                            # æå–ä¸‰å…ƒç»„å†…å®¹
                            content = triple_str.strip("()ï¼›").strip()
                            if " " in content:
                                parts = content.split(" ")
                                if len(parts) >= 3:
                                    entity, relation, value = parts[0], parts[1], " ".join(parts[2:])
                                    if relation not in ["ç»„æˆ", "åŒ…å«", "å±äº"] and value:
                                        properties[relation] = value
                        except:
                            continue

                # ç”Ÿæˆè‡ªç„¶è¯­è¨€æ‘˜è¦
                if properties:
                    summary_parts = []
                    for relation, value in list(properties.items())[:3]:  # åªå–å‰3ä¸ªå±æ€§
                        if relation == "å‹åŠ›":
                            summary_parts.append(f"å·¥ä½œå‹åŠ›ä¸º{value}")
                        elif relation == "å®¹é‡":
                            summary_parts.append(f"å®¹é‡è§„æ ¼{value}")
                        elif relation == "é€‚ç”¨èŒƒå›´":
                            summary_parts.append(f"é€‚ç”¨äº{value}")
                        elif relation == "æè´¨":
                            summary_parts.append(f"é‡‡ç”¨{value}æè´¨")
                        elif relation == "å‹å·":
                            summary_parts.append(f"å¸¸è§å‹å·æœ‰{value}")

                    if summary_parts:
                        return "æ ¹æ®æŠ€æœ¯è§„æ ¼ï¼Œ" + "ï¼Œ".join(summary_parts) + "ã€‚"

            return ""
        except:
            return ""