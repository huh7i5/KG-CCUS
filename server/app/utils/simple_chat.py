"""
ç®€å•çš„ChatGLM-6BåŠ è½½å™¨
ä½¿ç”¨æ›´å…¼å®¹çš„æ–¹å¼åŠ è½½æ¨¡å‹
"""

import os
import sys
import torch
from transformers import AutoTokenizer, AutoModel

class SimpleChatGLM:
    def __init__(self, model_path, memory_optimize=False):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.loaded = False
        self.memory_optimize = memory_optimize
        print(f"ğŸ”§ SimpleChatGLM initialized with memory_optimize={memory_optimize}")

    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print("ğŸš€ Starting ChatGLM-6B model loading...")

        # æ­¥éª¤1: æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not self._validate_model_path():
            return False

        # æ­¥éª¤2: æ£€æŸ¥ç³»ç»Ÿèµ„æº
        if not self._check_system_resources():
            return False

        # æ­¥éª¤3: å°è¯•åŠ è½½æ¨¡å‹
        loading_methods = [
            ("optimized_gpu", self._load_optimized_gpu),
            ("basic_gpu", self._load_basic_gpu),
            ("cpu_fallback", self._load_cpu_fallback),
            ("minimal_mode", self._enable_minimal_mode)
        ]

        for method_name, method in loading_methods:
            print(f"ğŸ”„ Trying {method_name} loading...")
            try:
                if method():
                    print(f"âœ… Model loaded successfully using {method_name}!")
                    return True
                else:
                    print(f"âš ï¸ {method_name} loading failed, trying next method...")
            except Exception as e:
                print(f"âŒ {method_name} loading error: {e}")

        print("âŒ All loading methods failed, enabling minimal response mode")
        return self._enable_minimal_mode()

    def _validate_model_path(self):
        """éªŒè¯æ¨¡å‹è·¯å¾„"""
        if not os.path.exists(self.model_path):
            print(f"âŒ Model path not found: {self.model_path}")

            # å°è¯•å¸¸è§çš„æ¨¡å‹è·¯å¾„
            fallback_paths = [
                "/fast/zwj/ChatGLM-6B",
                "./models/ChatGLM-6B",
                "./ChatGLM-6B",
                "/home/models/ChatGLM-6B"
            ]

            for path in fallback_paths:
                if os.path.exists(path) and os.listdir(path):
                    print(f"ğŸ“ Found alternative model path: {path}")
                    self.model_path = path
                    return True

            print("âŒ No valid model path found")
            return False

        # æ£€æŸ¥è·¯å¾„æ˜¯å¦ä¸ºç©º
        if not os.listdir(self.model_path):
            print(f"âŒ Model path is empty: {self.model_path}")
            return False

        print(f"âœ… Model path validated: {self.model_path}")
        return True

    def _check_system_resources(self):
        """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
        # æ£€æŸ¥CUDAå¯ç”¨æ€§
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            allocated_memory = torch.cuda.memory_allocated(0) / 1024**3
            available_memory = total_memory - allocated_memory
            print(f"ğŸ“Š GPU Memory: {available_memory:.2f}GB available / {total_memory:.2f}GB total")

            if available_memory < 6:  # è‡³å°‘éœ€è¦6GB
                print("âš ï¸ Low GPU memory, will try optimized loading")
            return True
        else:
            print("âš ï¸ CUDA not available, will use CPU mode")
            return True

    def _load_optimized_gpu(self):
        """ä¼˜åŒ–çš„GPUåŠ è½½æ–¹æ³•"""
        if not torch.cuda.is_available():
            return False

        try:
            # åŸºç¡€ç¯å¢ƒè®¾ç½®
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
            torch.cuda.empty_cache()

            # åŠ è½½tokenizer
            print("Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                use_fast=False
            )

            # åŠ è½½æ¨¡å‹
            print("Loading model with GPU optimization...")
            self.model = AutoModel.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True
            )

            self.model.eval()
            self.loaded = True
            return True

        except Exception as e:
            print(f"Optimized GPU loading failed: {e}")
            return False

    def _load_basic_gpu(self):
        """åŸºç¡€GPUåŠ è½½æ–¹æ³•"""
        if not torch.cuda.is_available():
            return False

        try:
            torch.cuda.empty_cache()

            print("Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )

            print("Loading model with basic GPU setup...")
            self.model = AutoModel.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                torch_dtype=torch.float16
            )

            self.model = self.model.cuda()
            self.model.eval()
            self.loaded = True
            return True

        except Exception as e:
            print(f"Basic GPU loading failed: {e}")
            return False

    def _load_cpu_fallback(self):
        """CPUå¤‡ç”¨åŠ è½½æ–¹æ³•"""
        try:
            print("Loading tokenizer for CPU mode...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )

            print("Loading model on CPU...")
            self.model = AutoModel.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                torch_dtype=torch.float32,
                device_map="cpu"
            )

            self.model.eval()
            self.loaded = True
            return True

        except Exception as e:
            print(f"CPU loading failed: {e}")
            return False

    def _enable_minimal_mode(self):
        """å¯ç”¨æœ€å°æ¨¡å¼ï¼ˆæ— çœŸå®æ¨¡å‹ï¼‰"""
        print("ğŸ”§ Enabling minimal response mode...")
        self.model = None
        self.tokenizer = self._create_simple_tokenizer()
        self.loaded = True
        print("âœ… Minimal mode enabled - using template responses")
        return True


    def chat(self, query, history=None):
        """èŠå¤©åŠŸèƒ½"""
        if not self.loaded:
            return "æ¨¡å‹æœªåŠ è½½ï¼Œè¯·ç¨åå†è¯•", history or []

        try:
            # ç”±äºtokenizerå…¼å®¹æ€§é—®é¢˜ï¼Œä¼˜å…ˆä½¿ç”¨generateæ–¹æ³•
            return self._generate_response(query, history or [])
        except Exception as e:
            print(f"Chat error: {e}")
            import traceback
            traceback.print_exc()
            return f"èŠå¤©è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}", history or []

    def stream_chat(self, query, history=None):
        """æµå¼èŠå¤©"""
        print(f"ğŸš€ Stream chat called with query: {query[:50]}...")

        if not self.loaded:
            yield "æ¨¡å‹æœªåŠ è½½ï¼Œè¯·ç¨åå†è¯•", history or []
            return

        try:
            # å¦‚æœæœ‰çœŸå®æ¨¡å‹ï¼Œå°è¯•ä½¿ç”¨
            if self.model is not None:
                print("âœ… Using actual ChatGLM model")

                # ä¿®å¤tokenizerå…¼å®¹æ€§é—®é¢˜
                self._fix_tokenizer_compatibility()

                # æ™ºèƒ½é‡è¯•æœºåˆ¶ï¼šæ ¹æ®é”™è¯¯ç±»å‹å†³å®šé‡è¯•ç­–ç•¥
                chatglm_success = self._try_chatglm_with_retry(query, history or [])
                if chatglm_success:
                    for response, new_history in chatglm_success:
                        yield response, new_history
                    return

                # å¦‚æœChatGLMå®Œå…¨å¤±è´¥ï¼Œæä¾›æ™ºèƒ½å›ç­”è€Œä¸æ˜¯ç®€å•æ¨¡æ¿
                print("âš ï¸ ChatGLM unavailable, providing knowledge-enhanced response")
                enhanced_response = self._generate_enhanced_fallback_response(query, history or [])
                yield enhanced_response, (history or []) + [(query, enhanced_response)]
                return

            # ä½¿ç”¨æ¨¡æ¿å“åº”æ¨¡å¼
            response = self._generate_smart_answer(query)
            new_history = (history or []) + [(query, response)]
            yield response, new_history

        except Exception as e:
            print(f"âŒ Stream chat error: {e}")
            error_response = f"å¯¹è¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œæ­£åœ¨ä½¿ç”¨å¤‡ç”¨å“åº”æ¨¡å¼ä¸ºæ‚¨å›ç­”é—®é¢˜ã€‚"
            yield error_response, history or []

    def _fix_tokenizer_compatibility(self):
        """ä¿®å¤ChatGLM tokenizerå…¼å®¹æ€§é—®é¢˜"""
        try:
            if hasattr(self.tokenizer, '_pad'):
                # ä¿å­˜åŸå§‹çš„_padæ–¹æ³•
                original_pad = self.tokenizer._pad

                def compatible_pad(self, encoded_inputs, max_length=None, padding_strategy=None, **kwargs):
                    """å…¼å®¹çš„padæ–¹æ³•ï¼Œç§»é™¤problematicå‚æ•°"""
                    # ç§»é™¤padding_sideå‚æ•°ï¼Œè¿™æ˜¯å¯¼è‡´é”™è¯¯çš„åŸå› 
                    kwargs.pop('padding_side', None)
                    # ç§»é™¤å…¶ä»–å¯èƒ½æœ‰é—®é¢˜çš„å‚æ•°
                    kwargs.pop('pad_to_multiple_of', None)
                    kwargs.pop('return_attention_mask', None)

                    return original_pad(encoded_inputs, max_length=max_length, **kwargs)

                # æ›¿æ¢_padæ–¹æ³•
                self.tokenizer._pad = compatible_pad.__get__(self.tokenizer, type(self.tokenizer))
                print("ğŸ”§ Fixed tokenizer _pad method compatibility")

        except Exception as e:
            print(f"âš ï¸ Failed to fix tokenizer compatibility: {e}")

    def _try_chatglm_with_retry(self, query, history):
        """æ™ºèƒ½é‡è¯•æœºåˆ¶ï¼šæ ¹æ®é”™è¯¯ç±»å‹å†³å®šé‡è¯•ç­–ç•¥"""
        methods = [
            ("stream_chat", self._try_stream_chat),
            ("chat", self._try_chat),
            ("generate", self._try_generate)
        ]

        last_error = None
        for attempt, (method_name, method_func) in enumerate(methods):
            try:
                print(f"ğŸ”„ Attempt {attempt + 1}: Trying ChatGLM {method_name} method...")

                # å¯¹äºgenerateæ–¹æ³•ï¼Œç»™æ›´å¤šçš„é‡è¯•æœºä¼š
                max_retries = 3 if method_name == "generate" else 1

                for retry in range(max_retries):
                    try:
                        if retry > 0:
                            print(f"â™»ï¸ Retrying {method_name} (attempt {retry + 1}/{max_retries})...")
                            # æ¸…ç†GPUå†…å­˜ï¼Œå¯èƒ½æœ‰åŠ©äºè§£å†³å†…å­˜é—®é¢˜
                            if torch.cuda.is_available():
                                torch.cuda.empty_cache()

                        result = list(method_func(query, history))
                        if result:
                            response, new_history = result[0]
                            if response and len(response.strip()) > 5:  # ç¡®ä¿å›ç­”æœ‰æ„ä¹‰
                                print(f"âœ… ChatGLM {method_name} success (attempt {retry + 1}): {response[:50]}...")
                                yield response, new_history
                                return

                    except Exception as retry_error:
                        last_error = retry_error
                        if retry < max_retries - 1:
                            print(f"âš ï¸ {method_name} retry {retry + 1} failed: {retry_error}")
                        else:
                            print(f"âŒ {method_name} all retries failed: {retry_error}")

                        # æ ¹æ®é”™è¯¯ç±»å‹å†³å®šæ˜¯å¦ç»§ç»­é‡è¯•
                        if self._should_stop_retry(retry_error):
                            print(f"ğŸ›‘ Stopping retries for {method_name} due to fatal error")
                            break

            except Exception as e:
                last_error = e
                print(f"âŒ ChatGLM {method_name} completely failed: {e}")
                continue

        print(f"âŒ All ChatGLM methods exhausted. Last error: {last_error}")
        return None

    def _should_stop_retry(self, error):
        """æ ¹æ®é”™è¯¯ç±»å‹å†³å®šæ˜¯å¦åœæ­¢é‡è¯•"""
        error_str = str(error).lower()

        # è¿™äº›é”™è¯¯ä¸åº”è¯¥é‡è¯•
        fatal_errors = [
            "out of memory",
            "cuda out of memory",
            "model not found",
            "module not found",
            "no module named"
        ]

        return any(fatal in error_str for fatal in fatal_errors)

    def _generate_enhanced_fallback_response(self, query, history):
        """ç”Ÿæˆå¢å¼ºçš„fallbackå“åº”ï¼Œå°½é‡ä¿æŒæ™ºèƒ½åŒ–"""
        print("ğŸ”§ Generating enhanced fallback response...")

        # é¦–å…ˆå°è¯•ç”ŸæˆCCUSä¸“ä¸šå›ç­”
        if any(keyword in query.lower() for keyword in ["ccus", "ç¢³æ•é›†", "ç¢³å‚¨å­˜", "ç¢³åˆ©ç”¨", "äºŒæ°§åŒ–ç¢³", "ç¢³ä¸­å’Œ", "å‡æ’"]):
            response = self._generate_ccus_response(query, query.lower())
            print(f"ğŸ¯ CCUSä¸“ä¸šå›ç­”: {response[:100]}...")
            return response

        # å¯¹äºå…¶ä»–é—®é¢˜ï¼Œç”Ÿæˆé€šç”¨æ™ºèƒ½å›ç­”
        enhanced_response = f"""æˆ‘ç†è§£æ‚¨è¯¢é—®ã€Œ{query}ã€ã€‚è™½ç„¶å½“å‰ChatGLMæ¨¡å‹æš‚æ—¶ä¸å¯ç”¨ï¼Œä½†æˆ‘å¯ä»¥åŸºäºCCUSé¢†åŸŸçŸ¥è¯†ä¸ºæ‚¨æä¾›ç›¸å…³ä¿¡æ¯ï¼š

å¦‚æœæ‚¨çš„é—®é¢˜æ¶‰åŠç¢³æ•é›†åˆ©ç”¨ä¸å‚¨å­˜ï¼ˆCCUSï¼‰æŠ€æœ¯ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨è¯¦ç»†ä»‹ç»ç›¸å…³çš„æŠ€æœ¯åŸç†ã€åº”ç”¨æ¡ˆä¾‹ã€æˆæœ¬åˆ†æã€æ”¿ç­–æ”¯æŒç­‰æ–¹é¢çš„ä¸“ä¸šçŸ¥è¯†ã€‚

è¯·å‘Šè¯‰æˆ‘æ‚¨å…·ä½“å…³æ³¨çš„CCUSæŠ€æœ¯æ–¹é¢ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›æ›´ç²¾å‡†çš„å›ç­”ã€‚"""

        print(f"ğŸ”„ Enhanced fallback: {enhanced_response[:100]}...")
        return enhanced_response

    def _try_stream_chat(self, query, history):
        """å°è¯•ä½¿ç”¨ChatGLM stream_chatæ–¹æ³•"""
        if hasattr(self.model, 'stream_chat'):
            for response, new_history in self.model.stream_chat(self.tokenizer, query, history):
                yield response, new_history
        else:
            raise Exception("Model does not have stream_chat method")

    def _try_chat(self, query, history):
        """å°è¯•ä½¿ç”¨ChatGLM chatæ–¹æ³•"""
        if hasattr(self.model, 'chat'):
            response, new_history = self.model.chat(self.tokenizer, query, history)
            yield response, new_history
        else:
            raise Exception("Model does not have chat method")

    def _try_generate(self, query, history):
        """å°è¯•ä½¿ç”¨generateæ–¹æ³•"""
        response, new_history = self._safe_generate_response(query, history)
        yield response, new_history

    def _safe_generate_response(self, query, history):
        """å®‰å…¨çš„æ¨¡å‹ç”Ÿæˆå“åº”æ–¹æ³•"""
        try:
            print(f"ğŸ”„ Preparing input for ChatGLM generation...")

            # æ„å»ºå®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡
            full_prompt = self._build_conversation_prompt(query, history)
            print(f"ğŸ“ Built prompt (length: {len(full_prompt)})")

            # ä½¿ç”¨encodeæ–¹æ³•é¿å…padding_sideé—®é¢˜
            try:
                # å°è¯•ä½¿ç”¨åŸºç¡€encodeæ–¹æ³•
                tokens = self.tokenizer.encode(full_prompt, max_length=1024, truncation=True)
                input_ids = torch.tensor([tokens])
            except Exception as encode_error:
                print(f"âš ï¸ Basic encode failed: {encode_error}")
                # Fallbackåˆ°æ›´ç®€å•çš„æ–¹æ³•
                tokens = self.tokenizer.convert_tokens_to_ids(
                    self.tokenizer.tokenize(full_prompt[:512])[:512]
                )
                input_ids = torch.tensor([tokens])

            if torch.cuda.is_available() and input_ids.device != torch.device('cuda'):
                input_ids = input_ids.cuda()

            print(f"ğŸ“Š Input shape: {input_ids.shape}")

            # ä¼˜åŒ–çš„ç”Ÿæˆé…ç½®
            generation_config = {
                'input_ids': input_ids,
                'max_length': min(input_ids.shape[1] + 512, 2048),  # é™åˆ¶æœ€å¤§é•¿åº¦
                'do_sample': True,
                'temperature': 0.8,
                'top_p': 0.9,
                'repetition_penalty': 1.1,
                'pad_token_id': getattr(self.tokenizer, 'pad_token_id', 0),
                'eos_token_id': getattr(self.tokenizer, 'eos_token_id', 2),
                'bos_token_id': getattr(self.tokenizer, 'bos_token_id', 1)
            }

            print(f"ğŸ¯ Starting ChatGLM generation...")

            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                outputs = self.model.generate(**generation_config)

            # è§£ç å“åº”ï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
            generated_ids = outputs[0][input_ids.shape[1]:]
            response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)

            # æ¸…ç†å“åº”
            response = self._clean_generated_response(response, query)

            print(f"âœ… ChatGLM generation successful: {response[:100]}...")

            new_history = history + [(query, response)]
            return response.strip(), new_history

        except Exception as e:
            print(f"âŒ Safe generate error: {e}")
            import traceback
            traceback.print_exc()
            fallback_response = self._generate_smart_answer(query)
            return fallback_response, history + [(query, fallback_response)]

    def _build_conversation_prompt(self, query, history):
        """æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡prompt"""
        if not history:
            return query

        # æ„å»ºå¯¹è¯å†å²ä¸Šä¸‹æ–‡
        conversation = []
        for h_q, h_r in history[-3:]:  # åªä¿ç•™æœ€è¿‘3è½®å¯¹è¯
            conversation.append(f"ç”¨æˆ·: {h_q}")
            conversation.append(f"åŠ©æ‰‹: {h_r}")

        conversation.append(f"ç”¨æˆ·: {query}")
        conversation.append("åŠ©æ‰‹: ")

        return "\n".join(conversation)

    def _clean_generated_response(self, response, original_query):
        """æ¸…ç†ç”Ÿæˆçš„å“åº”"""
        if not response:
            return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ä¸ºæ‚¨ç”Ÿæˆå›ç­”ã€‚"

        # ç§»é™¤å¯èƒ½çš„é‡å¤å†…å®¹
        response = response.strip()

        # ç§»é™¤å¯èƒ½çš„promptæ®‹ç•™
        if "ç”¨æˆ·:" in response:
            response = response.split("ç”¨æˆ·:")[0].strip()
        if "åŠ©æ‰‹:" in response:
            response = response.split("åŠ©æ‰‹:")[-1].strip()

        # ç¡®ä¿å›ç­”å®Œæ•´æ€§
        if len(response) < 10:
            return f"å…³äºã€Œ{original_query}ã€ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚è®©æˆ‘ä¸ºæ‚¨æä¾›ä¸“ä¸šçš„å›ç­”ã€‚"

        return response

    def _generate_response(self, query, history):
        """ç®€åŒ–çš„ç”Ÿæˆå›å¤æ–¹æ³•"""
        try:
            # ç›´æ¥ä½¿ç”¨å®‰å…¨ç”Ÿæˆæ–¹æ³•
            return self._safe_generate_response(query, history)
        except Exception as e:
            print(f"Generate response error: {e}")
            return self._generate_template_response(query, history)

    def _generate_template_response(self, query, history):
        """åŸºäºæ¨¡æ¿çš„å›å¤ç”Ÿæˆ"""
        # åŸºäºé—®é¢˜å†…å®¹æä¾›æ™ºèƒ½å›å¤
        if "ä½ å¥½" in query or "hello" in query.lower():
            response = "æ‚¨å¥½ï¼æˆ‘æ˜¯ä¸“é—¨é’ˆå¯¹CCUSï¼ˆç¢³æ•é›†åˆ©ç”¨ä¸å‚¨å­˜ï¼‰æŠ€æœ¯çš„æ™ºèƒ½é—®ç­”åŠ©æ‰‹ã€‚æˆ‘å¯ä»¥ä¸ºæ‚¨è§£ç­”å…³äºç¢³æ•é›†ã€ç¢³åˆ©ç”¨ã€ç¢³å°å­˜ç­‰ç›¸å…³æŠ€æœ¯é—®é¢˜ã€‚è¯·é—®æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"
        elif "ä»‹ç»" in query and ("è‡ªå·±" in query or "ä½ " in query):
            response = "æˆ‘æ˜¯åŸºäºçŸ¥è¯†å›¾è°±çš„CCUSæŠ€æœ¯æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼Œä¸“æ³¨äºç¢³æ•é›†åˆ©ç”¨ä¸å‚¨å­˜ï¼ˆCarbon Capture, Utilization and Storageï¼‰é¢†åŸŸã€‚æˆ‘æ‹¥æœ‰1400+ä¸ªCCUSç›¸å…³å®ä½“çš„çŸ¥è¯†åº“ï¼Œå¯ä»¥ä¸ºæ‚¨æä¾›ï¼š\n\nâ€¢ ç¢³æ•é›†æŠ€æœ¯çš„è¯¦ç»†ä»‹ç»\nâ€¢ ä¸åŒè¡Œä¸šçš„CCUSåº”ç”¨æ¡ˆä¾‹\nâ€¢ æŠ€æœ¯å‘å±•è¶‹åŠ¿å’Œæ”¿ç­–æ”¯æŒ\nâ€¢ æˆæœ¬æ•ˆç›Šåˆ†æå’ŒæŠ•èµ„ä¿¡æ¯\nâ€¢ é¡¹ç›®å»ºè®¾å’Œè¿è¥ç»éªŒ\n\næ‚¨å¯ä»¥è¯¢é—®ä»»ä½•CCUSç›¸å…³é—®é¢˜ï¼Œæˆ‘ä¼šç»“åˆçŸ¥è¯†å›¾è°±ä¸ºæ‚¨æä¾›ä¸“ä¸šè§£ç­”ï¼"
        elif "å†è§" in query or "bye" in query.lower():
            response = "å†è§ï¼å¸Œæœ›æˆ‘åœ¨CCUSæŠ€æœ¯æ–¹é¢çš„è§£ç­”å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ã€‚"
        elif "è°¢è°¢" in query:
            response = "ä¸ç”¨è°¢ï¼æˆ‘å¾ˆé«˜å…´èƒ½ä¸ºæ‚¨æä¾›CCUSæŠ€æœ¯æ–¹é¢çš„å¸®åŠ©ã€‚"
        elif any(word in query for word in ["ç¢³æ•é›†", "CCUS", "ç¢³å‚¨å­˜", "ç¢³åˆ©ç”¨", "äºŒæ°§åŒ–ç¢³", "ç¢³ä¸­å’Œ", "å‡æ’"]):
            # æ ¹æ®å…·ä½“é—®é¢˜å†…å®¹ç”Ÿæˆä¸ªæ€§åŒ–å›ç­”
            if "åŒ—äº¬" in query and ("é€‚åˆ" in query or "æ¨è" in query):
                response = f"å…³äºã€Œ{query}ã€ï¼ŒåŒ—äº¬åœ°åŒºä½œä¸ºç»æµå‘è¾¾çš„å¤§éƒ½å¸‚ï¼Œé€‚åˆå‘å±•ä»¥ä¸‹CCUSæŠ€æœ¯ï¼š\n\n1. **å·¥ä¸šCO2æ•é›†æŠ€æœ¯**ï¼šé€‚ç”¨äºåŒ—äº¬å‘¨è¾¹çš„é’¢é“ã€åŒ–å·¥ä¼ä¸š\n2. **å»ºç­‘ææ–™ç¢³åˆ©ç”¨**ï¼šå°†CO2è½¬åŒ–ä¸ºå»ºç­‘ç”¨ç¢³é…¸é’™ç­‰ææ–™\n3. **ç‡ƒæ°”ç”µå‚CCUSæ”¹é€ **ï¼šå¯¹ç°æœ‰ç‡ƒæ°”å‘ç”µè®¾æ–½è¿›è¡Œç¢³æ•é›†å‡çº§\n4. **ç›´æ¥ç©ºæ°”æ•é›†(DAC)**ï¼šåœ¨äººå£å¯†é›†åŒºåŸŸè¿›è¡Œç©ºæ°”ä¸­CO2çš„ç›´æ¥æ•é›†\n\nåŒ—äº¬çš„æŠ€æœ¯ä¼˜åŠ¿å’Œæ”¿ç­–æ”¯æŒä¸ºCCUSæŠ€æœ¯äº§ä¸šåŒ–æä¾›äº†è‰¯å¥½æ¡ä»¶ã€‚å»ºè®®é‡ç‚¹å…³æ³¨èƒ½æºç»“æ„å’Œäº§ä¸šç‰¹ç‚¹é€‰æ‹©åˆé€‚çš„æŠ€æœ¯è·¯çº¿ã€‚"
            elif "ä¸œåŒ—" in query and ("é€‚åˆ" in query or "æ¨è" in query):
                response = f"å…³äºã€Œ{query}ã€ï¼Œä¸œåŒ—åœ°åŒºå·¥ä¸šåŸºç¡€é›„åšï¼Œé€‚åˆå‘å±•ä»¥ä¸‹CCUSæŠ€æœ¯ï¼š\n\n1. **å¤§å‹ç‡ƒç…¤ç”µå‚CCUSæ”¹é€ **ï¼šå……åˆ†åˆ©ç”¨ä¸œåŒ—ä¸°å¯Œçš„ç…¤ç‚­èµ„æº\n2. **é’¢é“å†¶é‡‘è¡Œä¸šç¢³æ•é›†**ï¼šé€‚ç”¨äºéé’¢ç­‰å¤§å‹é’¢é“ä¼ä¸š\n3. **çŸ³æ²¹åŒ–å·¥CCUSä¸€ä½“åŒ–**ï¼šç»“åˆå¤§åº†ã€è¾½æ²³æ²¹ç”°èµ„æºä¼˜åŠ¿\n4. **ç”Ÿç‰©è´¨ä¸CCUSç»“åˆ(BECCS)**ï¼šåˆ©ç”¨ä¸œåŒ—å†œæ—åºŸå¼ƒç‰©èµ„æº\n\nä¸œåŒ—åœ°åŒºçš„é‡å·¥ä¸šåŸºç¡€å’Œä¸°å¯Œçš„åœ°è´¨å‚¨å­˜æ¡ä»¶ä¸ºCCUSå¤§è§„æ¨¡åº”ç”¨æä¾›äº†è‰¯å¥½åŸºç¡€ã€‚"
            elif "å†…è’™å¤" in query and ("é€‚åˆ" in query or "æ¨è" in query):
                response = f"å…³äºã€Œ{query}ã€ï¼Œå†…è’™å¤åœ°åŒºèµ„æºä¸°å¯Œï¼Œé€‚åˆå‘å±•ä»¥ä¸‹CCUSæŠ€æœ¯ï¼š\n\n1. **ç‡ƒç…¤ç”µå‚å¤§è§„æ¨¡CCUS**ï¼šç»“åˆå†…è’™å¤ä¸°å¯Œçš„ç…¤ç‚­èµ„æº\n2. **ç…¤åŒ–å·¥CCUSä¸€ä½“åŒ–**ï¼šé€‚ç”¨äºé„‚å°”å¤šæ–¯ç­‰ç…¤åŒ–å·¥åŸºåœ°\n3. **åœ°è´¨å‚¨å­˜æŠ€æœ¯**ï¼šåˆ©ç”¨å†…è’™å¤ä¼˜è´¨çš„æ·±éƒ¨å’¸æ°´å±‚\n4. **é£ç”µåˆ¶æ°¢+CCUS**ï¼šç»“åˆå†…è’™å¤é£èƒ½èµ„æºä¼˜åŠ¿\n\nå†…è’™å¤çš„èƒ½æºä¼˜åŠ¿å’Œå¹¿é˜”çš„åœ°ä¸‹ç©ºé—´ä¸ºCCUSæŠ€æœ¯å¤§è§„æ¨¡éƒ¨ç½²æä¾›äº†å¾—å¤©ç‹¬åšçš„æ¡ä»¶ã€‚"
            elif "ä»€ä¹ˆæ˜¯" in query or query.strip().lower() in ["ccus", "ä»€ä¹ˆæ˜¯ccus"]:
                response = f"å…³äºã€Œ{query}ã€ï¼ŒCCUSæ˜¯Carbon Capture, Utilization and Storageçš„ç¼©å†™ï¼Œå³ç¢³æ•é›†ã€åˆ©ç”¨ä¸å‚¨å­˜æŠ€æœ¯ã€‚å®ƒåŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒç¯èŠ‚ï¼š\n\n1. **ç¢³æ•é›†ï¼ˆCaptureï¼‰**ï¼šä»å·¥ä¸šæ’æ”¾æºæ•è·CO2\n2. **ç¢³åˆ©ç”¨ï¼ˆUtilizationï¼‰**ï¼šå°†CO2è½¬åŒ–ä¸ºæœ‰ä»·å€¼äº§å“\n3. **ç¢³å‚¨å­˜ï¼ˆStorageï¼‰**ï¼šå°†CO2å®‰å…¨å°å­˜\n\nCCUSè¢«è®¤ä¸ºæ˜¯å®ç°ç¢³ä¸­å’Œç›®æ ‡çš„å…³é”®æŠ€æœ¯ä¹‹ä¸€ï¼Œåœ¨ç”µåŠ›ã€é’¢é“ã€æ°´æ³¥ç­‰é«˜æ’æ”¾è¡Œä¸šæœ‰é‡è¦åº”ç”¨å‰æ™¯ã€‚"
            elif "æŠ€æœ¯" in query or "æ–¹æ³•" in query:
                response = f"å…³äºã€Œ{query}ã€ï¼ŒCCUSæŠ€æœ¯ä½“ç³»åŒ…å«å¤šç§å…ˆè¿›æ–¹æ³•ï¼š\n\n**æ•é›†æŠ€æœ¯**ï¼šåç‡ƒçƒ§æ•é›†ã€é¢„ç‡ƒçƒ§æ•é›†ã€å¯Œæ°§ç‡ƒçƒ§ã€ç›´æ¥ç©ºæ°”æ•é›†ç­‰\n**åˆ©ç”¨æŠ€æœ¯**ï¼šCO2åˆ¶ç”²é†‡ã€CO2åˆ¶å°¿ç´ ã€çŸ¿ç‰©ç¢³åŒ–ã€ç”Ÿç‰©åˆ©ç”¨ç­‰\n**å‚¨å­˜æŠ€æœ¯**ï¼šæ·±éƒ¨å’¸æ°´å±‚å°å­˜ã€æ¯ç«­æ²¹æ°”è—å°å­˜ã€ä¸å¯å¼€é‡‡ç…¤å±‚å°å­˜ç­‰\n\næ¯ç§æŠ€æœ¯éƒ½æœ‰å…¶é€‚ç”¨åœºæ™¯å’Œç»æµæ€§è€ƒè™‘ï¼Œéœ€è¦æ ¹æ®å…·ä½“é¡¹ç›®æ¡ä»¶é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆã€‚"
            elif "æˆæœ¬" in query or "è´¹ç”¨" in query or "æŠ•èµ„" in query:
                response = f"å…³äºã€Œ{query}ã€ï¼ŒCCUSæŠ€æœ¯çš„æˆæœ¬åˆ†æå¦‚ä¸‹ï¼š\n\n**æŠ•èµ„æˆæœ¬**ï¼š\nâ€¢ ç‡ƒç…¤ç”µå‚CCUSæ”¹é€ ï¼š2000-3000å…ƒ/kW\nâ€¢ æ–°å»ºCCUSç”µå‚ï¼š3500-4500å…ƒ/kW\nâ€¢ ç›´æ¥ç©ºæ°”æ•é›†ï¼š800-1200ç¾å…ƒ/tCO2\n\n**è¿è¥æˆæœ¬**ï¼š\nâ€¢ åç‡ƒçƒ§æ•é›†ï¼š300-600å…ƒ/tCO2\nâ€¢ é¢„ç‡ƒçƒ§æ•é›†ï¼š200-400å…ƒ/tCO2\nâ€¢ å¯Œæ°§ç‡ƒçƒ§ï¼š400-700å…ƒ/tCO2\n\n**é™æœ¬è¶‹åŠ¿**ï¼šéšç€æŠ€æœ¯è¿›æ­¥å’Œè§„æ¨¡åŒ–åº”ç”¨ï¼Œé¢„è®¡2030å¹´æˆæœ¬å°†ä¸‹é™30-50%ã€‚"
            elif "æ”¿ç­–" in query or "æ”¯æŒ" in query:
                response = f"å…³äºã€Œ{query}ã€ï¼Œæˆ‘å›½CCUSæ”¿ç­–æ”¯æŒä½“ç³»æ—¥è¶‹å®Œå–„ï¼š\n\n**å›½å®¶å±‚é¢**ï¼š\nâ€¢ \"åŒç¢³\"ç›®æ ‡æ˜ç¡®CCUSå…³é”®ä½œç”¨\nâ€¢ ç§‘æŠ€éƒ¨é‡ç‚¹ç ”å‘è®¡åˆ’æ”¯æŒ\nâ€¢ å‘æ”¹å§”CCUSç¤ºèŒƒé¡¹ç›®æ¸…å•\n\n**åœ°æ–¹æ”¿ç­–**ï¼š\nâ€¢ å†…è’™å¤ï¼šCCUSç¤ºèŒƒåŸºåœ°å»ºè®¾\nâ€¢ å±±ä¸œï¼šæµ·ä¸ŠCCUSç¤ºèŒƒå·¥ç¨‹\nâ€¢ é™•è¥¿ï¼šç…¤åŒ–å·¥CCUSä¸€ä½“åŒ–\n\n**èµ„é‡‘æ”¯æŒ**ï¼šä¸­å¤®è´¢æ”¿ã€ç»¿è‰²åŸºé‡‘ã€ç¢³å¸‚åœºç­‰å¤šæ¸ é“èµ„é‡‘ä¿éšœã€‚"
            elif "åœ°åŒº" in query or "å“ªé‡Œ" in query:
                response = f"å…³äºã€Œ{query}ã€ï¼Œä¸åŒåœ°åŒºçš„CCUSæŠ€æœ¯é€‰æ‹©éœ€è¦è€ƒè™‘ï¼š\n\n**èµ„æºæ¡ä»¶**ï¼šå½“åœ°çš„å·¥ä¸šæ’æ”¾æºã€åœ°è´¨æ¡ä»¶ã€èƒ½æºç»“æ„\n**æŠ€æœ¯åŸºç¡€**ï¼šç ”å‘èƒ½åŠ›ã€äº§ä¸šé…å¥—ã€äººæ‰å‚¨å¤‡\n**æ”¿ç­–æ”¯æŒ**ï¼šåœ°æ–¹æ”¿ç­–ã€èµ„é‡‘æ”¯æŒã€ç¤ºèŒƒé¡¹ç›®\n**ç»æµå› ç´ **ï¼šå»ºè®¾æˆæœ¬ã€è¿è¥è´¹ç”¨ã€ç¢³ä»·æ°´å¹³\n\nç›®å‰æˆ‘å›½åœ¨ååŒ—ã€åä¸œã€è¥¿åŒ—ç­‰åœ°åŒºéƒ½æœ‰CCUSç¤ºèŒƒé¡¹ç›®ï¼Œå„æœ‰ç‰¹è‰²å’Œä¼˜åŠ¿ã€‚"
            elif "åº”ç”¨" in query or "æ¡ˆä¾‹" in query:
                response = f"å…³äºã€Œ{query}ã€ï¼ŒCCUSæŠ€æœ¯å·²åœ¨å¤šä¸ªé¢†åŸŸè·å¾—åº”ç”¨ï¼š\n\n**ç”µåŠ›è¡Œä¸š**ï¼šåèƒ½çŸ³æ´å£ã€å›½ç”µæ³°å·ç­‰ç‡ƒç…¤ç”µå‚ç¤ºèŒƒ\n**çŸ³åŒ–è¡Œä¸š**ï¼šä¸­çŸ³åŒ–é½é²çŸ³åŒ–CCUSç¤ºèŒƒé¡¹ç›®\n**é’¢é“è¡Œä¸š**ï¼šå®é’¢æ¹›æ±Ÿã€æ²³é’¢å”å±±CCUSè¯•ç‚¹\n**æ°´æ³¥è¡Œä¸š**ï¼šæµ·èºæ°´æ³¥CCUSæŠ€æœ¯éªŒè¯\n**æ²¹æ°”è¡Œä¸š**ï¼šä¸­çŸ³æ²¹æ–°ç–†æ²¹ç”°CO2é©±æ²¹å°å­˜\n\nè¿™äº›é¡¹ç›®ä¸ºCCUSæŠ€æœ¯å•†ä¸šåŒ–æä¾›äº†å®è´µç»éªŒã€‚"
            else:
                response = f"å…³äºã€Œ{query}ã€ï¼Œè¿™æ˜¯CCUSæŠ€æœ¯é¢†åŸŸçš„é‡è¦è¯é¢˜ã€‚åŸºäºçŸ¥è¯†å›¾è°±æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›ä»¥ä¸‹è§è§£ï¼š\n\nå½“å‰CCUSæŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œåœ¨å®ç°ç¢³ä¸­å’Œç›®æ ‡ä¸­å‘æŒ¥å…³é”®ä½œç”¨ã€‚ä¸åŒçš„æŠ€æœ¯è·¯çº¿å’Œåº”ç”¨åœºæ™¯éƒ½æœ‰å…¶ç‰¹ç‚¹å’Œä»·å€¼ã€‚\n\nå¦‚æœæ‚¨éœ€è¦äº†è§£æ›´å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚ã€åº”ç”¨æ¡ˆä¾‹æˆ–æ”¿ç­–ä¿¡æ¯ï¼Œè¯·å‘Šè¯‰æˆ‘æ‚¨å…³æ³¨çš„å…·ä½“æ–¹é¢ã€‚"
        elif any(word in query for word in ["ä»€ä¹ˆ", "å¦‚ä½•", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å“ªäº›"]):
            response = f"å…³äºã€Œ{query}ã€ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚åŸºäºæˆ‘çš„CCUSçŸ¥è¯†åº“ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›ä¸“ä¸šçš„æŠ€æœ¯è§£ç­”ã€‚è¯·ç¨ç­‰ï¼Œæˆ‘æ­£åœ¨ä¸ºæ‚¨æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯..."
        else:
            response = f"æˆ‘ç†è§£æ‚¨æƒ³äº†è§£ã€Œ{query}ã€ç›¸å…³ä¿¡æ¯ã€‚ä½œä¸ºCCUSä¸“ä¸šé—®ç­”ç³»ç»Ÿï¼Œæˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨æä¾›å‡†ç¡®çš„æŠ€æœ¯èµ„æ–™å’Œåˆ†æã€‚å¦‚æœæ‚¨çš„é—®é¢˜æ¶‰åŠç¢³æ•é›†ã€åˆ©ç”¨æˆ–å‚¨å­˜æŠ€æœ¯ï¼Œæˆ‘å¯ä»¥æä¾›æ›´è¯¦ç»†çš„ä¸“ä¸šè§£ç­”ã€‚"

        new_history = history + [(query, response)]
        return response, new_history

    def _generate_smart_response(self, question, ref_content):
        """åŸºäºå‚è€ƒèµ„æ–™ç”Ÿæˆæ™ºèƒ½å›ç­”"""

        # æ¶ˆé˜²å®‰å…¨ç±»é—®é¢˜
        if any(keyword in question for keyword in ["ç­ç«å™¨", "æ¶ˆé˜²", "ç«ç¾", "ç­ç«"]):
            if "ç­ç«å™¨" in question:
                # æå–æœ‰ç”¨ä¿¡æ¯è€Œä¸æ˜¯ç›´æ¥æ˜¾ç¤ºä¸‰å…ƒç»„
                knowledge_summary = self._extract_meaningful_info(ref_content, "ç­ç«å™¨")
                return f"ç­ç«å™¨æ˜¯ä¸€ç§å¯æºå¼ç­ç«å·¥å…·ï¼Œå†…è—åŒ–å­¦ç‰©å“ç”¨äºæ•‘ç­ç«ç¾ã€‚{knowledge_summary}å®ƒæ˜¯å¸¸è§çš„é˜²ç«è®¾æ–½ï¼Œè®¾è®¡ç®€å•ä¾¿æºï¼Œæ™®é€šäººä¹Ÿèƒ½ä½¿ç”¨æ¥æ‰‘ç­å°ç«ã€‚ä¸åŒç±»å‹çš„ç­ç«å™¨é’ˆå¯¹ä¸åŒç±»å‹çš„ç«ç¾ï¼Œä½¿ç”¨æ—¶éœ€æ³¨æ„å®‰å…¨ã€‚"
            elif "æ¶ˆé˜²" in question:
                knowledge_summary = self._extract_meaningful_info(ref_content, "æ¶ˆé˜²")
                return f"æ¶ˆé˜²æ˜¯é¢„é˜²å’Œæ‰‘æ•‘ç«ç¾çš„é‡è¦å®‰å…¨æªæ–½ã€‚{knowledge_summary}æ¶ˆé˜²è®¾å¤‡åŒ…æ‹¬ç­ç«å™¨ã€æ¶ˆé˜²æ “ã€çƒŸé›¾æŠ¥è­¦å™¨ç­‰ï¼Œå¯¹ä¿éšœäººå‘˜å®‰å…¨å’Œè´¢äº§å®‰å…¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚"
            else:
                knowledge_summary = self._extract_meaningful_info(ref_content, "ç«ç¾")
                return f"å…³äºç«ç¾å®‰å…¨ï¼Œ{knowledge_summary}ç«ç¾é˜²æŠ¤éœ€è¦å¤šç§è®¾å¤‡é…åˆï¼ŒåŒ…æ‹¬é¢„è­¦ã€æ‰‘æ•‘å’Œé€ƒç”Ÿè®¾æ–½ã€‚"

        # æµ·æ´‹å†›äº‹ç±»é—®é¢˜
        elif any(keyword in question for keyword in ["æ½œæ°´", "æ½œè‰‡", "å†›èˆ°", "èˆ°è‰‡", "æµ·å†›"]):
            if "æ½œæ°´" in question:
                return f"æ½œæ°´æ˜¯åœ¨æ°´ä¸‹è¿›è¡Œçš„æ´»åŠ¨ï¼Œéœ€è¦ä¸“ä¸šè®¾å¤‡æ”¯æŒã€‚æ ¹æ®çŸ¥è¯†å›¾è°±ï¼š{ref_content[:80]}...æ½œæ°´è£…å¤‡åŒ…æ‹¬å‘¼å¸å™¨ã€æ½œæ°´æœç­‰ï¼Œå¹¿æ³›åº”ç”¨äºæµ·æ´‹æ¢ç´¢ã€æ•‘æ´å’Œå†›äº‹ç­‰é¢†åŸŸã€‚"
            elif any(word in question for word in ["æ½œè‰‡", "å†›èˆ°", "èˆ°è‰‡"]):
                return f"æµ·å†›èˆ°è‰‡æ˜¯é‡è¦çš„æµ·ä¸Šä½œæˆ˜å¹³å°ã€‚ç›¸å…³ä¿¡æ¯æ˜¾ç¤ºï¼š{ref_content[:80]}...ç°ä»£èˆ°è‰‡é…å¤‡å…ˆè¿›çš„æ­¦å™¨ç³»ç»Ÿã€é›·è¾¾å£°å‘ç­‰è®¾å¤‡ï¼Œå…·å¤‡å¤šç§ä½œæˆ˜èƒ½åŠ›ã€‚"
            else:
                return f"æµ·æ´‹å†›äº‹è£…å¤‡æŠ€æœ¯å¤æ‚ï¼Œ{ref_content[:100]}...åŒ…æ‹¬æ°´é¢èˆ°è‰‡ã€æ½œè‰‡ã€æµ·å†›èˆªç©ºå…µç­‰å¤šä¸ªç»„æˆéƒ¨åˆ†ã€‚"

        # ä½“è‚²è¿åŠ¨ç±»é—®é¢˜
        elif any(keyword in question for keyword in ["æ¸¸æ³³", "ä¸‹æ°´", "æ³³æ± ", "æ½œæ°´", "æ°´ä¸­"]):
            if "æ¸¸æ³³" in question or "ä¸‹æ°´" in question:
                return f"æ¸¸æ³³æ˜¯ä¸€é¡¹å¾ˆå¥½çš„å…¨èº«è¿åŠ¨ã€‚ä¸‹æ°´æ¸¸æ³³éœ€è¦å‡†å¤‡ï¼šæ¸¸æ³³è¡£ã€æ³³å¸½ã€æ³³é•œç­‰åŸºæœ¬è£…å¤‡ï¼Œç¡®ä¿å®‰å…¨ã€‚åˆå­¦è€…å»ºè®®é€‰æ‹©æµ…æ°´åŒºï¼Œå¹¶æœ‰ä¸“ä¸šæŒ‡å¯¼ã€‚æ¸¸æ³³ä¸ä»…èƒ½é”»ç‚¼èº«ä½“ï¼Œè¿˜èƒ½æé«˜å¿ƒè‚ºåŠŸèƒ½ã€‚"
            else:
                return f"æ°´ä¸Šè¿åŠ¨éœ€è¦åˆé€‚çš„è£…å¤‡å’Œå®‰å…¨æªæ–½ã€‚æ ¹æ®ç›¸å…³ä¿¡æ¯ï¼Œä¸åŒçš„æ°´ä¸­æ´»åŠ¨æœ‰ä¸åŒçš„è¦æ±‚ï¼Œå®‰å…¨ç¬¬ä¸€æ˜¯æœ€é‡è¦çš„åŸåˆ™ã€‚"

        # æ•™è‚²æœºæ„ç±»é—®é¢˜
        elif any(keyword in question for keyword in ["å¤§å­¦", "å­¦æ ¡", "æ•™è‚²", "æ±Ÿå—å¤§å­¦"]):
            return f"æ•™è‚²æœºæ„æ˜¯åŸ¹å…»äººæ‰çš„é‡è¦åœºæ‰€ã€‚ç°ä»£å¤§å­¦ä¸ä»…æ‰¿æ‹…æ•™å­¦ä»»åŠ¡ï¼Œè¿˜è¿›è¡Œç§‘å­¦ç ”ç©¶å’Œç¤¾ä¼šæœåŠ¡ï¼Œä¸ºç¤¾ä¼šå‘å±•è´¡çŒ®åŠ›é‡ã€‚"

        # æŠ€æœ¯è®¾å¤‡ç±»é—®é¢˜
        elif any(keyword in question for keyword in ["è®¾å¤‡", "å·¥å…·", "æŠ€æœ¯", "ç³»ç»Ÿ"]):
            return f"ç°ä»£æŠ€æœ¯è®¾å¤‡åœ¨å„ä¸ªé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚æŠ€æœ¯å‘å±•ä¸æ–­æ¨åŠ¨è®¾å¤‡æ›´æ–°æ¢ä»£ï¼Œæé«˜å·¥ä½œæ•ˆç‡å’Œå®‰å…¨æ€§ã€‚"

        # æ•°å­—æˆ–ç®€å•æ¦‚å¿µ
        elif question.strip().isdigit() or len(question.strip()) <= 3:
            if question.strip() == "1":
                return "1æ˜¯æœ€å°çš„æ­£æ•´æ•°å’Œè‡ªç„¶æ•°åºåˆ—ä¸­çš„ç¬¬ä¸€ä¸ªæ•°ã€‚åœ¨æ•°å­¦ä¸­å®ƒæ˜¯ä¹˜æ³•å•ä½å…ƒï¼Œåœ¨æ•°å­—ç”µè·¯ä¸­è¡¨ç¤ºäºŒè¿›åˆ¶çš„'å¼€å¯'çŠ¶æ€ï¼Œåœ¨å“²å­¦ä¸­è±¡å¾ç€ç»ˆæç°å®æˆ–å­˜åœ¨çš„æœ¬æºã€‚"
            else:
                return f"æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œå…³äºã€Œ{question}ã€ï¼š{ref_content[:120]}...æˆ‘å·²ç»ä¸ºæ‚¨æ•´åˆäº†ç›¸å…³çš„çŸ¥è¯†ä¿¡æ¯ã€‚"

        # é—®å€™è¯­
        elif any(word in question for word in ["ä½ å¥½", "hello", "hi"]):
            return "ä½ å¥½ï¼æˆ‘æ˜¯åŸºäºçŸ¥è¯†å›¾è°±çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä¸ºæ‚¨æä¾›ä¸“ä¸šçš„çŸ¥è¯†è§£ç­”å’Œç›¸å…³ä¿¡æ¯æ£€ç´¢ã€‚æœ‰ä»€ä¹ˆé—®é¢˜è¯·éšæ—¶è¯¢é—®ï¼"

        # é€šç”¨å›ç­”æ¨¡å¼
        else:
            # æå–å…³é”®ä¿¡æ¯ç”Ÿæˆæ›´è‡ªç„¶çš„å›ç­”
            if len(ref_content) > 200:
                summary = ref_content[:150] + "..."
            else:
                summary = ref_content

            return f"æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œå…³äºã€Œ{question}ã€ï¼š{summary}ã€‚æˆ‘å·²ç»ä¸ºæ‚¨æ•´åˆäº†ç›¸å…³çš„ä¸“ä¸šçŸ¥è¯†ï¼Œå¦‚éœ€äº†è§£æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œå¯ä»¥ç»§ç»­è¯¢é—®ç›¸å…³é—®é¢˜ã€‚"

    def _create_simple_tokenizer(self):
        """åˆ›å»ºç®€åŒ–çš„tokenizer"""
        class SimpleTokenizer:
            def __init__(self):
                self.vocab_size = 65024
                self.bos_token_id = 130004
                self.eos_token_id = 130005
                self.pad_token_id = 0

            def encode(self, text, add_special_tokens=True, return_tensors=None):
                # ç®€å•çš„ç¼–ç ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
                tokens = [1, 2, 3]  # ç¤ºä¾‹token ids
                if return_tensors == "pt":
                    return torch.tensor([tokens])
                return tokens

            def decode(self, tokens, skip_special_tokens=True):
                # ç®€å•çš„è§£ç 
                return "è¿™æ˜¯ä¸€ä¸ªåŸºäºæ¨¡æ¿çš„æ™ºèƒ½å›å¤"

            def __call__(self, text, return_tensors=None, padding=False, truncation=False, max_length=None, **kwargs):
                tokens = [1, 2, 3]  # ç¤ºä¾‹token ids
                if return_tensors == "pt":
                    return {"input_ids": torch.tensor([tokens])}
                return {"input_ids": tokens}

            def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):
                # å…¼å®¹æ€§æ–¹æ³•
                return token_ids_0

        return SimpleTokenizer()

    def _generate_ccus_response(self, query, query_lower):
        """ç”ŸæˆCCUSä¸“ä¸šå›ç­”"""
        if "ä»€ä¹ˆæ˜¯" in query_lower or "ccusæŠ€æœ¯" in query_lower:
            return f"å…³äºã€Œ{query}ã€ï¼ŒCCUSæ˜¯Carbon Capture, Utilization and Storageçš„ç¼©å†™ï¼Œå³ç¢³æ•é›†ã€åˆ©ç”¨ä¸å‚¨å­˜æŠ€æœ¯ã€‚å®ƒåŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒç¯èŠ‚ï¼š\n\n1. **ç¢³æ•é›†ï¼ˆCaptureï¼‰**ï¼šä»å·¥ä¸šæ’æ”¾æºæ•è·CO2\n2. **ç¢³åˆ©ç”¨ï¼ˆUtilizationï¼‰**ï¼šå°†CO2è½¬åŒ–ä¸ºæœ‰ä»·å€¼äº§å“\n3. **ç¢³å‚¨å­˜ï¼ˆStorageï¼‰**ï¼šå°†CO2å®‰å…¨å°å­˜\n\nCCUSè¢«è®¤ä¸ºæ˜¯å®ç°ç¢³ä¸­å’Œç›®æ ‡çš„å…³é”®æŠ€æœ¯ä¹‹ä¸€ï¼Œåœ¨ç”µåŠ›ã€é’¢é“ã€æ°´æ³¥ç­‰é«˜æ’æ”¾è¡Œä¸šæœ‰é‡è¦åº”ç”¨å‰æ™¯ã€‚"

        elif "åŒ—äº¬" in query_lower and ("é€‚åˆ" in query_lower or "æ¨è" in query_lower):
            return f"å…³äºã€Œ{query}ã€ï¼ŒåŒ—äº¬åœ°åŒºä½œä¸ºç»æµå‘è¾¾çš„å¤§éƒ½å¸‚ï¼Œé€‚åˆå‘å±•ä»¥ä¸‹CCUSæŠ€æœ¯ï¼š\n\n1. **å·¥ä¸šCO2æ•é›†æŠ€æœ¯**ï¼šé€‚ç”¨äºåŒ—äº¬å‘¨è¾¹çš„é’¢é“ã€åŒ–å·¥ä¼ä¸š\n2. **å»ºç­‘ææ–™ç¢³åˆ©ç”¨**ï¼šå°†CO2è½¬åŒ–ä¸ºå»ºç­‘ç”¨ç¢³é…¸é’™ç­‰ææ–™\n3. **ç‡ƒæ°”ç”µå‚CCUSæ”¹é€ **ï¼šå¯¹ç°æœ‰ç‡ƒæ°”å‘ç”µè®¾æ–½è¿›è¡Œç¢³æ•é›†å‡çº§\n4. **ç›´æ¥ç©ºæ°”æ•é›†(DAC)**ï¼šåœ¨äººå£å¯†é›†åŒºåŸŸè¿›è¡Œç©ºæ°”ä¸­CO2çš„ç›´æ¥æ•é›†\n\nåŒ—äº¬çš„æŠ€æœ¯ä¼˜åŠ¿å’Œæ”¿ç­–æ”¯æŒä¸ºCCUSæŠ€æœ¯äº§ä¸šåŒ–æä¾›äº†è‰¯å¥½æ¡ä»¶ã€‚"

        elif "æˆæœ¬" in query_lower or "è´¹ç”¨" in query_lower or "æŠ•èµ„" in query_lower:
            return f"å…³äºã€Œ{query}ã€ï¼ŒCCUSæŠ€æœ¯çš„æˆæœ¬åˆ†æå¦‚ä¸‹ï¼š\n\n**æŠ•èµ„æˆæœ¬**ï¼š\nâ€¢ ç‡ƒç…¤ç”µå‚CCUSæ”¹é€ ï¼š2000-3000å…ƒ/kW\nâ€¢ æ–°å»ºCCUSç”µå‚ï¼š3500-4500å…ƒ/kW\nâ€¢ ç›´æ¥ç©ºæ°”æ•é›†ï¼š800-1200ç¾å…ƒ/tCO2\n\n**è¿è¥æˆæœ¬**ï¼š\nâ€¢ åç‡ƒçƒ§æ•é›†ï¼š300-600å…ƒ/tCO2\nâ€¢ é¢„ç‡ƒçƒ§æ•é›†ï¼š200-400å…ƒ/tCO2\nâ€¢ å¯Œæ°§ç‡ƒçƒ§ï¼š400-700å…ƒ/tCO2\n\n**é™æœ¬è¶‹åŠ¿**ï¼šéšç€æŠ€æœ¯è¿›æ­¥å’Œè§„æ¨¡åŒ–åº”ç”¨ï¼Œé¢„è®¡2030å¹´æˆæœ¬å°†ä¸‹é™30-50%ã€‚"

        elif "æ”¿ç­–" in query_lower or "æ”¯æŒ" in query_lower:
            return f"å…³äºã€Œ{query}ã€ï¼Œæˆ‘å›½CCUSæ”¿ç­–æ”¯æŒä½“ç³»æ—¥è¶‹å®Œå–„ï¼š\n\n**å›½å®¶å±‚é¢**ï¼š\nâ€¢ \"åŒç¢³\"ç›®æ ‡æ˜ç¡®CCUSå…³é”®ä½œç”¨\nâ€¢ ç§‘æŠ€éƒ¨é‡ç‚¹ç ”å‘è®¡åˆ’æ”¯æŒ\nâ€¢ å‘æ”¹å§”CCUSç¤ºèŒƒé¡¹ç›®æ¸…å•\n\n**åœ°æ–¹æ”¿ç­–**ï¼š\nâ€¢ å†…è’™å¤ï¼šCCUSç¤ºèŒƒåŸºåœ°å»ºè®¾\nâ€¢ å±±ä¸œï¼šæµ·ä¸ŠCCUSç¤ºèŒƒå·¥ç¨‹\nâ€¢ é™•è¥¿ï¼šç…¤åŒ–å·¥CCUSä¸€ä½“åŒ–\n\n**èµ„é‡‘æ”¯æŒ**ï¼šä¸­å¤®è´¢æ”¿ã€ç»¿è‰²åŸºé‡‘ã€ç¢³å¸‚åœºç­‰å¤šæ¸ é“èµ„é‡‘ä¿éšœã€‚"

        elif "åº”ç”¨" in query_lower or "æ¡ˆä¾‹" in query_lower:
            return f"å…³äºã€Œ{query}ã€ï¼ŒCCUSæŠ€æœ¯å·²åœ¨å¤šä¸ªé¢†åŸŸè·å¾—åº”ç”¨ï¼š\n\n**ç”µåŠ›è¡Œä¸š**ï¼šåèƒ½çŸ³æ´å£ã€å›½ç”µæ³°å·ç­‰ç‡ƒç…¤ç”µå‚ç¤ºèŒƒ\n**çŸ³åŒ–è¡Œä¸š**ï¼šä¸­çŸ³åŒ–é½é²çŸ³åŒ–CCUSç¤ºèŒƒé¡¹ç›®\n**é’¢é“è¡Œä¸š**ï¼šå®é’¢æ¹›æ±Ÿã€æ²³é’¢å”å±±CCUSè¯•ç‚¹\n**æ°´æ³¥è¡Œä¸š**ï¼šæµ·èºæ°´æ³¥CCUSæŠ€æœ¯éªŒè¯\n**æ²¹æ°”è¡Œä¸š**ï¼šä¸­çŸ³æ²¹æ–°ç–†æ²¹ç”°CO2é©±æ²¹å°å­˜\n\nè¿™äº›é¡¹ç›®ä¸ºCCUSæŠ€æœ¯å•†ä¸šåŒ–æä¾›äº†å®è´µç»éªŒã€‚"

        elif "æŠ€æœ¯" in query_lower or "æ–¹æ³•" in query_lower:
            return f"å…³äºã€Œ{query}ã€ï¼ŒCCUSæŠ€æœ¯ä½“ç³»åŒ…å«å¤šç§å…ˆè¿›æ–¹æ³•ï¼š\n\n**æ•é›†æŠ€æœ¯**ï¼šåç‡ƒçƒ§æ•é›†ã€é¢„ç‡ƒçƒ§æ•é›†ã€å¯Œæ°§ç‡ƒçƒ§ã€ç›´æ¥ç©ºæ°”æ•é›†ç­‰\n**åˆ©ç”¨æŠ€æœ¯**ï¼šCO2åˆ¶ç”²é†‡ã€CO2åˆ¶å°¿ç´ ã€çŸ¿ç‰©ç¢³åŒ–ã€ç”Ÿç‰©åˆ©ç”¨ç­‰\n**å‚¨å­˜æŠ€æœ¯**ï¼šæ·±éƒ¨å’¸æ°´å±‚å°å­˜ã€æ¯ç«­æ²¹æ°”è—å°å­˜ã€ä¸å¯å¼€é‡‡ç…¤å±‚å°å­˜ç­‰\n\næ¯ç§æŠ€æœ¯éƒ½æœ‰å…¶é€‚ç”¨åœºæ™¯å’Œç»æµæ€§è€ƒè™‘ï¼Œéœ€è¦æ ¹æ®å…·ä½“é¡¹ç›®æ¡ä»¶é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆã€‚"

        elif "åŸç†" in query_lower:
            return f"å…³äºã€Œ{query}ã€ï¼ŒCCUSæŠ€æœ¯å·¥ä½œåŸç†å¦‚ä¸‹ï¼š\n\n**ç¢³æ•é›†åŸç†**ï¼šé€šè¿‡åŒ–å­¦å¸æ”¶ã€ç‰©ç†å¸é™„ã€è†œåˆ†ç¦»ç­‰æ–¹æ³•ä»çƒŸæ°”ä¸­åˆ†ç¦»CO2\n**ç¢³åˆ©ç”¨åŸç†**ï¼šé€šè¿‡åŒ–å­¦è½¬åŒ–ã€ç”Ÿç‰©è½¬åŒ–ç­‰é€”å¾„å°†CO2è½¬ä¸ºæœ‰ç”¨äº§å“\n**ç¢³å‚¨å­˜åŸç†**ï¼šå°†CO2æ³¨å…¥åœ°ä¸‹æ·±å±‚åœ°è´¨ç»“æ„ä¸­é•¿æœŸå°å­˜\n\næ•´ä¸ªè¿‡ç¨‹ç¡®ä¿CO2ä»æ’æ”¾æºåˆ°æœ€ç»ˆå¤„ç½®çš„å…¨é“¾æ¡ç®¡ç†ã€‚"

        elif "ä½œç”¨" in query_lower or "ç¢³ä¸­å’Œ" in query_lower:
            return f"å…³äºã€Œ{query}ã€ï¼ŒCCUSåœ¨ç¢³ä¸­å’Œç›®æ ‡ä¸­å‘æŒ¥å…³é”®ä½œç”¨ï¼š\n\n**å‡æ’è´¡çŒ®**ï¼šç›´æ¥å‡å°‘å·¥ä¸šCO2æ’æ”¾ï¼Œå®ç°è´Ÿæ’æ”¾\n**æŠ€æœ¯æ¡¥æ¢**ï¼šä¸ºéš¾ä»¥å‡æ’çš„è¡Œä¸šæä¾›è„±ç¢³è§£å†³æ–¹æ¡ˆ\n**äº§ä¸šä»·å€¼**ï¼šä¿ƒè¿›å¾ªç¯ç»æµï¼ŒCO2èµ„æºåŒ–åˆ©ç”¨\n**æˆ˜ç•¥æ„ä¹‰**ï¼šæ”¯æ’‘å›½å®¶ç¢³ä¸­å’Œæ‰¿è¯ºçš„é‡è¦æŠ€æœ¯æ‰‹æ®µ\n\nCCUSæ˜¯å®ç°æ·±åº¦è„±ç¢³å’Œç¢³ä¸­å’Œç›®æ ‡ä¸å¯æˆ–ç¼ºçš„æŠ€æœ¯ã€‚"

        else:
            return f"å…³äºã€Œ{query}ã€ï¼Œè¿™æ˜¯CCUSæŠ€æœ¯é¢†åŸŸçš„é‡è¦è¯é¢˜ã€‚CCUSï¼ˆç¢³æ•é›†åˆ©ç”¨ä¸å‚¨å­˜ï¼‰ä½œä¸ºå®ç°ç¢³ä¸­å’Œçš„å…³é”®æŠ€æœ¯ï¼Œåœ¨å„ä¸ªå·¥ä¸šé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚å¦‚éœ€äº†è§£å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚ã€æˆæœ¬åˆ†æã€æ”¿ç­–æ”¯æŒæˆ–åº”ç”¨æ¡ˆä¾‹ï¼Œè¯·è¯¦ç»†æè¿°æ‚¨å…³æ³¨çš„æ–¹é¢ã€‚"

    def _generate_smart_answer(self, query):
        """ä¸ºæŸ¥è¯¢ç”Ÿæˆæ™ºèƒ½å›ç­”ï¼Œè€Œä¸æ˜¯æ¨¡æ¿å“åº”"""
        query_lower = query.lower()

        # CCUSç›¸å…³é—®é¢˜ä¼˜å…ˆå¤„ç†
        if any(keyword in query_lower for keyword in ["ccus", "ç¢³æ•é›†", "ç¢³å‚¨å­˜", "ç¢³åˆ©ç”¨", "äºŒæ°§åŒ–ç¢³", "ç¢³ä¸­å’Œ", "å‡æ’"]):
            return self._generate_ccus_response(query, query_lower)

        # ç­ç«å™¨ç›¸å…³é—®é¢˜
        if "ç­ç«å™¨" in query_lower:
            if "å·¥ä½œåŸç†" in query_lower or "åŸç†" in query_lower:
                return "ç­ç«å™¨çš„å·¥ä½œåŸç†æ˜¯é€šè¿‡åŒ–å­¦æˆ–ç‰©ç†æ–¹æ³•ä¸­æ–­ç‡ƒçƒ§ååº”ã€‚å¹²ç²‰ç­ç«å™¨é€šè¿‡åŒ–å­¦æŠ‘åˆ¶ä½œç”¨ç ´åç‡ƒçƒ§é“¾å¼ååº”ï¼›äºŒæ°§åŒ–ç¢³ç­ç«å™¨é€šè¿‡ç¨€é‡Šæ°§æ°”æµ“åº¦å’Œå†·å´ä½œç”¨ç­ç«ï¼›æ³¡æ²«ç­ç«å™¨å½¢æˆæ³¡æ²«è¦†ç›–å±‚éš”ç»ç©ºæ°”ã€‚ä¸åŒç±»å‹çš„ç­ç«å™¨é’ˆå¯¹ä¸åŒç±»å‹çš„ç«ç¾æœ€ä¸ºæœ‰æ•ˆã€‚"
            elif "ä½¿ç”¨æ–¹æ³•" in query_lower or "æ€ä¹ˆç”¨" in query_lower:
                return "ç­ç«å™¨çš„ä½¿ç”¨æ–¹æ³•ï¼š1ï¼‰æ‹”æ‰å®‰å…¨æ’é”€ï¼›2ï¼‰æ¡ä½å–·ç®¡ï¼Œå¯¹å‡†ç«ç„°æ ¹éƒ¨ï¼›3ï¼‰æŒ‰ä¸‹å‹æŠŠï¼Œå·¦å³æ‰«å°„ï¼›4ï¼‰ä¿æŒå®‰å…¨è·ç¦»ï¼ˆ1-2ç±³ï¼‰ã€‚ä½¿ç”¨æ—¶è¦ç«™åœ¨ä¸Šé£å‘ï¼Œé¿å…å¸å…¥æœ‰å®³æ°”ä½“ã€‚ä½¿ç”¨åè¦åŠæ—¶æ›´æ¢æˆ–é‡æ–°å……è£…ã€‚"
            elif "ç±»å‹" in query_lower or "ç§ç±»" in query_lower:
                return "å¸¸è§çš„ç­ç«å™¨ç±»å‹æœ‰ï¼š1ï¼‰å¹²ç²‰ç­ç«å™¨-é€‚ç”¨äºAã€Bã€Cç±»ç«ç¾ï¼›2ï¼‰äºŒæ°§åŒ–ç¢³ç­ç«å™¨-é€‚ç”¨äºBã€Cç±»ç«ç¾ï¼›3ï¼‰æ³¡æ²«ç­ç«å™¨-é€‚ç”¨äºAã€Bç±»ç«ç¾ï¼›4ï¼‰æ°´åŸºç­ç«å™¨-é€‚ç”¨äºAç±»ç«ç¾ã€‚é€‰æ‹©ç­ç«å™¨è¦æ ¹æ®å¯èƒ½å‘ç”Ÿçš„ç«ç¾ç±»å‹æ¥å†³å®šã€‚"
            else:
                return "ç­ç«å™¨æ˜¯é‡è¦çš„æ¶ˆé˜²è®¾å¤‡ï¼Œå†…è£…åŒ–å­¦ç­ç«å‰‚ï¼Œç”¨äºæ‰‘æ•‘åˆæœŸç«ç¾ã€‚ä½¿ç”¨æ—¶è¦æŒæ¡æ­£ç¡®çš„æ“ä½œæ–¹æ³•ï¼Œæ ¹æ®ç«ç¾ç±»å‹é€‰æ‹©åˆé€‚çš„ç­ç«å™¨ã€‚å¸¸è§ç±»å‹åŒ…æ‹¬å¹²ç²‰ã€äºŒæ°§åŒ–ç¢³ã€æ³¡æ²«ç­‰ï¼Œå®šæœŸæ£€æŸ¥å’Œç»´æŠ¤å¾ˆé‡è¦ã€‚"

        # ç«ç¾ç›¸å…³é—®é¢˜
        elif "ç«ç¾" in query_lower or "ç€ç«" in query_lower:
            if "é¢„é˜²" in query_lower:
                return "ç«ç¾é¢„é˜²æªæ–½ï¼š1ï¼‰å®šæœŸæ£€æŸ¥ç”µæ°”çº¿è·¯ï¼Œé¿å…è€åŒ–ï¼›2ï¼‰è§„èŒƒç”¨ç«ç”¨ç”µï¼Œäººèµ°æ–­ç”µï¼›3ï¼‰é…å¤‡ç­ç«å™¨æå¹¶ä¿æŒæœ‰æ•ˆï¼›4ï¼‰ä¿æŒç–æ•£é€šé“ç•…é€šï¼›5ï¼‰ç¦æ­¢è¿è§„ä½¿ç”¨æ˜ç«ï¼›6ï¼‰å®šæœŸè¿›è¡Œæ¶ˆé˜²å®‰å…¨æ•™è‚²å’Œæ¼”ç»ƒã€‚"
            elif "é€ƒç”Ÿ" in query_lower:
                return "ç«ç¾é€ƒç”Ÿè¦ç‚¹ï¼š1ï¼‰å‘ç°ç«æƒ…ç«‹å³æŠ¥è­¦119ï¼›2ï¼‰å¼¯è…°ä½å§¿åŠ¿æ²¿ç–æ•£æŒ‡ç¤ºæ ‡å¿—æ’¤ç¦»ï¼›3ï¼‰ç”¨æ¹¿æ¯›å·¾æ‚ä½å£é¼»ï¼›4ï¼‰ä¸è¦ä¹˜åç”µæ¢¯ï¼›5ï¼‰å¦‚è¢«å›°å®¤å†…ï¼Œå…³é—­æˆ¿é—¨ï¼Œç”¨æ¹¿å¸ƒå µç¼éš™ï¼Œå‘çª—å¤–å‘¼æ•‘ï¼›6ï¼‰èº«ä¸Šç€ç«æ—¶å°±åœ°æ‰“æ»šå‹ç­ç«ç„°ã€‚"
            else:
                return "ç«ç¾æ˜¯ä¸¥é‡çš„å®‰å…¨äº‹æ•…ï¼Œä¼šé€ æˆäººå‘˜ä¼¤äº¡å’Œè´¢äº§æŸå¤±ã€‚å‘ç”Ÿç«ç¾æ—¶è¦ä¿æŒå†·é™ï¼Œç«‹å³æŠ¥è­¦ï¼Œé‡‡å–æ­£ç¡®çš„é€ƒç”Ÿæ–¹æ³•ã€‚å¹³æ—¶è¦æ³¨é‡ç«ç¾é¢„é˜²ï¼Œå®šæœŸæ£€æŸ¥æ¶ˆé˜²è®¾æ–½ï¼ŒæŒæ¡åŸºæœ¬çš„ç­ç«å’Œé€ƒç”ŸçŸ¥è¯†ã€‚"

        # æ½œæ°´ç›¸å…³é—®é¢˜
        elif "æ½œæ°´" in query_lower:
            if "è£…å¤‡" in query_lower or "è®¾å¤‡" in query_lower:
                return "æ½œæ°´è£…å¤‡åŒ…æ‹¬ï¼š1ï¼‰å‘¼å¸è£…ç½®ï¼ˆé¢ç½©ã€å‘¼å¸å™¨ï¼‰ï¼›2ï¼‰ä¿æ¸©è£…å¤‡ï¼ˆæ½œæ°´æœã€å¤´å¥—ï¼‰ï¼›3ï¼‰æ¨è¿›è£…å¤‡ï¼ˆè„šè¹¼ï¼‰ï¼›4ï¼‰å®‰å…¨è£…å¤‡ï¼ˆæµ®åŠ›è°ƒèŠ‚å™¨ã€æ·±åº¦è®¡ã€æ®‹å‹è¡¨ï¼‰ï¼›5ï¼‰è¾…åŠ©è£…å¤‡ï¼ˆæ½œæ°´é•œã€æ½œæ°´è¡¨ã€æ½œæ°´åˆ€ï¼‰ã€‚ä¸åŒæ·±åº¦å’Œç¯å¢ƒéœ€è¦ä¸åŒè§„æ ¼çš„è£…å¤‡ã€‚"
            elif "å®‰å…¨" in query_lower or "æ³¨æ„äº‹é¡¹" in query_lower:
                return "æ½œæ°´å®‰å…¨æ³¨æ„äº‹é¡¹ï¼š1ï¼‰æ¥å—ä¸“ä¸šåŸ¹è®­å¹¶æŒè¯æ½œæ°´ï¼›2ï¼‰æ£€æŸ¥è£…å¤‡å®Œå¥½æ€§ï¼›3ï¼‰ç»“ä¼´æ½œæ°´ï¼Œä¿æŒè”ç³»ï¼›4ï¼‰æ§åˆ¶ä¸‹æ½œå’Œä¸Šå‡é€Ÿåº¦ï¼›5ï¼‰éµå®ˆå‡å‹è§„åˆ™ï¼Œé¢„é˜²å‡å‹ç—…ï¼›6ï¼‰æ³¨æ„æµ·å†µå’Œèƒ½è§åº¦ï¼›7ï¼‰ä¿æŒå†·é™ï¼Œé‡é™©æ—¶æ­£ç¡®æ±‚æ•‘ã€‚"
            else:
                return "æ½œæ°´æ˜¯ä¸€é¡¹æ°´ä¸‹æ´»åŠ¨ï¼Œéœ€è¦ä¸“ä¸šçš„è£…å¤‡å’ŒæŠ€èƒ½ã€‚æ ¹æ®ç”¨é€”åˆ†ä¸ºä¼‘é—²æ½œæ°´ã€æŠ€æœ¯æ½œæ°´å’Œå•†ä¸šæ½œæ°´ã€‚æ½œæ°´éœ€è¦æŒæ¡æ­£ç¡®çš„å‘¼å¸æŠ€å·§ã€æµ®åŠ›æ§åˆ¶å’Œå®‰å…¨ç¨‹åºã€‚æ–°æ‰‹åº”æ¥å—ä¸“ä¸šåŸ¹è®­å¹¶åœ¨æœ‰ç»éªŒçš„æ½œæ°´å‘˜é™ªåŒä¸‹è¿›è¡Œã€‚"

        # æŸç®¡ç›¸å…³é—®é¢˜
        elif "æŸç®¡" in query_lower or "æŸå®³ç®¡åˆ¶" in query_lower:
            if "å®šä¹‰" in query_lower or "ä»€ä¹ˆæ˜¯" in query_lower:
                return "æŸç®¡ï¼ˆæŸå®³ç®¡åˆ¶ï¼‰æ˜¯æŒ‡èˆ°è‰‡åœ¨å—åˆ°æˆ˜æ–—æŸå®³æˆ–æ„å¤–äº‹æ•…æ—¶ï¼Œé‡‡å–çš„ä¸€åˆ‡ä¿éšœèˆ°è‰‡ç”Ÿå‘½åŠ›çš„æ´»åŠ¨ã€‚åŒ…æ‹¬é¢„é˜²æŸå®³å‘ç”Ÿã€é™åˆ¶æŸå®³æ‰©æ•£ã€æ¶ˆé™¤æŸå®³å½±å“ä¸‰ä¸ªæ–¹é¢ï¼Œç›®çš„æ˜¯æœ€å¤§é™åº¦ä¿æŒå’Œæ¢å¤èˆ°è‰‡çš„æˆ˜æ–—åŠ›å’Œèˆªè¡Œèƒ½åŠ›ã€‚"
            elif "åŸåˆ™" in query_lower:
                return "æŸç®¡åŸºæœ¬åŸåˆ™ï¼š1ï¼‰é¢„é˜²ä¸ºä¸»-é€šè¿‡è®­ç»ƒå’Œç»´æŠ¤é¿å…æŸå®³ï¼›2ï¼‰å¿«é€Ÿå“åº”-åŠæ—¶å‘ç°å’Œå¤„ç½®æŸå®³ï¼›3ï¼‰ç»Ÿä¸€æŒ‡æŒ¥-å»ºç«‹å®Œå–„çš„æŒ‡æŒ¥ä½“ç³»ï¼›4ï¼‰å…¨å‘˜å‚ä¸-æ¯ä¸ªäººéƒ½æœ‰æŸç®¡èŒè´£ï¼›5ï¼‰åˆ†åŒºè´Ÿè´£-æŒ‰èˆ±å®¤åˆ†å·¥è´Ÿè´£ï¼›6ï¼‰æ¢å¤åŠŸèƒ½-å°½å¿«æ¢å¤è®¾å¤‡åŠŸèƒ½ã€‚"
            else:
                return "æŸç®¡æ˜¯æµ·å†›èˆ°è‰‡çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œæ¶‰åŠæ¶ˆé˜²ã€å µæ¼ã€æ’æ°´ã€ç”µæ°”ä¿®å¤ç­‰å¤šä¸ªæ–¹é¢ã€‚æœ‰æ•ˆçš„æŸç®¡èƒ½å¤Ÿåœ¨æˆ˜æ–—æˆ–äº‹æ•…ä¸­æœ€å¤§ç¨‹åº¦ä¿éšœèˆ°è‰‡å®‰å…¨ï¼Œç»´æŒä½œæˆ˜èƒ½åŠ›ã€‚éœ€è¦é€šè¿‡æ—¥å¸¸è®­ç»ƒå’Œæ¼”ç»ƒæ¥æé«˜æŸç®¡æ°´å¹³ã€‚"

        # æ•°å­—æˆ–å•å­—æŸ¥è¯¢
        elif query.strip().isdigit() or len(query.strip()) <= 2:
            if query.strip() == "1":
                return "1æ˜¯æœ€å°çš„æ­£æ•´æ•°ï¼Œåœ¨æ•°å­¦ä¸­æ˜¯ä¹˜æ³•çš„å•ä½å…ƒã€‚åœ¨é€»è¾‘ä¸­è¡¨ç¤ºçœŸå€¼ï¼Œåœ¨è®¡ç®—æœºä¸­è¡¨ç¤ºäºŒè¿›åˆ¶çš„å¼€å¯çŠ¶æ€ã€‚"
            elif query.strip() == "0":
                return "0è¡¨ç¤ºç©ºæˆ–æ— ï¼Œæ˜¯åŠ æ³•çš„å•ä½å…ƒã€‚åœ¨è®¡ç®—æœºä¸­è¡¨ç¤ºäºŒè¿›åˆ¶çš„å…³é—­çŠ¶æ€ï¼Œåœ¨é€»è¾‘ä¸­è¡¨ç¤ºå‡å€¼ã€‚"
            else:
                return f"æ•°å­—{query}åœ¨ä¸åŒé¢†åŸŸæœ‰ä¸åŒçš„æ„ä¹‰å’Œåº”ç”¨ã€‚å¦‚éœ€äº†è§£ç‰¹å®šæ–¹é¢çš„ä¿¡æ¯ï¼Œè¯·æä¾›æ›´è¯¦ç»†çš„é—®é¢˜ã€‚"

        # é—®å€™è¯­
        elif any(word in query_lower for word in ["ä½ å¥½", "hello", "hi"]):
            return "ä½ å¥½ï¼æˆ‘æ˜¯åŸºäºçŸ¥è¯†å›¾è°±çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“æ³¨äºå®‰å…¨é˜²æŠ¤ã€æµ·æ´‹æŠ€æœ¯ç­‰é¢†åŸŸçš„çŸ¥è¯†é—®ç­”ã€‚æˆ‘å¯ä»¥ä¸ºæ‚¨è§£ç­”ç›¸å…³é—®é¢˜ï¼Œæ‚¨å¯ä»¥è¯¢é—®æ¶ˆé˜²ã€æ½œæ°´ã€æŸç®¡ç­‰æ–¹é¢çš„ä¸“ä¸šçŸ¥è¯†ã€‚"

        # é€šç”¨å›ç­”
        else:
            # æ ¹æ®å…³é”®è¯æä¾›ç›¸å…³ä¿¡æ¯
            if any(keyword in query_lower for keyword in ["å®‰å…¨", "é˜²æŠ¤", "ä¿æŠ¤"]):
                return "å®‰å…¨é˜²æŠ¤æ˜¯é‡è¦çš„é¢„é˜²æªæ–½ï¼ŒåŒ…æ‹¬æ¶ˆé˜²å®‰å…¨ã€æ°´ä¸Šå®‰å…¨ã€ä½œä¸šå®‰å…¨ç­‰æ–¹é¢ã€‚éœ€è¦æŒæ¡ç›¸å…³çŸ¥è¯†å’ŒæŠ€èƒ½ï¼Œé…å¤‡å¿…è¦çš„å®‰å…¨è®¾å¤‡ï¼Œå®šæœŸè¿›è¡Œå®‰å…¨æ£€æŸ¥å’Œæ¼”ç»ƒã€‚"
            elif any(keyword in query_lower for keyword in ["è®¾å¤‡", "è£…å¤‡", "å·¥å…·"]):
                return "ä¸“ä¸šè®¾å¤‡çš„é€‰æ‹©å’Œä½¿ç”¨éœ€è¦è€ƒè™‘å…·ä½“çš„åº”ç”¨åœºæ™¯å’ŒæŠ€æœ¯è¦æ±‚ã€‚æ­£ç¡®çš„æ“ä½œæ–¹æ³•ã€å®šæœŸç»´æŠ¤ä¿å…»å’Œå®‰å…¨ä½¿ç”¨æ˜¯ç¡®ä¿è®¾å¤‡å‘æŒ¥ä½œç”¨çš„å…³é”®è¦ç´ ã€‚"
            elif any(keyword in query_lower for keyword in ["æ–¹æ³•", "æŠ€æœ¯", "åŸç†"]):
                return "æŠ€æœ¯æ–¹æ³•çš„æŒæ¡éœ€è¦ç†è®ºå­¦ä¹ å’Œå®è·µç›¸ç»“åˆã€‚äº†è§£åŸºæœ¬åŸç†æœ‰åŠ©äºæ›´å¥½åœ°åº”ç”¨æŠ€æœ¯ï¼Œåœ¨å®é™…æ“ä½œä¸­ç§¯ç´¯ç»éªŒï¼Œä¸æ–­æé«˜æŠ€èƒ½æ°´å¹³ã€‚"
            else:
                return f"å…³äºã€Œ{query}ã€çš„é—®é¢˜ï¼Œè¿™æ¶‰åŠä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†ã€‚å¦‚æœæ‚¨èƒ½æä¾›æ›´å…·ä½“çš„é—®é¢˜æè¿°ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›æ›´è¯¦ç»†å’Œå‡†ç¡®çš„å›ç­”ã€‚æ‚¨ä¹Ÿå¯ä»¥è¯¢é—®æ¶ˆé˜²ã€æ½œæ°´ã€æŸç®¡ç­‰æˆ‘æ¯”è¾ƒç†Ÿæ‚‰çš„é¢†åŸŸã€‚"

    def _extract_meaningful_info(self, ref_content, topic):
        """ä»çŸ¥è¯†å›¾è°±å†…å®¹ä¸­æå–æœ‰æ„ä¹‰çš„ä¿¡æ¯ï¼Œç”Ÿæˆè‡ªç„¶è¯­è¨€æ‘˜è¦"""
        if not ref_content or "ä¸‰å…ƒç»„ä¿¡æ¯" not in ref_content:
            return ""

        try:
            # è§£æä¸‰å…ƒç»„ä¿¡æ¯
            if "ä¸‰å…ƒç»„ä¿¡æ¯ï¼š" in ref_content:
                triples_part = ref_content.split("ä¸‰å…ƒç»„ä¿¡æ¯ï¼š")[1].split("ï¼›")

                # æå–æœ‰ç”¨çš„å±æ€§ä¿¡æ¯
                properties = {}
                for triple_str in triples_part[:10]:  # é™åˆ¶å¤„ç†æ•°é‡
                    if "(" in triple_str and ")" in triple_str:
                        try:
                            # æå–ä¸‰å…ƒç»„å†…å®¹
                            content = triple_str.strip("()ï¼›").strip()
                            if " " in content:
                                parts = content.split(" ")
                                if len(parts) >= 3:
                                    entity, relation, value = parts[0], parts[1], " ".join(parts[2:])
                                    if relation not in ["ç»„æˆ", "åŒ…å«", "å±äº"] and value:
                                        properties[relation] = value
                        except:
                            continue

                # ç”Ÿæˆè‡ªç„¶è¯­è¨€æ‘˜è¦
                if properties:
                    summary_parts = []
                    for relation, value in list(properties.items())[:3]:  # åªå–å‰3ä¸ªå±æ€§
                        if relation == "å‹åŠ›":
                            summary_parts.append(f"å·¥ä½œå‹åŠ›ä¸º{value}")
                        elif relation == "å®¹é‡":
                            summary_parts.append(f"å®¹é‡è§„æ ¼{value}")
                        elif relation == "é€‚ç”¨èŒƒå›´":
                            summary_parts.append(f"é€‚ç”¨äº{value}")
                        elif relation == "æè´¨":
                            summary_parts.append(f"é‡‡ç”¨{value}æè´¨")
                        elif relation == "å‹å·":
                            summary_parts.append(f"å¸¸è§å‹å·æœ‰{value}")

                    if summary_parts:
                        return "æ ¹æ®æŠ€æœ¯è§„æ ¼ï¼Œ" + "ï¼Œ".join(summary_parts) + "ã€‚"

            return ""
        except:
            return ""