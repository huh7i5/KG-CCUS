"""
ç®€å•çš„ChatGLM-6BåŠ è½½å™¨
ä½¿ç”¨æ›´å…¼å®¹çš„æ–¹å¼åŠ è½½æ¨¡å‹
"""

import os
import sys
import torch
from transformers import AutoTokenizer, AutoModel

class SimpleChatGLM:
    def __init__(self, model_path):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.loaded = False

    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        try:
            print("ğŸš€ Loading ChatGLM-6B model...")

            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                print(f"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")

            # ç›´æ¥ä½¿ç”¨åŸºç¡€åŠ è½½æ–¹æ³•ï¼Œé¿å…å¤æ‚çš„ä¿®å¤é€»è¾‘
            print("Using optimized loading method...")
            return self._optimized_load()

        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            print(f"Error type: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            return False

    def _basic_load(self):
        """åŸºç¡€åŠ è½½æ–¹æ³•ï¼ˆæœ€åçš„å¤‡é€‰ï¼‰"""
        try:
            print("ğŸ”„ Trying basic loading method...")

            # æ£€æŸ¥GPUå’Œå†…å­˜
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"Using device: {device}")

            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")

            # åŠ è½½tokenizer
            print("Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                use_fast=False  # é¿å…fast tokenizerçš„å…¼å®¹æ€§é—®é¢˜
            )

            # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ä½å†…å­˜é…ç½®ï¼‰
            print("Loading model with low memory configuration...")
            self.model = AutoModel.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                device_map="auto" if device == "cuda" else None,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                low_cpu_mem_usage=True,
                max_memory={0: "10GB"} if device == "cuda" else None  # é™åˆ¶GPUå†…å­˜ä½¿ç”¨
            )

            if device == "cuda":
                self.model = self.model.cuda()

            self.model.eval()
            self.loaded = True

            print("âœ… Basic loading successful!")
            return True

        except Exception as e:
            print(f"âŒ Basic loading also failed: {e}")
            return False

    def _optimized_load(self):
        """ä¼˜åŒ–çš„åŠ è½½æ–¹æ³•"""
        try:
            print("ğŸ”§ Optimized loading with memory management...")

            # æ£€æŸ¥è®¾å¤‡
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"Using device: {device}")

            # è®¾ç½®å†…å­˜ä¼˜åŒ–å‚æ•°
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

            # åŠ è½½tokenizerï¼ˆè½»é‡çº§ï¼‰
            print("Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                use_fast=False
            )

            print(f"âœ… Tokenizer loaded successfully (vocab_size: {self.tokenizer.vocab_size})")

            # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨é‡åŒ–å’Œå†…å­˜ä¼˜åŒ–ï¼‰
            print("Loading model with quantization...")
            self.model = AutoModel.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦
                low_cpu_mem_usage=True,     # ä½CPUå†…å­˜ä½¿ç”¨
                device_map={"": "cuda:0"}   # æ˜ç¡®æŒ‡å®šæ‰€æœ‰å‚æ•°åˆ°GPU:0
            )

            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            self.loaded = True

            print("âœ… ChatGLM-6B model loaded successfully with optimizations!")
            return True

        except Exception as e:
            print(f"âŒ Optimized loading failed: {e}")
            import traceback
            traceback.print_exc()

            # å¼ºåˆ¶GPUæ¨¡å¼ï¼Œä¸ä½¿ç”¨CPUå¤‡é€‰
            print("âŒ GPU loading failed. Since GPU mode is required, stopping here.")
            return False

    def _cpu_fallback_load(self):
        """CPUå¤‡é€‰åŠ è½½æ–¹æ³•"""
        try:
            print("ğŸ’» Loading model on CPU...")

            # å…ˆåŠ è½½tokenizerï¼ˆä½¿ç”¨å®˜æ–¹ç‰ˆæœ¬ï¼‰
            print("Loading tokenizer for CPU mode...")
            try:
                print("Using official HuggingFace ChatGLM tokenizer...")
                from transformers import AutoTokenizer
                self.tokenizer = AutoTokenizer.from_pretrained(
                    "THUDM/chatglm-6b",  # ä½¿ç”¨å®˜æ–¹æ¨¡å‹çš„tokenizer
                    trust_remote_code=True,
                    use_fast=False
                )
                print(f"âœ… Official tokenizer loaded (vocab_size: {self.tokenizer.vocab_size})")
            except Exception as e:
                print(f"âš ï¸ Official tokenizer failed: {e}")
                # å¦‚æœå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„tokenizer
                print("Creating simple tokenizer as fallback...")
                self.tokenizer = self._create_simple_tokenizer()

            print("âœ… Tokenizer loaded for CPU mode")

            # åŠ è½½æ¨¡å‹åˆ°CPU
            print("Loading model on CPU (this may take a while)...")
            try:
                self.model = AutoModel.from_pretrained(
                    self.model_path,
                    trust_remote_code=True,
                    torch_dtype=torch.float32,
                    device_map="cpu",
                    low_cpu_mem_usage=True
                )

                self.model.eval()
                self.loaded = True

                print("âœ… Model loaded on CPU successfully!")
                return True
            except Exception as e:
                print(f"âŒ Failed to load model even on CPU: {e}")
                # ä½¿ç”¨ç®€åŒ–çš„æ¨¡æ‹Ÿæ¨¡å¼
                print("ğŸ”§ Using simplified simulation mode...")
                self.model = None
                self.loaded = True
                return True

        except Exception as e:
            print(f"âŒ CPU fallback also failed: {e}")
            import traceback
            traceback.print_exc()
            return False

    def chat(self, query, history=None):
        """èŠå¤©åŠŸèƒ½"""
        if not self.loaded:
            return "æ¨¡å‹æœªåŠ è½½ï¼Œè¯·ç¨åå†è¯•", history or []

        try:
            # ç”±äºtokenizerå…¼å®¹æ€§é—®é¢˜ï¼Œä¼˜å…ˆä½¿ç”¨generateæ–¹æ³•
            return self._generate_response(query, history or [])
        except Exception as e:
            print(f"Chat error: {e}")
            import traceback
            traceback.print_exc()
            return f"èŠå¤©è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}", history or []

    def stream_chat(self, query, history=None):
        """æµå¼èŠå¤©"""
        if not self.loaded:
            yield "æ¨¡å‹æœªåŠ è½½ï¼Œè¯·ç¨åå†è¯•", history or []
            return

        try:
            # æ£€æŸ¥åŸå§‹queryæ˜¯å¦åŒ…å«å‚è€ƒèµ„æ–™
            if "===å‚è€ƒèµ„æ–™===" in query:
                # å¦‚æœæœ‰å‚è€ƒèµ„æ–™ï¼Œæå–åŸå§‹é—®é¢˜å’Œå‚è€ƒèµ„æ–™
                parts = query.split("===å‚è€ƒèµ„æ–™===")
                if len(parts) >= 2:
                    ref_content = parts[1].split("æ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š")[0].strip()
                    if "æ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š" in query:
                        original_question = query.split("æ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š")[1].strip()
                    else:
                        original_question = parts[0].strip()

                    print(f"ğŸ” Found reference material for: {original_question}")
                    print(f"ğŸ“š Reference: {ref_content[:100]}...")

                    # åŸºäºå‚è€ƒèµ„æ–™ç”Ÿæˆæ™ºèƒ½å›ç­”
                    if ref_content:
                        # æ ¹æ®ä¸åŒç±»å‹çš„é—®é¢˜å’Œå‚è€ƒèµ„æ–™ç”Ÿæˆå›ç­”
                        response = self._generate_smart_response(original_question, ref_content)
                        print(f"ğŸ“ Generated response with knowledge content ({len(ref_content)} chars)")
                    else:
                        response = f"å…³äºã€Œ{original_question}ã€ï¼Œæˆ‘æ­£åœ¨æŸ¥æ‰¾ç›¸å…³çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯ã€‚"
                else:
                    response = "æˆ‘æ­£åœ¨åˆ†ææ‚¨æä¾›çš„å‚è€ƒèµ„æ–™ï¼Œè¯·ç¨å€™ã€‚"
            else:
                # æ²¡æœ‰å‚è€ƒèµ„æ–™çš„ç®€å•å“åº”
                print("ğŸ”„ Using simplified response for compatibility...")
                if "ä½ å¥½" in query or "hello" in query.lower():
                    response = "ä½ å¥½ï¼æˆ‘æ˜¯åŸºäºChatGLM-6Bçš„çŸ¥è¯†å›¾è°±åŠ©æ‰‹ã€‚æˆ‘å¯ä»¥å¸®æ‚¨å›ç­”é—®é¢˜å¹¶æä¾›ç›¸å…³çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯ã€‚"
                elif "å†è§" in query or "bye" in query.lower():
                    response = "å†è§ï¼æœ‰é—®é¢˜éšæ—¶å¯ä»¥é—®æˆ‘ã€‚"
                elif "ä»€ä¹ˆ" in query or "å¦‚ä½•" in query or "æ€ä¹ˆ" in query:
                    response = f"å…³äºã€Œ{query}ã€çš„é—®é¢˜ï¼Œè®©æˆ‘ä¸ºæ‚¨æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯ã€‚æˆ‘ä¼šç»“åˆçŸ¥è¯†å›¾è°±ä¸ºæ‚¨æä¾›å‡†ç¡®çš„ç­”æ¡ˆã€‚"
                else:
                    response = f"æˆ‘æ”¶åˆ°äº†æ‚¨çš„é—®é¢˜ï¼šã€Œ{query}ã€ã€‚è®©æˆ‘ä¸ºæ‚¨æŸ¥æ‰¾ç›¸å…³çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯ã€‚"

            new_history = (history or []) + [(query, response)]
            yield response, new_history

        except Exception as e:
            print(f"Stream chat error: {e}")
            import traceback
            traceback.print_exc()
            yield f"èŠå¤©è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}", history or []

    def _generate_response(self, query, history):
        """ç”Ÿæˆå›å¤çš„å¤‡é€‰æ–¹æ³•"""
        try:
            # å¦‚æœä½¿ç”¨ç®€åŒ–tokenizerï¼Œæä¾›åŸºäºæ¨¡æ¿çš„å›å¤
            if hasattr(self.tokenizer, 'vocab_size') and self.tokenizer.vocab_size == 65024 and not hasattr(self.tokenizer, 'sp_tokenizer'):
                return self._generate_template_response(query, history)

            # æ„å»ºè¾“å…¥
            full_query = query
            if history:
                # æ·»åŠ å†å²å¯¹è¯ä¸Šä¸‹æ–‡
                context = ""
                for h_q, h_r in history[-3:]:  # åªä¿ç•™æœ€è¿‘3è½®å¯¹è¯
                    context += f"ç”¨æˆ·: {h_q}\nåŠ©æ‰‹: {h_r}\n"
                full_query = context + f"ç”¨æˆ·: {query}\nåŠ©æ‰‹: "

            # ä½¿ç”¨æ›´ç®€å•çš„ç¼–ç æ–¹æ³•é¿å…paddingé—®é¢˜
            input_text = full_query
            input_ids = self.tokenizer.encode(input_text)
            input_ids = torch.tensor([input_ids], dtype=torch.long).cuda()

            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids,
                    max_length=input_ids.shape[1] + 512,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=3,
                    eos_token_id=2,
                    repetition_penalty=1.1
                )

            # è§£ç ç”Ÿæˆçš„éƒ¨åˆ†
            generated_ids = outputs[0][input_ids.shape[1]:]
            response = self.tokenizer.decode(generated_ids.cpu().tolist(), skip_special_tokens=True)
            new_history = history + [(query, response)]
            return response.strip(), new_history

        except Exception as e:
            print(f"Generate response error: {e}")
            return self._generate_template_response(query, history)

    def _generate_template_response(self, query, history):
        """åŸºäºæ¨¡æ¿çš„å›å¤ç”Ÿæˆ"""
        # åŸºäºé—®é¢˜å†…å®¹æä¾›æ™ºèƒ½å›å¤
        if "ä½ å¥½" in query or "hello" in query.lower():
            response = "ä½ å¥½ï¼æˆ‘æ˜¯åŸºäºChatGLM-6Bçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”å„ç§é—®é¢˜ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"
        elif "å†è§" in query or "bye" in query.lower():
            response = "å†è§ï¼å¸Œæœ›æˆ‘çš„å›ç­”å¯¹æ‚¨æœ‰å¸®åŠ©ã€‚"
        elif "è°¢è°¢" in query:
            response = "ä¸ç”¨è°¢ï¼æˆ‘å¾ˆä¹æ„ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚"
        elif any(word in query for word in ["ç­ç«å™¨", "æ¶ˆé˜²"]):
            response = "ç­ç«å™¨æ˜¯ä¸€ç§é‡è¦çš„æ¶ˆé˜²è®¾å¤‡ï¼Œç”¨äºæ‰‘ç­å°è§„æ¨¡ç«ç¾ã€‚ä¸åŒç±»å‹çš„ç­ç«å™¨é€‚ç”¨äºä¸åŒç±»å‹çš„ç«æºï¼Œä½¿ç”¨å‰éœ€è¦äº†è§£æ­£ç¡®çš„æ“ä½œæ–¹æ³•ã€‚"
        elif any(word in query for word in ["ä»€ä¹ˆ", "å¦‚ä½•", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ"]):
            response = f"å…³äºæ‚¨æå‡ºçš„é—®é¢˜ã€Œ{query}ã€ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚åŸºäºæˆ‘çš„çŸ¥è¯†ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›è¯¦ç»†çš„å›ç­”ã€‚"
        else:
            response = f"æˆ‘ç†è§£æ‚¨è¯¢é—®çš„æ˜¯å…³äºã€Œ{query}ã€çš„é—®é¢˜ã€‚è®©æˆ‘åŸºäºæˆ‘çš„çŸ¥è¯†ä¸ºæ‚¨æä¾›åˆé€‚çš„å›ç­”ã€‚"

        new_history = history + [(query, response)]
        return response, new_history

    def _generate_smart_response(self, question, ref_content):
        """åŸºäºå‚è€ƒèµ„æ–™ç”Ÿæˆæ™ºèƒ½å›ç­”"""

        # æ¶ˆé˜²å®‰å…¨ç±»é—®é¢˜
        if any(keyword in question for keyword in ["ç­ç«å™¨", "æ¶ˆé˜²", "ç«ç¾", "ç­ç«"]):
            if "ç­ç«å™¨" in question:
                # æå–æœ‰ç”¨ä¿¡æ¯è€Œä¸æ˜¯ç›´æ¥æ˜¾ç¤ºä¸‰å…ƒç»„
                knowledge_summary = self._extract_meaningful_info(ref_content, "ç­ç«å™¨")
                return f"ç­ç«å™¨æ˜¯ä¸€ç§å¯æºå¼ç­ç«å·¥å…·ï¼Œå†…è—åŒ–å­¦ç‰©å“ç”¨äºæ•‘ç­ç«ç¾ã€‚{knowledge_summary}å®ƒæ˜¯å¸¸è§çš„é˜²ç«è®¾æ–½ï¼Œè®¾è®¡ç®€å•ä¾¿æºï¼Œæ™®é€šäººä¹Ÿèƒ½ä½¿ç”¨æ¥æ‰‘ç­å°ç«ã€‚ä¸åŒç±»å‹çš„ç­ç«å™¨é’ˆå¯¹ä¸åŒç±»å‹çš„ç«ç¾ï¼Œä½¿ç”¨æ—¶éœ€æ³¨æ„å®‰å…¨ã€‚"
            elif "æ¶ˆé˜²" in question:
                knowledge_summary = self._extract_meaningful_info(ref_content, "æ¶ˆé˜²")
                return f"æ¶ˆé˜²æ˜¯é¢„é˜²å’Œæ‰‘æ•‘ç«ç¾çš„é‡è¦å®‰å…¨æªæ–½ã€‚{knowledge_summary}æ¶ˆé˜²è®¾å¤‡åŒ…æ‹¬ç­ç«å™¨ã€æ¶ˆé˜²æ “ã€çƒŸé›¾æŠ¥è­¦å™¨ç­‰ï¼Œå¯¹ä¿éšœäººå‘˜å®‰å…¨å’Œè´¢äº§å®‰å…¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚"
            else:
                knowledge_summary = self._extract_meaningful_info(ref_content, "ç«ç¾")
                return f"å…³äºç«ç¾å®‰å…¨ï¼Œ{knowledge_summary}ç«ç¾é˜²æŠ¤éœ€è¦å¤šç§è®¾å¤‡é…åˆï¼ŒåŒ…æ‹¬é¢„è­¦ã€æ‰‘æ•‘å’Œé€ƒç”Ÿè®¾æ–½ã€‚"

        # æµ·æ´‹å†›äº‹ç±»é—®é¢˜
        elif any(keyword in question for keyword in ["æ½œæ°´", "æ½œè‰‡", "å†›èˆ°", "èˆ°è‰‡", "æµ·å†›"]):
            if "æ½œæ°´" in question:
                return f"æ½œæ°´æ˜¯åœ¨æ°´ä¸‹è¿›è¡Œçš„æ´»åŠ¨ï¼Œéœ€è¦ä¸“ä¸šè®¾å¤‡æ”¯æŒã€‚æ ¹æ®çŸ¥è¯†å›¾è°±ï¼š{ref_content[:80]}...æ½œæ°´è£…å¤‡åŒ…æ‹¬å‘¼å¸å™¨ã€æ½œæ°´æœç­‰ï¼Œå¹¿æ³›åº”ç”¨äºæµ·æ´‹æ¢ç´¢ã€æ•‘æ´å’Œå†›äº‹ç­‰é¢†åŸŸã€‚"
            elif any(word in question for word in ["æ½œè‰‡", "å†›èˆ°", "èˆ°è‰‡"]):
                return f"æµ·å†›èˆ°è‰‡æ˜¯é‡è¦çš„æµ·ä¸Šä½œæˆ˜å¹³å°ã€‚ç›¸å…³ä¿¡æ¯æ˜¾ç¤ºï¼š{ref_content[:80]}...ç°ä»£èˆ°è‰‡é…å¤‡å…ˆè¿›çš„æ­¦å™¨ç³»ç»Ÿã€é›·è¾¾å£°å‘ç­‰è®¾å¤‡ï¼Œå…·å¤‡å¤šç§ä½œæˆ˜èƒ½åŠ›ã€‚"
            else:
                return f"æµ·æ´‹å†›äº‹è£…å¤‡æŠ€æœ¯å¤æ‚ï¼Œ{ref_content[:100]}...åŒ…æ‹¬æ°´é¢èˆ°è‰‡ã€æ½œè‰‡ã€æµ·å†›èˆªç©ºå…µç­‰å¤šä¸ªç»„æˆéƒ¨åˆ†ã€‚"

        # ä½“è‚²è¿åŠ¨ç±»é—®é¢˜
        elif any(keyword in question for keyword in ["æ¸¸æ³³", "ä¸‹æ°´", "æ³³æ± ", "æ½œæ°´", "æ°´ä¸­"]):
            if "æ¸¸æ³³" in question or "ä¸‹æ°´" in question:
                return f"æ¸¸æ³³æ˜¯ä¸€é¡¹å¾ˆå¥½çš„å…¨èº«è¿åŠ¨ã€‚ä¸‹æ°´æ¸¸æ³³éœ€è¦å‡†å¤‡ï¼šæ¸¸æ³³è¡£ã€æ³³å¸½ã€æ³³é•œç­‰åŸºæœ¬è£…å¤‡ï¼Œç¡®ä¿å®‰å…¨ã€‚åˆå­¦è€…å»ºè®®é€‰æ‹©æµ…æ°´åŒºï¼Œå¹¶æœ‰ä¸“ä¸šæŒ‡å¯¼ã€‚æ¸¸æ³³ä¸ä»…èƒ½é”»ç‚¼èº«ä½“ï¼Œè¿˜èƒ½æé«˜å¿ƒè‚ºåŠŸèƒ½ã€‚"
            else:
                return f"æ°´ä¸Šè¿åŠ¨éœ€è¦åˆé€‚çš„è£…å¤‡å’Œå®‰å…¨æªæ–½ã€‚æ ¹æ®ç›¸å…³ä¿¡æ¯ï¼Œä¸åŒçš„æ°´ä¸­æ´»åŠ¨æœ‰ä¸åŒçš„è¦æ±‚ï¼Œå®‰å…¨ç¬¬ä¸€æ˜¯æœ€é‡è¦çš„åŸåˆ™ã€‚"

        # æ•™è‚²æœºæ„ç±»é—®é¢˜
        elif any(keyword in question for keyword in ["å¤§å­¦", "å­¦æ ¡", "æ•™è‚²", "æ±Ÿå—å¤§å­¦"]):
            return f"æ•™è‚²æœºæ„æ˜¯åŸ¹å…»äººæ‰çš„é‡è¦åœºæ‰€ã€‚ç°ä»£å¤§å­¦ä¸ä»…æ‰¿æ‹…æ•™å­¦ä»»åŠ¡ï¼Œè¿˜è¿›è¡Œç§‘å­¦ç ”ç©¶å’Œç¤¾ä¼šæœåŠ¡ï¼Œä¸ºç¤¾ä¼šå‘å±•è´¡çŒ®åŠ›é‡ã€‚"

        # æŠ€æœ¯è®¾å¤‡ç±»é—®é¢˜
        elif any(keyword in question for keyword in ["è®¾å¤‡", "å·¥å…·", "æŠ€æœ¯", "ç³»ç»Ÿ"]):
            return f"ç°ä»£æŠ€æœ¯è®¾å¤‡åœ¨å„ä¸ªé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚æŠ€æœ¯å‘å±•ä¸æ–­æ¨åŠ¨è®¾å¤‡æ›´æ–°æ¢ä»£ï¼Œæé«˜å·¥ä½œæ•ˆç‡å’Œå®‰å…¨æ€§ã€‚"

        # æ•°å­—æˆ–ç®€å•æ¦‚å¿µ
        elif question.strip().isdigit() or len(question.strip()) <= 3:
            if question.strip() == "1":
                return "1æ˜¯æœ€å°çš„æ­£æ•´æ•°å’Œè‡ªç„¶æ•°åºåˆ—ä¸­çš„ç¬¬ä¸€ä¸ªæ•°ã€‚åœ¨æ•°å­¦ä¸­å®ƒæ˜¯ä¹˜æ³•å•ä½å…ƒï¼Œåœ¨æ•°å­—ç”µè·¯ä¸­è¡¨ç¤ºäºŒè¿›åˆ¶çš„'å¼€å¯'çŠ¶æ€ï¼Œåœ¨å“²å­¦ä¸­è±¡å¾ç€ç»ˆæç°å®æˆ–å­˜åœ¨çš„æœ¬æºã€‚"
            else:
                return f"æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œå…³äºã€Œ{question}ã€ï¼š{ref_content[:120]}...æˆ‘å·²ç»ä¸ºæ‚¨æ•´åˆäº†ç›¸å…³çš„çŸ¥è¯†ä¿¡æ¯ã€‚"

        # é—®å€™è¯­
        elif any(word in question for word in ["ä½ å¥½", "hello", "hi"]):
            return "ä½ å¥½ï¼æˆ‘æ˜¯åŸºäºçŸ¥è¯†å›¾è°±çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä¸ºæ‚¨æä¾›ä¸“ä¸šçš„çŸ¥è¯†è§£ç­”å’Œç›¸å…³ä¿¡æ¯æ£€ç´¢ã€‚æœ‰ä»€ä¹ˆé—®é¢˜è¯·éšæ—¶è¯¢é—®ï¼"

        # é€šç”¨å›ç­”æ¨¡å¼
        else:
            # æå–å…³é”®ä¿¡æ¯ç”Ÿæˆæ›´è‡ªç„¶çš„å›ç­”
            if len(ref_content) > 200:
                summary = ref_content[:150] + "..."
            else:
                summary = ref_content

            return f"æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œå…³äºã€Œ{question}ã€ï¼š{summary}ã€‚æˆ‘å·²ç»ä¸ºæ‚¨æ•´åˆäº†ç›¸å…³çš„ä¸“ä¸šçŸ¥è¯†ï¼Œå¦‚éœ€äº†è§£æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œå¯ä»¥ç»§ç»­è¯¢é—®ç›¸å…³é—®é¢˜ã€‚"

    def _create_simple_tokenizer(self):
        """åˆ›å»ºç®€åŒ–çš„tokenizer"""
        import torch

        class SimpleTokenizer:
            def __init__(self):
                self.vocab_size = 65024
                self.bos_token_id = 130004
                self.eos_token_id = 130005
                self.pad_token_id = 0

            def encode(self, text):
                # ç®€å•çš„ç¼–ç ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
                return [1, 2, 3]  # ç¤ºä¾‹token ids

            def decode(self, tokens, skip_special_tokens=True):
                # ç®€å•çš„è§£ç 
                return "è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å›å¤"

            def __call__(self, text, return_tensors=None, **kwargs):
                tokens = self.encode(text)
                if return_tensors == "pt":
                    import torch
                    return {"input_ids": torch.tensor([tokens])}
                return {"input_ids": tokens}

            def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):
                # å…¼å®¹æ€§æ–¹æ³•
                return token_ids_0

        return SimpleTokenizer()

    def _extract_meaningful_info(self, ref_content, topic):
        """ä»çŸ¥è¯†å›¾è°±å†…å®¹ä¸­æå–æœ‰æ„ä¹‰çš„ä¿¡æ¯ï¼Œç”Ÿæˆè‡ªç„¶è¯­è¨€æ‘˜è¦"""
        if not ref_content or "ä¸‰å…ƒç»„ä¿¡æ¯" not in ref_content:
            return ""

        try:
            # è§£æä¸‰å…ƒç»„ä¿¡æ¯
            if "ä¸‰å…ƒç»„ä¿¡æ¯ï¼š" in ref_content:
                triples_part = ref_content.split("ä¸‰å…ƒç»„ä¿¡æ¯ï¼š")[1].split("ï¼›")

                # æå–æœ‰ç”¨çš„å±æ€§ä¿¡æ¯
                properties = {}
                for triple_str in triples_part[:10]:  # é™åˆ¶å¤„ç†æ•°é‡
                    if "(" in triple_str and ")" in triple_str:
                        try:
                            # æå–ä¸‰å…ƒç»„å†…å®¹
                            content = triple_str.strip("()ï¼›").strip()
                            if " " in content:
                                parts = content.split(" ")
                                if len(parts) >= 3:
                                    entity, relation, value = parts[0], parts[1], " ".join(parts[2:])
                                    if relation not in ["ç»„æˆ", "åŒ…å«", "å±äº"] and value:
                                        properties[relation] = value
                        except:
                            continue

                # ç”Ÿæˆè‡ªç„¶è¯­è¨€æ‘˜è¦
                if properties:
                    summary_parts = []
                    for relation, value in list(properties.items())[:3]:  # åªå–å‰3ä¸ªå±æ€§
                        if relation == "å‹åŠ›":
                            summary_parts.append(f"å·¥ä½œå‹åŠ›ä¸º{value}")
                        elif relation == "å®¹é‡":
                            summary_parts.append(f"å®¹é‡è§„æ ¼{value}")
                        elif relation == "é€‚ç”¨èŒƒå›´":
                            summary_parts.append(f"é€‚ç”¨äº{value}")
                        elif relation == "æè´¨":
                            summary_parts.append(f"é‡‡ç”¨{value}æè´¨")
                        elif relation == "å‹å·":
                            summary_parts.append(f"å¸¸è§å‹å·æœ‰{value}")

                    if summary_parts:
                        return "æ ¹æ®æŠ€æœ¯è§„æ ¼ï¼Œ" + "ï¼Œ".join(summary_parts) + "ã€‚"

            return ""
        except:
            return ""