
# æš‚æ—¶ç¦ç”¨PaddleNLPä¾èµ–ï¼Œä½¿ç”¨åŸºäºå…³é”®è¯çš„å®ä½“è¯†åˆ«
# from paddlenlp import Taskflow
import json
import os

class Ner:
    def __init__(self):
        print("Warning: Using keyword-based NER instead of deep learning model")
        # åŠ è½½çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“ä½œä¸ºè¯å…¸
        self.entity_dict = self._load_entities_from_kg()

    def _load_entities_from_kg(self):
        """ä»çŸ¥è¯†å›¾è°±ä¸­åŠ è½½å®ä½“è¯å…¸"""
        entities = set()

        # ä»data.jsonä¸­åŠ è½½å®ä½“
        data_path = "data/data.json"
        if os.path.exists(data_path):
            try:
                with open(data_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for node in data.get('nodes', []):
                        entity_name = node.get('name', '').strip()
                        if entity_name and len(entity_name) > 1:  # è¿‡æ»¤è¿‡çŸ­çš„å®ä½“
                            entities.add(entity_name)
                print(f"âœ… Loaded {len(entities)} entities from knowledge graph")
            except Exception as e:
                print(f"âŒ Error loading entities from {data_path}: {e}")

        # æ·»åŠ ä¸€äº›å¸¸è§çš„æ¶ˆé˜²å’Œæµ·å†›ç›¸å…³å…³é”®è¯
        common_entities = {
            'ç­ç«å™¨', 'æ¶ˆé˜²', 'ç«ç¾', 'æ¶ˆé˜²è½¦', 'æ¶ˆé˜²å‘˜', 'æ¶ˆé˜²æ “', 'çƒŸé›¾æŠ¥è­¦å™¨',
            'æ½œæ°´', 'æ½œè‰‡', 'å†›èˆ°', 'èˆ°è‰‡', 'æµ·å†›', 'æ½œæ°´å‘˜', 'å‘¼å¸å™¨', 'æ½œæ°´è£…å…·',
            'èˆ°è‰‡æŸç®¡', 'æŸå®³ç®¡åˆ¶', 'æ°´ä¸‹ä½œä¸š', 'æ•‘ç”Ÿè®¾å¤‡', 'é˜²ç«è®¾æ–½',
            'æ±Ÿå—å¤§å­¦', 'å¤§å­¦', 'å­¦æ ¡', 'æ•™è‚²', 'ç ”ç©¶', 'æµ‹æ·±ä»ª', 'å£°å‘', 'é›·è¾¾'
        }
        entities.update(common_entities)

        return entities

    def predict(self, text):
        # è¿”å›ç©ºåˆ—è¡¨ä½œä¸ºä¸´æ—¶è§£å†³æ–¹æ¡ˆ
        return []

    def get_entities(self, text, etypes=None):
        '''è·å–å¥å­ä¸­æŒ‡å®šç±»å‹çš„å®ä½“

        Args:
            text: å¥å­
            etypes: å®ä½“ç±»å‹åˆ—è¡¨
        Returns:
            entities: å®ä½“åˆ—è¡¨
        '''
        entities = []

        # åŸºäºå…³é”®è¯åŒ¹é…æå–å®ä½“
        for entity in self.entity_dict:
            if entity in text and entity not in entities:
                entities.append(entity)

        # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆé€‰æ‹©è¾ƒé•¿çš„å®ä½“ï¼ˆé¿å…åŒ¹é…å­ä¸²ï¼‰
        entities.sort(key=len, reverse=True)

        # å»é‡ï¼šå¦‚æœä¸€ä¸ªå®ä½“æ˜¯å¦ä¸€ä¸ªå®ä½“çš„å­ä¸²ï¼Œåˆ™ç§»é™¤è¾ƒçŸ­çš„
        filtered_entities = []
        for entity in entities:
            is_substring = False
            for existing in filtered_entities:
                if entity in existing and entity != existing:
                    is_substring = True
                    break
            if not is_substring:
                filtered_entities.append(entity)

        print(f"ğŸ” Extracted entities from '{text}': {filtered_entities}")
        return filtered_entities[:5]  # é™åˆ¶è¿”å›æœ€å¤š5ä¸ªå®ä½“