
import json
import os
import re

class Ner:
    """CCUSé¢†åŸŸå‘½åå®ä½“è¯†åˆ«æ¨¡å—"""

    def __init__(self):
        print("ğŸ”§ Initializing CCUS Domain NER System...")
        # åŠ è½½CCUSé¢†åŸŸå®ä½“è¯å…¸
        self.entity_dict = self._load_ccus_entities()
        self.patterns = self._build_patterns()

    def _load_ccus_entities(self):
        """åŠ è½½CCUSé¢†åŸŸå®ä½“è¯å…¸"""
        entities = set()

        # ä»çŸ¥è¯†å›¾è°±ä¸­åŠ è½½å®ä½“
        data_path = "data/data.json"
        if os.path.exists(data_path):
            try:
                with open(data_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for node in data.get('nodes', []):
                        entity_name = node.get('name', '').strip()
                        if entity_name and len(entity_name) > 1:
                            entities.add(entity_name)
                print(f"âœ… Loaded {len(entities)} entities from knowledge graph")
            except Exception as e:
                print(f"âŒ Error loading entities from {data_path}: {e}")

        # CCUSé¢†åŸŸæ ¸å¿ƒå®ä½“
        ccus_entities = {
            # æŠ€æœ¯ç±»å‹
            'CCUS', 'CCS', 'CCU', 'ç¢³æ•é›†', 'ç¢³åˆ©ç”¨', 'ç¢³å‚¨å­˜', 'ç¢³å°å­˜',
            'åç‡ƒçƒ§æ•é›†', 'é¢„ç‡ƒçƒ§æ•é›†', 'å¯Œæ°§ç‡ƒçƒ§', 'ç›´æ¥ç©ºæ°”æ•é›†', 'DAC',
            'è†œåˆ†ç¦»', 'åŒ–å­¦å¸æ”¶', 'ç‰©ç†å¸é™„', 'å¸æ”¶æ³•', 'å¸é™„æ³•',

            # åŒ–å­¦ç‰©è´¨å’Œææ–™
            'äºŒæ°§åŒ–ç¢³', 'CO2', 'MEA', 'MDEA', 'èƒºç±»æº¶å‰‚', 'ç¦»å­æ¶²ä½“',
            'é‡‘å±æœ‰æœºæ¡†æ¶', 'MOF', 'åˆ†å­ç­›', 'æ´»æ€§ç‚­', 'æ°§åŒ–é’™',

            # è®¾å¤‡å’Œå·¥è‰º
            'å¸æ”¶å¡”', 'å†ç”Ÿå¡”', 'å‹ç¼©æœº', 'çƒ­äº¤æ¢å™¨', 'ååº”å™¨', 'åˆ†ç¦»å™¨',
            'ç®¡é“è¿è¾“', 'èˆ¹èˆ¶è¿è¾“', 'IGCC', 'æ•´ä½“ç…¤æ°”åŒ–è”åˆå¾ªç¯',

            # åº”ç”¨è¡Œä¸š
            'ç‡ƒç…¤ç”µå‚', 'ç«åŠ›å‘ç”µ', 'é’¢é“å†¶é‡‘', 'æ°´æ³¥ç”Ÿäº§', 'çŸ³æ²¹åŒ–å·¥',
            'ç…¤åŒ–å·¥', 'å¤©ç„¶æ°”å‘ç”µ', 'ç”Ÿç‰©è´¨å‘ç”µ', 'å·¥ä¸šé”…ç‚‰',

            # åœ°ç†å’Œé¡¹ç›®
            'é„‚å°”å¤šæ–¯', 'å¤§åº†æ²¹ç”°', 'èƒœåˆ©æ²¹ç”°', 'ååŒ—æ²¹ç”°', 'æ¸¤æµ·æ¹¾',
            'æ·±éƒ¨å’¸æ°´å±‚', 'æ¯ç«­æ²¹æ°”è—', 'ç…¤å±‚æ°”å‚¨å±‚', 'ç›ç©´å‚¨å­˜',

            # æ”¿ç­–å’Œæ ‡å‡†
            'ç¢³ä¸­å’Œ', 'ç¢³è¾¾å³°', 'åŒç¢³ç›®æ ‡', 'ç¢³æ’æ”¾æƒ', 'ç¢³äº¤æ˜“',
            'ç¢³é…é¢', 'ç¢³è¶³è¿¹', 'æ¸©å®¤æ°”ä½“', 'æ°”å€™å˜åŒ–', 'å·´é»åå®š',

            # æŠ€æœ¯æŒ‡æ ‡
            'æ•é›†ç‡', 'å‚¨å­˜å®¹é‡', 'æ³„æ¼ç‡', 'èƒ½è€—', 'æˆæœ¬æ•ˆç›Š',
            'ç”Ÿå‘½å‘¨æœŸè¯„ä¼°', 'LCA', 'æŠ€æœ¯æˆç†Ÿåº¦', 'ç»æµæ€§åˆ†æ'
        }
        entities.update(ccus_entities)

        return entities

    def _build_patterns(self):
        """æ„å»ºå®ä½“è¯†åˆ«æ¨¡å¼"""
        patterns = []

        # æ•°å€¼+å•ä½æ¨¡å¼
        patterns.extend([
            r'\d+(?:\.\d+)?(?:ä¸‡å¨|å¨|åƒå¨|Mt|Gt|ç«‹æ–¹ç±³|mÂ³)',
            r'\d+(?:\.\d+)?%',
            r'\d+(?:\.\d+)?(?:åº¦|â„ƒ|Â°C|MPa|bar|kPa)',
            r'\d+(?:\.\d+)?(?:å¹´|æœˆ|æ—¥|å°æ—¶|h|å¤©)',
        ])

        # æŠ€æœ¯åç§°æ¨¡å¼
        patterns.extend([
            r'[A-Z]{2,6}æŠ€æœ¯',
            r'.{1,4}æ•é›†æŠ€æœ¯',
            r'.{1,4}å‚¨å­˜æŠ€æœ¯',
            r'.{1,4}åˆ©ç”¨æŠ€æœ¯',
        ])

        return [re.compile(pattern) for pattern in patterns]

    def predict(self, text):
        """é¢„æµ‹æ¥å£ï¼ˆå…¼å®¹æ€§ä¿ç•™ï¼‰"""
        return self.get_entities(text)

    def get_entities(self, text, etypes=None):
        """è·å–å¥å­ä¸­çš„CCUSé¢†åŸŸå®ä½“

        Args:
            text: è¾“å…¥æ–‡æœ¬
            etypes: å®ä½“ç±»å‹åˆ—è¡¨ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼Œä½†åœ¨CCUSé¢†åŸŸä¸ä½¿ç”¨ï¼‰
        Returns:
            entities: æå–åˆ°çš„å®ä½“åˆ—è¡¨
        """
        entities = []
        text_processed = text.strip()

        # 1. åŸºäºè¯å…¸çš„ç²¾ç¡®åŒ¹é…
        dict_entities = self._extract_dict_entities(text_processed)
        entities.extend(dict_entities)

        # 2. åŸºäºæ¨¡å¼çš„åŒ¹é…
        pattern_entities = self._extract_pattern_entities(text_processed)
        entities.extend(pattern_entities)

        # 3. å»é‡å’Œè¿‡æ»¤
        filtered_entities = self._filter_entities(entities)

        print(f"ğŸ” CCUS NER result for '{text}': {filtered_entities}")
        return filtered_entities[:8]  # è¿”å›æœ€å¤š8ä¸ªå®ä½“

    def _extract_dict_entities(self, text):
        """åŸºäºè¯å…¸æå–å®ä½“"""
        entities = []
        text_lower = text.lower()

        # æŒ‰é•¿åº¦å€’åºæ’åºï¼Œä¼˜å…ˆåŒ¹é…é•¿å®ä½“
        sorted_entities = sorted(self.entity_dict, key=len, reverse=True)

        for entity in sorted_entities:
            entity_lower = entity.lower()
            if entity_lower in text_lower:
                # æ£€æŸ¥è¾¹ç•Œï¼Œé¿å…éƒ¨åˆ†åŒ¹é…
                import re
                pattern = re.compile(r'\b' + re.escape(entity_lower) + r'\b', re.IGNORECASE)
                if pattern.search(text) or entity_lower in text_lower:
                    entities.append(entity)
                    # ä»æ–‡æœ¬ä¸­ç§»é™¤å·²åŒ¹é…çš„å®ä½“ï¼Œé¿å…é‡å¤åŒ¹é…
                    text_lower = text_lower.replace(entity_lower, ' ', 1)

        return entities

    def _extract_pattern_entities(self, text):
        """åŸºäºæ¨¡å¼æå–å®ä½“"""
        entities = []

        for pattern in self.patterns:
            matches = pattern.findall(text)
            for match in matches:
                if match and len(match.strip()) > 1:
                    entities.append(match.strip())

        return entities

    def _filter_entities(self, entities):
        """è¿‡æ»¤å’Œå»é‡å®ä½“"""
        if not entities:
            return []

        # å»é™¤é‡å¤é¡¹
        unique_entities = list(dict.fromkeys(entities))

        # ç§»é™¤è¢«åŒ…å«çš„çŸ­å®ä½“
        filtered = []
        for entity in unique_entities:
            is_contained = False
            for other in unique_entities:
                if (entity != other and
                    len(entity) < len(other) and
                    entity.lower() in other.lower()):
                    is_contained = True
                    break

            if not is_contained:
                filtered.append(entity)

        # æŒ‰ç›¸å…³æ€§æ’åºï¼ˆCCUSæ ¸å¿ƒæœ¯è¯­ä¼˜å…ˆï¼‰
        ccus_core_terms = {'ccus', 'ccs', 'ccu', 'ç¢³æ•é›†', 'ç¢³å‚¨å­˜', 'ç¢³åˆ©ç”¨', 'äºŒæ°§åŒ–ç¢³'}

        def entity_priority(entity):
            entity_lower = entity.lower()
            if any(core in entity_lower for core in ccus_core_terms):
                return 0  # æœ€é«˜ä¼˜å…ˆçº§
            elif any(keyword in entity_lower for keyword in ['ç¢³', 'æ•é›†', 'å‚¨å­˜', 'åˆ©ç”¨', 'co2']):
                return 1  # é«˜ä¼˜å…ˆçº§
            else:
                return 2  # æ™®é€šä¼˜å…ˆçº§

        filtered.sort(key=lambda x: (entity_priority(x), -len(x)))
        return filtered