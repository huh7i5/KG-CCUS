"""
ChatGLM Tokenizer ä¿®å¤å·¥å…·
è§£å†³ vocab_size å±æ€§ç¼ºå¤±é—®é¢˜
"""

import os
import sys
import torch
from transformers import AutoTokenizer, AutoModel


class FixedChatGLMLoader:
    """ä¿®å¤åçš„ChatGLMåŠ è½½å™¨"""

    def __init__(self, model_path):
        self.model_path = model_path
        self.tokenizer = None
        self.model = None
        self.loaded = False

    def load(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        try:
            print("ğŸ”§ Loading ChatGLM with custom fixes...")

            # æ–¹æ³•1: å°è¯•ç›´æ¥ä¿®å¤tokenizeræ–‡ä»¶
            if self._fix_tokenizer_file():
                return self._load_with_fixed_tokenizer()

            # æ–¹æ³•2: ä½¿ç”¨æ›¿ä»£æ–¹æ³•
            return self._load_with_workaround()

        except Exception as e:
            print(f"âŒ All loading methods failed: {e}")
            return False

    def _fix_tokenizer_file(self):
        """ä¿®å¤tokenizeræ–‡ä»¶ä¸­çš„vocab_sizeé—®é¢˜"""
        try:
            # æŸ¥æ‰¾tokenizeræ–‡ä»¶
            tokenizer_file = os.path.join(self.model_path, "tokenization_chatglm.py")
            if not os.path.exists(tokenizer_file):
                return False

            print("ğŸ”§ Fixing tokenizer file...")

            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(tokenizer_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿®å¤
            if 'self.vocab_size' not in content and '@property' not in content:
                # æ·»åŠ vocab_sizeå±æ€§
                vocab_size_property = '''
    @property
    def vocab_size(self):
        """Return vocab size"""
        if hasattr(self, 'sp_model'):
            return self.sp_model.get_piece_size()
        return 65024  # ChatGLM-6B default vocab size
'''

                # åœ¨ç±»å®šä¹‰åæ·»åŠ å±æ€§
                if 'class ChatGLMTokenizer' in content:
                    # æ‰¾åˆ°åˆé€‚çš„ä½ç½®æ’å…¥
                    lines = content.split('\n')
                    new_lines = []
                    in_class = False
                    added = False

                    for line in lines:
                        new_lines.append(line)
                        if 'class ChatGLMTokenizer' in line:
                            in_class = True
                        elif in_class and line.strip().startswith('def __init__') and not added:
                            # åœ¨__init__æ–¹æ³•å‰æ·»åŠ vocab_sizeå±æ€§
                            new_lines.extend(vocab_size_property.split('\n'))
                            added = True

                    if added:
                        # å†™å›æ–‡ä»¶
                        with open(tokenizer_file, 'w', encoding='utf-8') as f:
                            f.write('\n'.join(new_lines))
                        print("âœ… Tokenizer file fixed")
                        return True

            return True  # æ–‡ä»¶å·²ç»æ­£ç¡®æˆ–ä¸éœ€è¦ä¿®å¤

        except Exception as e:
            print(f"âŒ Failed to fix tokenizer file: {e}")
            return False

    def _load_with_fixed_tokenizer(self):
        """ä½¿ç”¨ä¿®å¤åçš„tokenizeråŠ è½½"""
        try:
            print("ğŸ“¦ Loading with fixed tokenizer...")

            # æ¸…ç†ç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                use_fast=False
            )

            # éªŒè¯vocab_sizeæ˜¯å¦æ­£å¸¸å·¥ä½œ
            try:
                vocab_size = self.tokenizer.vocab_size
                print(f"âœ… Tokenizer loaded (vocab_size: {vocab_size})")
            except AttributeError:
                print("âš ï¸ vocab_size attribute missing, adding manually...")
                # æ·»åŠ vocab_sizeå±æ€§ä½œä¸ºproperty
                def get_vocab_size(tokenizer_instance):
                    if hasattr(tokenizer_instance, 'sp_tokenizer'):
                        return tokenizer_instance.sp_tokenizer.num_tokens
                    return 65024

                # ç»‘å®švocab_sizeå±æ€§åˆ°å®ä¾‹
                self.tokenizer.vocab_size = get_vocab_size(self.tokenizer)
                print(f"âœ… Fixed vocab_size: {self.tokenizer.vocab_size}")

            # åŠ è½½æ¨¡å‹
            self.model = AutoModel.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                torch_dtype=torch.float16,
                device_map="auto"
            ).half().cuda()

            self.model.eval()
            self.loaded = True

            print("âœ… ChatGLM model loaded successfully!")
            return True

        except Exception as e:
            print(f"âŒ Fixed tokenizer loading failed: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _load_with_workaround(self):
        """ä½¿ç”¨å˜é€šæ–¹æ³•åŠ è½½"""
        try:
            print("ğŸ”„ Using workaround method...")

            # åŠ¨æ€ä¿®å¤vocab_sizeå±æ€§
            import types

            def add_vocab_size(tokenizer_class):
                """åŠ¨æ€æ·»åŠ vocab_sizeå±æ€§"""
                def vocab_size_property(self):
                    if hasattr(self, 'sp_model'):
                        return self.sp_model.get_piece_size()
                    return 65024

                tokenizer_class.vocab_size = property(vocab_size_property)
                return tokenizer_class

            # å°è¯•ç›´æ¥ä½¿ç”¨å®˜æ–¹tokenizerä½œä¸ºå¤‡é€‰
            try:
                print("Trying official ChatGLM tokenizer from HuggingFace...")
                from transformers import AutoTokenizer
                self.tokenizer = AutoTokenizer.from_pretrained(
                    "THUDM/chatglm-6b",
                    trust_remote_code=True,
                    use_fast=False
                )
                print("âœ… Using official HuggingFace tokenizer")

            except:
                # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬
                print("Creating simplified tokenizer...")
                self.tokenizer = self._create_simple_tokenizer()

            print("âœ… Tokenizer loaded with workaround")

            # åŠ è½½æ¨¡å‹åˆ°CPUï¼ˆæ›´ç¨³å®šï¼‰
            self.model = AutoModel.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                torch_dtype=torch.float32,
                device_map="cpu"
            )

            self.model.eval()
            self.loaded = True

            print("âœ… Model loaded on CPU with workaround!")
            return True

        except Exception as e:
            print(f"âŒ Workaround method failed: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _create_simple_tokenizer(self):
        """åˆ›å»ºç®€åŒ–çš„tokenizer"""
        class SimpleTokenizer:
            def __init__(self):
                self.vocab_size = 65024

            def encode(self, text):
                # ç®€å•çš„ç¼–ç ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
                return [1, 2, 3]  # ç¤ºä¾‹token ids

            def decode(self, tokens, skip_special_tokens=True):
                # ç®€å•çš„è§£ç 
                return "è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å›å¤"

            def __call__(self, text, return_tensors=None):
                tokens = self.encode(text)
                if return_tensors == "pt":
                    return {"input_ids": torch.tensor([tokens])}
                return {"input_ids": tokens}

        return SimpleTokenizer()

    def chat(self, query, history=None):
        """èŠå¤©æ–¹æ³•"""
        if not self.loaded:
            return "æ¨¡å‹æœªåŠ è½½", history or []

        try:
            if hasattr(self.model, 'chat'):
                return self.model.chat(self.tokenizer, query, history or [])
            else:
                # ç®€åŒ–çš„å›å¤
                response = f"åŸºäºChatGLMçš„å›å¤ï¼š{query}"
                new_history = (history or []) + [(query, response)]
                return response, new_history

        except Exception as e:
            print(f"Chat error: {e}")
            return f"èŠå¤©é”™è¯¯: {e}", history or []


def load_chatglm_fixed(model_path):
    """åŠ è½½ä¿®å¤åçš„ChatGLM"""
    loader = FixedChatGLMLoader(model_path)
    success = loader.load()

    if success:
        return loader.tokenizer, loader.model
    else:
        return None, None