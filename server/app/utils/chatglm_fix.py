"""
ChatGLM-6Bå…¼å®¹æ€§ä¿®å¤
è§£å†³tokenizerçš„vocab_sizeå±æ€§é—®é¢˜
"""

import os
import sys
import torch
from transformers import AutoTokenizer, AutoModel
import sentencepiece as spm

def fix_chatglm_tokenizer(model_path):
    """ä¿®å¤ChatGLM tokenizerçš„å…¼å®¹æ€§é—®é¢˜"""

    # ç¡®ä¿sentencepieceæ¨¡å‹æ–‡ä»¶å­˜åœ¨
    spm_model_path = os.path.join(model_path, "ice_text.model")
    if not os.path.exists(spm_model_path):
        print(f"âŒ SentencePiece model not found: {spm_model_path}")
        return None, None

    try:
        print("ğŸ”§ Loading ChatGLM with compatibility fixes...")

        # 1. å…ˆå°è¯•åŠ è½½tokenizerï¼Œå¹¶ä¿®å¤vocab_sizeé—®é¢˜
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                use_fast=False,
                local_files_only=True
            )
        except AttributeError as e:
            if "vocab_size" in str(e):
                print("âš ï¸ Original tokenizer has vocab_size issue, using simple tokenizer")
                tokenizer = create_simple_tokenizer(model_path)
            else:
                raise e

        # 2. æ‰‹åŠ¨ä¿®å¤sp_tokenizerå±æ€§
        if hasattr(tokenizer, 'sp_tokenizer') and tokenizer.sp_tokenizer is None:
            # åˆ›å»ºSentencePieceå¤„ç†å™¨
            sp_processor = spm.SentencePieceProcessor()
            sp_processor.Load(spm_model_path)
            tokenizer.sp_tokenizer = sp_processor
            print("âœ… Fixed sp_tokenizer attribute")

        # 3. ç¡®ä¿num_tokenså±æ€§å­˜åœ¨
        if hasattr(tokenizer, 'sp_tokenizer') and hasattr(tokenizer.sp_tokenizer, 'vocab_size'):
            tokenizer.sp_tokenizer.num_tokens = tokenizer.sp_tokenizer.vocab_size()

        print("âœ… Tokenizer loaded and fixed")

        # 4. åŠ è½½æ¨¡å‹
        print("Loading model...")
        model = AutoModel.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map="auto",
            local_files_only=True
        )

        # 5. ç§»åŠ¨åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if torch.cuda.is_available():
            model = model.cuda()
            print("âœ… Model loaded on GPU")
        else:
            print("âš ï¸ Model loaded on CPU")

        model.eval()
        print("âœ… ChatGLM-6B loaded successfully with fixes!")

        return tokenizer, model

    except Exception as e:
        print(f"âŒ Failed to load with fixes: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def load_chatglm_alternative(model_path):
    """å¤‡é€‰åŠ è½½æ–¹æ¡ˆï¼šä½¿ç”¨æœ¬åœ°æ–‡ä»¶"""
    try:
        print("ğŸ”„ Trying alternative loading method...")

        # ä½¿ç”¨æœ¬åœ°æ–‡ä»¶åŠ è½½
        from transformers import AutoConfig

        # 1. åŠ è½½é…ç½®
        config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)

        # 2. æ‰‹åŠ¨åˆ›å»ºtokenizerï¼ˆè·³è¿‡problematicåˆå§‹åŒ–ï¼‰
        import json
        tokenizer_config_path = os.path.join(model_path, "tokenizer_config.json")

        if os.path.exists(tokenizer_config_path):
            with open(tokenizer_config_path, 'r') as f:
                tokenizer_config = json.load(f)

        # 3. ç›´æ¥åŠ è½½æ¨¡å‹
        model = AutoModel.from_pretrained(
            model_path,
            config=config,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            local_files_only=True
        )

        # 4. å°è¯•ä»HuggingFaceç›´æ¥åŠ è½½tokenizer
        try:
            tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
            print("âœ… Loaded tokenizer from HuggingFace")
        except:
            # å¦‚æœå¤±è´¥ï¼Œåˆ›å»ºç®€åŒ–çš„tokenizer
            tokenizer = create_simple_tokenizer(model_path)
            print("âœ… Created simple tokenizer")

        if torch.cuda.is_available():
            model = model.cuda()

        model.eval()
        print("âœ… Alternative loading successful!")

        return tokenizer, model

    except Exception as e:
        print(f"âŒ Alternative loading failed: {e}")
        return None, None

def create_simple_tokenizer(model_path):
    """åˆ›å»ºç®€åŒ–çš„tokenizer"""
    import sentencepiece as spm

    class TokenizedInputs(dict):
        """æ”¯æŒ.to()æ–¹æ³•çš„å­—å…¸ç±»ï¼Œå…¼å®¹ChatGLMæ¨¡å‹éœ€æ±‚"""
        def to(self, device):
            """å°†æ‰€æœ‰tensorç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
            result = TokenizedInputs()
            for key, value in self.items():
                if hasattr(value, 'to'):
                    result[key] = value.to(device)
                else:
                    result[key] = value
            return result

    class SimpleTokenizer:
        def __init__(self, model_path):
            self.sp_model = spm.SentencePieceProcessor()
            spm_path = os.path.join(model_path, "ice_text.model")
            self.sp_model.Load(spm_path)
            self._vocab_size = self.sp_model.vocab_size()

            # æ·»åŠ å¿…è¦çš„å±æ€§
            self.eos_token = "</s>"
            self.eos_token_id = self.sp_model.PieceToId("</s>")
            self.pad_token = "<pad>"
            self.pad_token_id = 0

        @property
        def vocab_size(self):
            return self._vocab_size

        def encode(self, text, return_tensors=None, **kwargs):
            """ç¼–ç æ–‡æœ¬ä¸ºtoken ids"""
            if isinstance(text, str):
                ids = self.sp_model.EncodeAsIds(text)
            else:
                ids = text

            if return_tensors == "pt":
                import torch
                return torch.tensor([ids])
            elif return_tensors is not None:
                # å¯¹äºå…¶ä»–return_tensorså€¼ï¼Œè¿”å›åˆ—è¡¨æ ¼å¼
                return [ids]
            return ids

        def decode(self, ids, skip_special_tokens=True, **kwargs):
            """è§£ç token idsä¸ºæ–‡æœ¬"""
            if hasattr(ids, 'tolist'):
                ids = ids.tolist()
            if isinstance(ids[0], list):
                ids = ids[0]
            return self.sp_model.DecodeIds(ids)

        def __call__(self, text, **kwargs):
            """ä½¿tokenizerå¯è°ƒç”¨"""
            # å¤„ç†åˆ—è¡¨è¾“å…¥ (åƒ tokenizer([prompt], return_tensors="pt"))
            if isinstance(text, list):
                if len(text) == 1:
                    # å•ä¸ªå­—ç¬¦ä¸²çš„åˆ—è¡¨
                    result = self.encode(text[0], **kwargs)
                    # è¿”å›TokenizedInputså­—å…¸ï¼ŒåŒæ—¶æ”¯æŒ.to()æ–¹æ³•
                    import torch
                    if isinstance(result, torch.Tensor):
                        return TokenizedInputs({"input_ids": result})
                    else:
                        return TokenizedInputs({"input_ids": torch.tensor([result])})
                else:
                    # å¤šä¸ªå­—ç¬¦ä¸²çš„åˆ—è¡¨ï¼Œæ‰¹å¤„ç†
                    results = []
                    for t in text:
                        results.append(self.encode(t, return_tensors=None))
                    if kwargs.get("return_tensors") == "pt":
                        import torch
                        return TokenizedInputs({"input_ids": torch.tensor(results)})
                    return TokenizedInputs({"input_ids": results})
            else:
                # å•ä¸ªå­—ç¬¦ä¸²
                return self.encode(text, **kwargs)

        def convert_tokens_to_ids(self, tokens):
            if isinstance(tokens, str):
                return self.sp_model.PieceToId(tokens)
            return [self.sp_model.PieceToId(token) for token in tokens]

        def convert_ids_to_tokens(self, ids):
            if isinstance(ids, int):
                return self.sp_model.IdToPiece(ids)
            return [self.sp_model.IdToPiece(id) for id in ids]

        def tokenize(self, text):
            """åˆ†è¯"""
            return self.sp_model.EncodeAsPieces(text)

        def convert_tokens_to_string(self, tokens):
            """å°†tokensè½¬æ¢ä¸ºå­—ç¬¦ä¸²"""
            return self.sp_model.DecodePieces(tokens)

        @property
        def bos_token_id(self):
            return self.sp_model.PieceToId("<sop>")

        @property
        def gmask_token_id(self):
            return self.sp_model.PieceToId("[gMASK]")

        def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):
            """æ„å»ºå¸¦ç‰¹æ®Štokençš„è¾“å…¥"""
            gmask_id = self.gmask_token_id
            bos_id = self.bos_token_id
            if gmask_id is None:
                gmask_id = 150001  # é»˜è®¤å€¼
            if bos_id is None:
                bos_id = 150004  # é»˜è®¤å€¼

            token_ids_0 = token_ids_0 + [gmask_id, bos_id]
            if token_ids_1 is not None:
                token_ids_0 = token_ids_0 + token_ids_1 + [self.eos_token_id]
            return token_ids_0

    return SimpleTokenizer(model_path)