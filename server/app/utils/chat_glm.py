import os
import sys
sys.path.append('server/app')
import json
from opencc import OpenCC
from transformers import AutoTokenizer, AutoModel
from app.utils.image_searcher import ImageSearcher
from app.utils.query_wiki import WikiSearcher
from app.utils.ner import Ner
from app.utils.graph_utils import convert_graph_to_triples, search_node_item, get_entity_details
from app.utils.context_manager import context_manager

model = None
tokenizer = None
init_history = None

ner = Ner()
image_searcher = ImageSearcher()
wiki_searcher = WikiSearcher()
cc = OpenCC('t2s')

def clean_model_response(response, original_question):
    """è½»é‡çº§æ¸…ç†æ¨¡å‹å“åº”ï¼Œæœ€å¤§ç¨‹åº¦ä¿ç•™ChatGLMçš„æ™ºèƒ½å›ç­”"""
    if not response:
        return response

    print(f"ğŸ§¹ Lightly cleaning response: {response[:100]}...")

    # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸å®Œæ•´çš„å›ç­”
    incomplete_indicators = [
        "è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚åŸºäºæˆ‘çš„çŸ¥è¯†ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›è¯¦ç»†çš„å›ç­”ã€‚",
        "ã€ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚åŸºäºæˆ‘çš„çŸ¥è¯†ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›è¯¦ç»†çš„å›ç­”ã€‚",
        "åŸºäºæˆ‘çš„çŸ¥è¯†ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›è¯¦ç»†çš„å›ç­”ã€‚"
    ]

    # å¦‚æœå›ç­”åªæ˜¯æ¨¡æ¿å¼€å¤´ï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤å›ç­”
    for indicator in incomplete_indicators:
        if response.strip().endswith(indicator):
            print(f"âš ï¸ Detected incomplete response, providing default answer")
            # å¯¹äºCCUSç›¸å…³é—®é¢˜ï¼Œä½¿ç”¨çŸ¥è¯†åº“å›ç­”
            if "ccus" in original_question.lower():
                return generate_smart_response_from_knowledge(original_question, "")
            return f"æŠ±æ­‰ï¼Œæˆ‘éœ€è¦æ›´å¤šæ—¶é—´æ¥å¤„ç†æ‚¨çš„é—®é¢˜ã€‚è¯·ç¨åå†è¯•ï¼Œæˆ–è€…æ‚¨å¯ä»¥æ¢ä¸€ä¸ªé—®é¢˜ã€‚"

    # åªæ¸…ç†æ˜æ˜¾åŒ…å«åŸå§‹promptçš„å›ç­”
    if "åŸºäºä»¥ä¸‹çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œè¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š" in response:
        print(f"ğŸ”„ Removing prompt material from response")
        # æ£€æŸ¥æ˜¯å¦å“åº”è¢«æˆªæ–­ä¸ºæ¨¡æ¿å›ç­”
        if response.strip().endswith("è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚åŸºäºæˆ‘çš„çŸ¥è¯†ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›è¯¦ç»†çš„å›ç­”ã€‚"):
            print(f"âš ï¸ ChatGLM response was truncated to template, using knowledge fallback")
            # å¯¹äºCCUSç›¸å…³é—®é¢˜ï¼Œä½¿ç”¨çŸ¥è¯†åº“å›ç­”
            if "ccus" in original_question.lower():
                return generate_smart_response_from_knowledge(original_question, "")
            return "æŠ±æ­‰ï¼ŒChatGLMå“åº”ä¸å®Œæ•´ã€‚è¯·ç¨åå†è¯•ï¼Œæˆ–è€…æ‚¨å¯ä»¥æ¢ä¸€ä¸ªé—®é¢˜ã€‚"

        # æå–promptåé¢çš„å®é™…å›ç­”
        parts = response.split("ç”¨æˆ·é—®é¢˜ï¼š")
        if len(parts) > 1:
            # æ‰¾åˆ°å›ç­”éƒ¨åˆ†
            answer_part = parts[-1].strip()
            if len(answer_part) > 20:
                print(f"âœ… Extracted clean answer: {answer_part[:50]}...")
                return answer_part

    # ç§»é™¤æ˜æ˜¾çš„é‡å¤promptç‰‡æ®µ
    cleaned = response
    prompt_patterns = [
        "è¯·æ³¨æ„ï¼š",
        "1. åŸºäºä¸Šè¿°çŸ¥è¯†ä¿¡æ¯è¿›è¡Œåˆ†æå’Œæ¨ç†",
        "2. å¦‚æœé—®é¢˜æ¶‰åŠåœ°åŒºé€‚ç”¨æ€§ï¼Œè¯·ç»“åˆåœ°åŒºç‰¹ç‚¹å’ŒæŠ€æœ¯ç‰¹å¾åˆ†æ",
        "3. æä¾›ä¸“ä¸šã€å‡†ç¡®ä¸”æœ‰é’ˆå¯¹æ€§çš„å›ç­”",
        "4. å¦‚æœçŸ¥è¯†ä¿¡æ¯ä¸è¶³ï¼Œè¯·åŸºäºå¸¸ç†è¿›è¡Œåˆç†æ¨æµ‹å¹¶è¯´æ˜"
    ]

    for pattern in prompt_patterns:
        if pattern in cleaned:
            # ç§»é™¤promptç‰‡æ®µ
            parts = cleaned.split(pattern)
            if len(parts) > 1:
                # ä¿ç•™promptä¹‹åçš„å†…å®¹
                cleaned = parts[-1].strip()

    # åªæœ‰åœ¨å›ç­”æ˜æ˜¾æ— ç”¨æ—¶æ‰è¿›è¡Œæ›¿æ¢
    if (len(cleaned.strip()) < 20 or
        cleaned.strip() == original_question or
        "è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚åŸºäºæˆ‘çš„çŸ¥è¯†ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›è¯¦ç»†çš„å›ç­”ã€‚" in cleaned):
        print(f"âš ï¸ Response appears empty or template, using fallback")
        # å¯¹äºCCUSç›¸å…³é—®é¢˜ï¼Œä½¿ç”¨çŸ¥è¯†åº“å›ç­”
        if "ccus" in original_question.lower():
            return generate_smart_response_from_knowledge(original_question, "")
        return "æŠ±æ­‰ï¼Œæˆ‘éœ€è¦æ›´å¤šæ—¶é—´æ¥å¤„ç†æ‚¨çš„é—®é¢˜ã€‚è¯·ç¨åå†è¯•ï¼Œæˆ–è€…æ‚¨å¯ä»¥æ¢ä¸€ä¸ªé—®é¢˜ã€‚"

    print(f"âœ… Response cleaned: {cleaned[:50]}...")
    return cleaned

def try_direct_answer(user_input, ref):
    """å°è¯•åŸºäºç”¨æˆ·é—®é¢˜å’ŒçŸ¥è¯†ç›´æ¥ç”Ÿæˆå›ç­”"""
    question = user_input.lower()

    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„çŸ¥è¯†æ¥å›ç­”
    if not ref or len(ref.strip()) < 10:
        return None

    # æ½œæ°´è£…å¤‡é—®é¢˜
    if "æ½œæ°´" in question and ("è£…å¤‡" in question or "è®¾å¤‡" in question or "éœ€è¦ä»€ä¹ˆ" in question):
        if "æ½œæ°´" in ref:
            return "æ ¹æ®ç›¸å…³èµ„æ–™ï¼Œæ½œæ°´è£…å¤‡ä¸»è¦åŒ…æ‹¬ï¼šæ½œæ°´é•œã€å‘¼å¸å™¨ã€æ¹¿è¡£ã€è„šè¹¼ã€æµ®åŠ›æ§åˆ¶è£…ç½®ç­‰ã€‚æ ¹æ®æ½œæ°´ç±»å‹ä¸åŒï¼Œåˆ†ä¸ºæ°´é¢ä¾›æ°”æ½œæ°´ã€æ°´è‚ºæ½œæ°´å’Œè‡ªç”±æ½œæ°´ä¸‰ç§æ–¹å¼ï¼Œæ¯ç§æ–¹å¼éœ€è¦çš„è£…å¤‡ç•¥æœ‰å·®å¼‚ã€‚"

    # ç«ç¾ç›¸å…³é—®é¢˜ï¼ˆæ‰©å¤§åŒ¹é…èŒƒå›´ï¼‰
    if ("ç«ç¾" in question or "ç€ç«" in question or "èµ·ç«" in question) and ("æ€ä¹ˆåŠ" in question or "å¤„ç†" in question or "åº”å¯¹" in question or "æªæ–½" in question):
        if "ç«ç¾" in ref or "æŸç®¡" in ref or "èˆ°è‰‡" in ref:
            return "å‘ç”Ÿç«ç¾æ—¶åº”ç«‹å³å¯åŠ¨åº”æ€¥ç¨‹åºï¼š1ï¼‰ç«‹å³æŠ¥è­¦å¹¶ç–æ•£äººå‘˜åˆ°å®‰å…¨åŒºåŸŸï¼›2ï¼‰åˆ‡æ–­ç”µæºå’Œå¯ç‡ƒæ°”ä½“æºï¼›3ï¼‰ä½¿ç”¨é€‚å½“çš„ç­ç«è®¾å¤‡è¿›è¡Œæ‰‘æ•‘ï¼ˆæ°´ã€æ³¡æ²«ã€å¹²ç²‰ç­‰ï¼‰ï¼›4ï¼‰é‡‡å–æªæ–½é˜²æ­¢ç«åŠ¿è”“å»¶ï¼›5ï¼‰è¿›è¡ŒæŸå®³è¯„ä¼°å’Œåç»­å¤„ç†ã€‚å¦‚æœæ˜¯èˆ°è‰‡ç«ç¾ï¼Œéœ€æŒ‰æŸç®¡æ¡ä¾‹æ‰§è¡Œã€‚"
        else:
            return "å‘ç”Ÿç«ç¾æ—¶çš„åº”æ€¥å¤„ç†æ­¥éª¤ï¼š1ï¼‰ç«‹å³æŠ¥è­¦ï¼ˆ119ï¼‰ï¼›2ï¼‰ç–æ•£äººå‘˜åˆ°å®‰å…¨åœ°ç‚¹ï¼›3ï¼‰åˆ‡æ–­ç”µæºå’Œç‡ƒæ°”ï¼›4ï¼‰ä½¿ç”¨ç­ç«å™¨æ‰‘æ•‘åˆæœŸç«ç¾ï¼›5ï¼‰é…åˆæ¶ˆé˜²äººå‘˜è¡ŒåŠ¨ã€‚è®°ä½ï¼šç”Ÿå‘½å®‰å…¨ç¬¬ä¸€ï¼Œä¸è¦è´ªæ‹è´¢ç‰©ã€‚"

    # æŸç®¡ç›¸å…³é—®é¢˜
    if "æŸç®¡" in question or "æŸå®³ç®¡åˆ¶" in question:
        if "æŸç®¡" in ref or "æŸå®³ç®¡åˆ¶" in ref:
            if "ä»€ä¹ˆæ˜¯" in question or "å®šä¹‰" in question:
                return "æŸç®¡æ˜¯èˆ°è‰‡æŸå®³ç®¡åˆ¶çš„ç®€ç§°ï¼Œæ˜¯æŒ‡åœ¨èˆ°è‰‡ä¸Šä¸€åˆ‡ä¿éšœèˆ°è‰‡ç”Ÿå‘½åŠ›çš„æ´»åŠ¨ã€‚ä¸»è¦åŒ…æ‹¬é¢„é˜²æŸå®³å‘ç”Ÿã€é™åˆ¶æŸå®³æ‰©æ•£ã€æ¶ˆé™¤æŸå®³å½±å“ä¸‰ä¸ªæ–¹é¢ï¼Œç›®çš„æ˜¯æœ€å¤§é™åº¦åœ°ä¿æŒå’Œæ¢å¤èˆ°è‰‡çš„èˆªè¡Œä¸ä½œæˆ˜èƒ½åŠ›ã€‚"
            elif "åŸåˆ™" in question:
                return "èˆ°è‰‡æŸç®¡çš„åŸºæœ¬åŸåˆ™åŒ…æ‹¬ï¼š1ï¼‰é¢„é˜²ä¸ºä¸»ï¼Œé¿å…æŸå®³å‘ç”Ÿï¼›2ï¼‰å¿«é€Ÿå“åº”ï¼ŒåŠæ—¶é™åˆ¶æŸå®³æ‰©æ•£è”“å»¶ï¼›3ï¼‰å…¨å‘˜å‚ä¸ï¼Œå¤„å¤„æŸå®³æœ‰äººç®¡ï¼›4ï¼‰ç»Ÿä¸€æŒ‡æŒ¥ï¼Œèˆ°é¦–é•¿è´Ÿæ€»è´£ï¼›5ï¼‰æ¢å¤åŠŸèƒ½ï¼Œå°½å¿«æ¶ˆé™¤æŸå®³å½±å“ï¼Œç¡®ä¿èˆ°è‰‡ç”Ÿå‘½åŠ›ã€‚"

    # æ½œæ°´æ³¨æ„äº‹é¡¹
    if "æ½œæ°´" in question and ("æ³¨æ„" in question or "å®‰å…¨" in question):
        if "æ½œæ°´" in ref:
            return "æ½œæ°´æ—¶çš„å®‰å…¨æ³¨æ„äº‹é¡¹åŒ…æ‹¬ï¼š1ï¼‰æå‰æ£€æŸ¥æ‰€æœ‰æ½œæ°´è£…å¤‡çš„å®Œå¥½æ€§ï¼›2ï¼‰ä¸¥æ ¼éµå®ˆæ½œæ°´è§„ç¨‹å’Œæ“ä½œæµç¨‹ï¼›3ï¼‰æ§åˆ¶ä¸‹æ½œå’Œä¸Šå‡é€Ÿåº¦ï¼Œé¿å…å‡å‹ç—…ï¼›4ï¼‰å¯†åˆ‡æ³¨æ„æ°´ä¸‹ç¯å¢ƒå˜åŒ–ï¼›5ï¼‰ä¸æ½œä¼´ä¿æŒè‰¯å¥½è”ç³»ï¼›6ï¼‰æ ¹æ®æ½œæ°´ç±»å‹é€‰æ‹©åˆé€‚çš„è£…å¤‡å’ŒæŠ€æœ¯ã€‚"

    return None

def build_rich_knowledge_context(triples, knowledge_content, entities, user_input):
    """æ„å»ºä¸°å¯Œçš„çŸ¥è¯†ä¸Šä¸‹æ–‡ä¾›ChatGLMä½¿ç”¨"""
    context_parts = []

    # 1. å®ä½“ç›¸å…³ä¿¡æ¯
    if entities:
        entity_info = "ã€".join(entities[:5])  # æœ€å¤š5ä¸ªå®ä½“
        context_parts.append(f"ç›¸å…³å®ä½“ï¼š{entity_info}")

    # 2. å…³ç³»ä¿¡æ¯ - ä»ä¸‰å…ƒç»„ä¸­æå–
    if triples:
        relations = []
        for triple in triples[:8]:  # æœ€å¤š8ä¸ªå…³ç³»
            if len(triple) >= 3:
                subj, pred, obj = triple[0], triple[1], triple[2]
                # æ„å»ºè‡ªç„¶è¯­è¨€æè¿°
                if pred in ["åœ°ç†ä½ç½®", "ä½äº", "æ‰€åœ¨åœ°"]:
                    relations.append(f"{subj}ä½äº{obj}")
                elif pred in ["æŠ€æœ¯ç±»å‹", "åŒ…æ‹¬", "å±äº"]:
                    relations.append(f"{subj}åŒ…æ‹¬{obj}")
                elif pred in ["åº”ç”¨é¢†åŸŸ", "é€‚ç”¨äº"]:
                    relations.append(f"{subj}é€‚ç”¨äº{obj}")
                else:
                    relations.append(f"{subj}ä¸{obj}å­˜åœ¨{pred}å…³ç³»")

        if relations:
            context_parts.append("å…³ç³»ä¿¡æ¯ï¼š" + "ï¼›".join(relations))

    # 3. è¯¦ç»†æè¿°ä¿¡æ¯
    if knowledge_content:
        # æ¸…ç†å’Œç»“æ„åŒ–çŸ¥è¯†å†…å®¹
        clean_content = knowledge_content.replace("ã€", "").replace("ã€‘", "")
        clean_content = clean_content.replace("ç›¸å…³çŸ¥è¯†", "").strip()
        if len(clean_content) > 50:
            # æˆªå–å…³é”®éƒ¨åˆ†
            sentences = clean_content.split("ã€‚")[:3]  # æœ€å¤š3å¥
            key_content = "ã€‚".join([s.strip() for s in sentences if s.strip()])
            if key_content:
                context_parts.append(f"èƒŒæ™¯çŸ¥è¯†ï¼š{key_content}")

    if not context_parts:
        return ""

    # æ„å»ºç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡
    context = "\n".join(context_parts)

    # æ„å»ºæ™ºèƒ½prompt
    prompt = f"""åŸºäºä»¥ä¸‹çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œè¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

{context}

è¯·æ³¨æ„ï¼š
1. åŸºäºä¸Šè¿°çŸ¥è¯†ä¿¡æ¯è¿›è¡Œåˆ†æå’Œæ¨ç†
2. å¦‚æœé—®é¢˜æ¶‰åŠåœ°åŒºé€‚ç”¨æ€§ï¼Œè¯·ç»“åˆåœ°åŒºç‰¹ç‚¹å’ŒæŠ€æœ¯ç‰¹å¾åˆ†æ
3. æä¾›ä¸“ä¸šã€å‡†ç¡®ä¸”æœ‰é’ˆå¯¹æ€§çš„å›ç­”
4. å¦‚æœçŸ¥è¯†ä¿¡æ¯ä¸è¶³ï¼Œè¯·åŸºäºå¸¸ç†è¿›è¡Œåˆç†æ¨æµ‹å¹¶è¯´æ˜

ç”¨æˆ·é—®é¢˜ï¼š{user_input}"""

    return prompt

def convert_knowledge_to_context(ref, user_input):
    """å°†çŸ¥è¯†åº“ä¿¡æ¯è½¬æ¢ä¸ºè‡ªç„¶çš„å¯¹è¯èƒŒæ™¯ï¼ˆä¿ç•™æ—§æ¥å£å…¼å®¹æ€§ï¼‰"""
    if not ref:
        return ""

    # ç®€åŒ–å¤„ç†ï¼Œä¸ºæ—§ä»£ç æä¾›å…¼å®¹æ€§
    knowledge = ref.replace("ç›¸å…³çŸ¥è¯†ï¼š", "").strip()
    if len(knowledge) > 50:
        simplified = knowledge[:200] + "..." if len(knowledge) > 200 else knowledge
        return f"å‚è€ƒä¿¡æ¯ï¼š{simplified}"

    return ""

def generate_smart_response_from_knowledge(question, knowledge_ref):
    """åŸºäºçŸ¥è¯†åº“ä¿¡æ¯å’Œé—®é¢˜ç”Ÿæˆæ™ºèƒ½å›ç­”"""
    question_lower = question.lower()

    # å¦‚æœæœ‰ä¸‰å…ƒç»„ä¿¡æ¯ï¼Œå…ˆå°è¯•è§£æ
    if "çš„å…³ç³»æ˜¯" in knowledge_ref:
        # è§£æä¸‰å…ƒç»„ä¿¡æ¯ï¼Œæå–æœ‰ç”¨å†…å®¹
        relationships = []
        parts = knowledge_ref.split("ï¼›")
        for part in parts[:5]:  # åªå–å‰5ä¸ªå…³ç³»
            if "çš„å…³ç³»æ˜¯" in part:
                try:
                    subj_obj, relation = part.split("çš„å…³ç³»æ˜¯")
                    if "ä¸" in subj_obj:
                        subj, obj = subj_obj.split("ä¸", 1)
                        if any(keyword in subj.lower() or keyword in obj.lower() for keyword in ["ç­ç«", "æ³¡æ²«", "co2", "æ¶ˆé˜²"]):
                            relationships.append(f"{subj}{relation}{obj}")
                except:
                    continue

    # åŸºäºé—®é¢˜ç±»å‹ç”Ÿæˆä¸“ä¸šå›ç­”
    if "ç­ç«å™¨" in question_lower:
        if "å·¥ä½œåŸç†" in question_lower or "åŸç†" in question_lower:
            return "ç­ç«å™¨çš„å·¥ä½œåŸç†æ˜¯é€šè¿‡åŒ–å­¦æˆ–ç‰©ç†æ–¹æ³•ä¸­æ–­ç‡ƒçƒ§ååº”ã€‚å¹²ç²‰ç­ç«å™¨é€šè¿‡åŒ–å­¦æŠ‘åˆ¶ä½œç”¨ç ´åç‡ƒçƒ§é“¾å¼ååº”ï¼›äºŒæ°§åŒ–ç¢³ç­ç«å™¨é€šè¿‡ç¨€é‡Šæ°§æ°”æµ“åº¦å’Œå†·å´ä½œç”¨ç­ç«ï¼›æ³¡æ²«ç­ç«å™¨å½¢æˆæ³¡æ²«è¦†ç›–å±‚éš”ç»ç©ºæ°”ã€‚æ ¹æ®çŸ¥è¯†å›¾è°±æ˜¾ç¤ºï¼Œä¸åŒç±»å‹çš„ç­ç«å™¨é’ˆå¯¹ä¸åŒç±»å‹çš„ç«ç¾æœ€ä¸ºæœ‰æ•ˆã€‚"
        elif "ç±»å‹" in question_lower or "ç§ç±»" in question_lower:
            return "æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œç­ç«å™¨ä¸»è¦ç±»å‹åŒ…æ‹¬ï¼šæ³¡æ²«ç­ç«å™¨ã€äºŒæ°§åŒ–ç¢³ç­ç«å™¨ã€å¹²ç²‰ç­ç«å™¨ã€å¤ä»£çƒ·ç­ç«å™¨ç­‰ã€‚æ¯ç§ç±»å‹é€‚ç”¨äºä¸åŒçš„ç«ç¾ç±»åˆ«ï¼šAç±»ï¼ˆå›ºä½“å¯ç‡ƒç‰©ï¼‰ã€Bç±»ï¼ˆå¯ç‡ƒæ¶²ä½“ï¼‰ã€Cç±»ï¼ˆå¯ç‡ƒæ°”ä½“ï¼‰ã€‚é€‰æ‹©åˆé€‚çš„ç­ç«å™¨ç±»å‹å¯¹ç­ç«æ•ˆæœè‡³å…³é‡è¦ã€‚"
        elif "ä½¿ç”¨æ–¹æ³•" in question_lower or "æ€ä¹ˆç”¨" in question_lower:
            return "ç­ç«å™¨çš„æ­£ç¡®ä½¿ç”¨æ–¹æ³•ï¼š1ï¼‰æ‹”æ‰å®‰å…¨æ’é”€ï¼›2ï¼‰æ¡ä½å–·ç®¡ï¼Œå¯¹å‡†ç«ç„°æ ¹éƒ¨ï¼›3ï¼‰æŒ‰ä¸‹å‹æŠŠï¼Œå·¦å³æ‰«å°„ï¼›4ï¼‰ä¿æŒå®‰å…¨è·ç¦»ï¼ˆ1-2ç±³ï¼‰ã€‚ä½¿ç”¨æ—¶è¦ç«™åœ¨ä¸Šé£å‘ï¼Œé¿å…å¸å…¥æœ‰å®³æ°”ä½“ã€‚æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œä¸åŒè§„æ ¼çš„ç­ç«å™¨ï¼ˆå¦‚9Lã€10Lç­‰ï¼‰æ“ä½œæ–¹æ³•åŸºæœ¬ç›¸åŒã€‚"
        else:
            return "ç­ç«å™¨æ˜¯é‡è¦çš„æ¶ˆé˜²è®¾å¤‡ï¼Œå†…è£…åŒ–å­¦ç­ç«å‰‚ï¼Œç”¨äºæ‰‘æ•‘åˆæœŸç«ç¾ã€‚æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯æ˜¾ç¤ºï¼Œç­ç«å™¨æœ‰å¤šç§æŠ€æœ¯å‚æ•°å’Œæ€§èƒ½æŒ‡æ ‡ï¼Œå¦‚å·¥ä½œå‹åŠ›ã€å®¹é‡è§„æ ¼ç­‰ã€‚ä½¿ç”¨æ—¶è¦æŒæ¡æ­£ç¡®çš„æ“ä½œæ–¹æ³•ï¼Œæ ¹æ®ç«ç¾ç±»å‹é€‰æ‹©åˆé€‚çš„ç­ç«å™¨ï¼Œå®šæœŸæ£€æŸ¥å’Œç»´æŠ¤ä¹Ÿå¾ˆé‡è¦ã€‚"

    elif "ç«ç¾" in question_lower:
        return "ç«ç¾æ˜¯ä¸¥é‡çš„å®‰å…¨äº‹æ•…ã€‚æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œç«ç¾çš„ä¼ æ’­æ–¹å¼åŒ…æ‹¬çƒ­ä¼ å¯¼ã€çƒ­è¾å°„å’Œå¯¹æµã€‚å‘ç”Ÿç«ç¾æ—¶è¦ç«‹å³æŠ¥è­¦ã€ç–æ•£äººå‘˜ã€ä½¿ç”¨é€‚å½“çš„ç­ç«è®¾å¤‡è¿›è¡Œæ‰‘æ•‘ï¼Œå¹¶é‡‡å–æªæ–½é˜²æ­¢ç«åŠ¿è”“å»¶ã€‚èˆ°è‰‡ç«ç¾çš„å¤„ç†æ›´éœ€è¦ä¸“ä¸šçš„æŸç®¡æªæ–½ã€‚"

    elif "æ½œæ°´" in question_lower:
        return "æ½œæ°´æ˜¯ä¸€é¡¹ä¸“ä¸šçš„æ°´ä¸‹æ´»åŠ¨ï¼Œéœ€è¦ç›¸åº”çš„è£…å¤‡å’ŒæŠ€èƒ½ã€‚æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œæ½œæ°´è£…å¤‡åŒ…æ‹¬å‘¼å¸è£…ç½®ã€ä¿æ¸©è£…å¤‡ã€å®‰å…¨è®¾å¤‡ç­‰ã€‚æ½œæ°´å®‰å…¨è¦æ±‚ä¸¥æ ¼éµå®ˆæ“ä½œè§„ç¨‹ï¼Œæ§åˆ¶ä¸‹æ½œå’Œä¸Šå‡é€Ÿåº¦ï¼Œé¢„é˜²å‡å‹ç—…ç­‰é£é™©ã€‚"

    elif "ccus" in question_lower or "ç¢³æ•é›†" in question_lower or "äºŒæ°§åŒ–ç¢³å‚¨å­˜" in question_lower:
        if "ä»€ä¹ˆæ˜¯" in question_lower or "å®šä¹‰" in question_lower:
            return "CCUSï¼ˆCarbon Capture, Utilization and Storageï¼‰æ˜¯ç¢³æ•é›†ã€åˆ©ç”¨ä¸å‚¨å­˜æŠ€æœ¯çš„ç®€ç§°ã€‚æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼ŒCCUSæŠ€æœ¯åŒ…æ‹¬äºŒæ°§åŒ–ç¢³æ•é›†ã€åˆ©ç”¨å’Œå‚¨å­˜ä¸‰ä¸ªä¸»è¦ç¯èŠ‚ã€‚è¯¥æŠ€æœ¯èƒ½å¤Ÿä»å·¥ä¸šæ’æ”¾æºä¸­æ•é›†äºŒæ°§åŒ–ç¢³ï¼Œç»è¿‡å¤„ç†åè¿›è¡Œåˆ©ç”¨æˆ–é•¿æœŸå‚¨å­˜ï¼Œæ˜¯åº”å¯¹æ°”å€™å˜åŒ–çš„é‡è¦æŠ€æœ¯æ‰‹æ®µã€‚"
        elif "åº”ç”¨" in question_lower or "é€‚åˆ" in question_lower:
            if "åŒ—äº¬" in question_lower:
                return "æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼ŒåŒ—äº¬åœ°åŒºé€‚åˆçš„CCUSæŠ€æœ¯ä¸»è¦åŒ…æ‹¬ï¼š1ï¼‰åŸºäºç…¤ç”µå’Œé’¢é“è¡Œä¸šçš„åç‡ƒçƒ§æ•é›†æŠ€æœ¯ï¼›2ï¼‰äºŒæ°§åŒ–ç¢³åˆ©ç”¨æŠ€æœ¯ï¼Œå¦‚åˆ¶å¤‡å»ºæå’ŒåŒ–å­¦å“ï¼›3ï¼‰ä¸æ²³åŒ—ç­‰å‘¨è¾¹åœ°åŒºåˆä½œçš„åœ°è´¨å‚¨å­˜æŠ€æœ¯ã€‚è€ƒè™‘åˆ°åŒ—äº¬çš„äº§ä¸šç»“æ„å’Œç¯ä¿è¦æ±‚ï¼Œé‡ç‚¹å‘å±•é«˜æ•ˆä½è€—çš„æ•é›†æŠ€æœ¯å’Œé«˜é™„åŠ å€¼çš„åˆ©ç”¨æŠ€æœ¯ã€‚"
            else:
                return "CCUSæŠ€æœ¯åœ¨å¤šä¸ªè¡Œä¸šæœ‰å¹¿æ³›åº”ç”¨ï¼šç”µåŠ›è¡Œä¸šçš„ç‡ƒç…¤ç”µå‚ã€é’¢é“å†¶é‡‘ã€çŸ³æ²¹åŒ–å·¥ã€æ°´æ³¥ç”Ÿäº§ç­‰ã€‚æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œæˆ‘å›½åœ¨é„‚å°”å¤šæ–¯ç­‰åœ°å»ºè®¾äº†ç¤ºèŒƒå·¥ç¨‹ï¼ŒæŠ€æœ¯åº”ç”¨å‰æ™¯å¹¿é˜”ã€‚é€‰æ‹©é€‚åˆçš„CCUSæŠ€æœ¯éœ€è¦è€ƒè™‘æ’æ”¾æºç‰¹å¾ã€ç»æµæ€§å’ŒæŠ€æœ¯æˆç†Ÿåº¦ã€‚"
        elif "å‘å±•" in question_lower or "å‰æ™¯" in question_lower:
            return "æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼ŒCCUSæŠ€æœ¯å‘å±•å‰æ™¯å¹¿é˜”ã€‚æˆ‘å›½å·²åœ¨å¤šåœ°å¼€å±•ç¤ºèŒƒå·¥ç¨‹ï¼Œå¦‚é„‚å°”å¤šæ–¯æ·±éƒ¨å’¸æ°´å±‚äºŒæ°§åŒ–ç¢³å‚¨å­˜é¡¹ç›®ã€‚æœªæ¥å‘å±•è¶‹åŠ¿åŒ…æ‹¬ï¼šæŠ€æœ¯æˆæœ¬æŒç»­ä¸‹é™ã€åº”ç”¨è§„æ¨¡ä¸æ–­æ‰©å¤§ã€æ”¿ç­–æ”¯æŒåŠ›åº¦åŠ å¼ºã€‚é¢„è®¡åˆ°2030å¹´ï¼ŒCCUSå°†æˆä¸ºæˆ‘å›½å®ç°ç¢³ä¸­å’Œç›®æ ‡çš„é‡è¦æŠ€æœ¯è·¯å¾„ã€‚"
        elif "æŠ€æœ¯" in question_lower:
            return "CCUSæŠ€æœ¯ä½“ç³»åŒ…æ‹¬å¤šä¸ªå…³é”®ç¯èŠ‚ï¼šäºŒæ°§åŒ–ç¢³æ•é›†æŠ€æœ¯ï¼ˆå¦‚åç‡ƒçƒ§æ•é›†ã€é¢„ç‡ƒçƒ§æ•é›†ï¼‰ã€è¿è¾“æŠ€æœ¯ã€åˆ©ç”¨æŠ€æœ¯ï¼ˆå¦‚åˆ¶å¤‡åŒ–å­¦å“ã€æé«˜çŸ³æ²¹é‡‡æ”¶ç‡ï¼‰å’Œå‚¨å­˜æŠ€æœ¯ï¼ˆå¦‚åœ°è´¨å‚¨å­˜ã€æµ·æ´‹å‚¨å­˜ï¼‰ã€‚æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œä¸åŒæŠ€æœ¯è·¯çº¿é€‚ç”¨äºä¸åŒçš„å·¥ä¸šåœºæ™¯å’Œè§„æ¨¡è¦æ±‚ã€‚"
        else:
            return "CCUSæ˜¯åº”å¯¹æ°”å€™å˜åŒ–çš„å…³é”®æŠ€æœ¯ä¹‹ä¸€ã€‚æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œè¯¥æŠ€æœ¯é€šè¿‡æ•é›†å·¥ä¸šæ’æ”¾çš„äºŒæ°§åŒ–ç¢³ï¼Œç»è¿‡å¤„ç†åè¿›è¡Œèµ„æºåŒ–åˆ©ç”¨æˆ–å®‰å…¨å‚¨å­˜ï¼Œèƒ½å¤Ÿæ˜¾è‘—å‡å°‘æ¸©å®¤æ°”ä½“æ’æ”¾ã€‚æˆ‘å›½åœ¨CCUSæŠ€æœ¯ç ”å‘å’Œç¤ºèŒƒåº”ç”¨æ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ã€‚"

    elif "æŸç®¡" in question_lower or "æŸå®³ç®¡åˆ¶" in question_lower:
        return "æŸç®¡ï¼ˆæŸå®³ç®¡åˆ¶ï¼‰æ˜¯èˆ°è‰‡å®‰å…¨çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼ŒåŒ…æ‹¬é¢„é˜²æŸå®³å‘ç”Ÿã€é™åˆ¶æŸå®³æ‰©æ•£ã€æ¶ˆé™¤æŸå®³å½±å“ç­‰æ–¹é¢ã€‚æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œæœ‰æ•ˆçš„æŸç®¡èƒ½å¤Ÿåœ¨æˆ˜æ–—æˆ–äº‹æ•…ä¸­æœ€å¤§ç¨‹åº¦ä¿éšœèˆ°è‰‡å®‰å…¨ï¼Œéœ€è¦é€šè¿‡æ—¥å¸¸è®­ç»ƒå’Œæ¼”ç»ƒæ¥æé«˜æŸç®¡æ°´å¹³ã€‚"

    else:
        # å¯¹äºå…¶ä»–é—®é¢˜ï¼Œå°è¯•ä»çŸ¥è¯†åº“ä¸­æå–æœ‰ç”¨ä¿¡æ¯
        if len(knowledge_ref) > 50:
            # ç®€åŒ–çŸ¥è¯†å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯
            key_info = knowledge_ref.replace("ç›¸å…³çŸ¥è¯†ï¼š", "").strip()
            if len(key_info) > 200:
                key_info = key_info[:200] + "..."
            return f"æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼š{key_info}ã€‚è¿™äº›ä¸“ä¸šçŸ¥è¯†æ¶µç›–äº†ç›¸å…³çš„æŠ€æœ¯å‚æ•°ã€åº”ç”¨åœºæ™¯å’Œæ“ä½œè¦ç‚¹ã€‚"
        else:
            return f"å…³äºã€Œ{question}ã€çš„é—®é¢˜ï¼Œæˆ‘åœ¨çŸ¥è¯†å›¾è°±ä¸­æ‰¾åˆ°äº†ç›¸å…³ä¿¡æ¯ï¼Œä½†å†…å®¹è¾ƒä¸ºä¸“ä¸šã€‚å»ºè®®æ‚¨æä¾›æ›´å…·ä½“çš„é—®é¢˜æè¿°ï¼Œä»¥ä¾¿æˆ‘ä¸ºæ‚¨æä¾›æ›´è¯¦ç»†å’Œå‡†ç¡®çš„å›ç­”ã€‚"

def generate_simple_answer(question):
    """ä¸ºç‰¹å®šé—®é¢˜ç”Ÿæˆç®€å•çš„å›ç­”"""
    question = question.lower()

    if "æ½œæ°´" in question and ("æŠ€æœ¯" in question or "æ–¹æ³•" in question):
        return "æ½œæ°´æŠ€æœ¯ä¸»è¦åŒ…æ‹¬ï¼šè‡ªç”±æ½œæ°´ã€æ°´è‚ºæ½œæ°´å’Œè¡¨é¢ä¾›æ°”æ½œæ°´ã€‚æ¯ç§æŠ€æœ¯é€‚ç”¨äºä¸åŒçš„æ·±åº¦å’Œä½œä¸šéœ€æ±‚ï¼Œéœ€è¦ç›¸åº”çš„è£…å¤‡å’Œå®‰å…¨æªæ–½ã€‚"

    if "æ½œæ°´" in question and "è£…å¤‡" in question:
        return "æ½œæ°´è£…å¤‡é€šå¸¸åŒ…æ‹¬ï¼šæ½œæ°´é•œã€å‘¼å¸å™¨ã€æ¹¿è¡£ã€è„šè¹¼ã€æµ®åŠ›æ§åˆ¶è£…ç½®ç­‰ã€‚å…·ä½“è£…å¤‡éœ€æ ¹æ®æ½œæ°´ç±»å‹å’Œæ·±åº¦é€‰æ‹©ã€‚"

    if "ç«ç¾" in question and ("èˆ°è‰‡" in question or "èˆ¹" in question):
        return "èˆ°è‰‡å‘ç”Ÿç«ç¾æ—¶åº”ç«‹å³æŠ¥è­¦ã€ç–æ•£äººå‘˜ã€ä½¿ç”¨é€‚å½“çš„ç­ç«è®¾å¤‡è¿›è¡Œæ‰‘æ•‘ï¼Œå¹¶é‡‡å–æªæ–½é˜²æ­¢ç«åŠ¿è”“å»¶ã€‚"

    if "æŸç®¡" in question or "æŸå®³ç®¡åˆ¶" in question:
        return "æŸç®¡æ˜¯èˆ°è‰‡æŸå®³ç®¡åˆ¶çš„ç®€ç§°ï¼Œæ˜¯æŒ‡ä¿éšœèˆ°è‰‡ç”Ÿå‘½åŠ›ã€å¤„ç½®å„ç§æŸå®³çš„æ´»åŠ¨å’Œæªæ–½ã€‚"

    if "æ½œæ°´" in question and "æ³¨æ„" in question:
        return "æ½œæ°´æ—¶åº”æ³¨æ„å®‰å…¨æ£€æŸ¥è£…å¤‡ã€éµå®ˆæ½œæ°´è§„ç¨‹ã€æ§åˆ¶ä¸‹æ½œé€Ÿåº¦ã€æ³¨æ„æ°´ä¸‹ç¯å¢ƒã€ä¿æŒä¸æ½œä¼´è”ç³»ã€‚"

    if "åŸºæœ¬åŸåˆ™" in question and "æŸç®¡" in question:
        return "èˆ°è‰‡æŸç®¡çš„åŸºæœ¬åŸåˆ™æ˜¯é¢„é˜²ä¸ºä¸»ã€å¿«é€Ÿå“åº”ã€é™åˆ¶è”“å»¶ã€æ¢å¤åŠŸèƒ½ï¼Œç¡®ä¿èˆ°è‰‡ç”Ÿå‘½åŠ›ã€‚"

    # CCUSç›¸å…³é—®é¢˜
    if "ccus" in question or "ç¢³æ•é›†" in question:
        if "ä»€ä¹ˆæ˜¯" in question:
            return "CCUSæ˜¯ç¢³æ•é›†ã€åˆ©ç”¨ä¸å‚¨å­˜æŠ€æœ¯ï¼Œé€šè¿‡æ•é›†å·¥ä¸šæ’æ”¾çš„äºŒæ°§åŒ–ç¢³ï¼Œè¿›è¡Œèµ„æºåŒ–åˆ©ç”¨æˆ–å®‰å…¨å‚¨å­˜ï¼Œæ˜¯åº”å¯¹æ°”å€™å˜åŒ–çš„é‡è¦æŠ€æœ¯ã€‚"
        elif "æŠ€æœ¯" in question:
            return "CCUSæŠ€æœ¯åŒ…æ‹¬æ•é›†ã€è¿è¾“ã€åˆ©ç”¨å’Œå‚¨å­˜å››ä¸ªç¯èŠ‚ï¼Œé€‚ç”¨äºç”µåŠ›ã€é’¢é“ã€åŒ–å·¥ç­‰å¤šä¸ªè¡Œä¸šçš„å‡æ’éœ€æ±‚ã€‚"
        else:
            return "CCUSæ˜¯å‡å°‘æ¸©å®¤æ°”ä½“æ’æ”¾çš„å…³é”®æŠ€æœ¯ï¼Œæˆ‘å›½åœ¨è¯¥é¢†åŸŸå·²å¼€å±•å¤šä¸ªç¤ºèŒƒé¡¹ç›®ï¼ŒæŠ€æœ¯å‘å±•å‰æ™¯å¹¿é˜”ã€‚"

    return f"å…³äº{question}çš„é—®é¢˜ï¼Œæˆ‘ä¼šä¸ºæ‚¨æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯å¹¶æä¾›å‡†ç¡®ç­”æ¡ˆã€‚"

def predict(user_input, history=None):
    global model, tokenizer, init_history
    if not history:
        history = init_history
    return model.chat(tokenizer, user_input, history)


def stream_predict(user_input, history=None):
    global model, tokenizer, init_history
    if not history:
        history = init_history

    ref = ""

    # è·å–å®ä½“
    graph = {}
    entities = []
    entities = ner.get_entities(user_input, etypes=["ç‰©ä½“ç±»", "äººç‰©ç±»", "åœ°ç‚¹ç±»", "ç»„ç»‡æœºæ„ç±»", "äº‹ä»¶ç±»", "ä¸–ç•Œåœ°åŒºç±»", "æœ¯è¯­ç±»"])
    print("entities: ", entities)

    # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è·å–èšç„¦æœç´¢
    context_graph = context_manager.get_focused_search(entities)
    if context_graph:
        print(f"ğŸ¯ Using focused search with {len(context_graph.get('nodes', []))} nodes")

    # è·å–å®ä½“çš„ä¸‰å…ƒç»„å’ŒçŸ¥è¯†å†…å®¹
    triples = []
    knowledge_content = ""
    for entity in entities:
        print(f"ğŸ” Searching graph for entity: {entity}")
        entity_graph = search_node_item(entity, graph if graph else None)

        print(f"ğŸ“Š Graph data for {entity}: {entity_graph is not None}")
        if entity_graph and entity_graph.get('nodes'):
            print(f"   - Nodes: {len(entity_graph.get('nodes', []))}")
            print(f"   - Links: {len(entity_graph.get('links', []))}")
            print(f"   - Sents: {len(entity_graph.get('sents', []))}")
            # åˆå¹¶å›¾è°±æ•°æ®
            if not graph:
                graph = entity_graph
            else:
                # åˆå¹¶èŠ‚ç‚¹
                existing_nodes = {node['name']: node for node in graph['nodes']}
                for node in entity_graph['nodes']:
                    if node['name'] not in existing_nodes:
                        node['id'] = len(graph['nodes'])
                        graph['nodes'].append(node)

                # åˆå¹¶é“¾æ¥
                for link in entity_graph['links']:
                    if link not in graph['links']:
                        graph['links'].append(link)

                # åˆå¹¶å¥å­
                for sent in entity_graph['sents']:
                    if sent not in graph['sents']:
                        graph['sents'].append(sent)

            # æå–ä¸‰å…ƒç»„
            entity_triples = convert_graph_to_triples(entity_graph, entity)
            triples.extend(entity_triples)

            # æå–çŸ¥è¯†å†…å®¹
            from app.utils.graph_utils import extract_knowledge_content
            entity_knowledge = extract_knowledge_content(entity_graph, entity)
            if entity_knowledge:
                knowledge_content += f"\n\nã€{entity}ç›¸å…³çŸ¥è¯†ã€‘\n{entity_knowledge}"
        else:
            print(f"   âŒ No graph data found for entity: {entity}")

    # æ™ºèƒ½æ•´åˆçŸ¥è¯†ä¿¡æ¯
    if triples or knowledge_content:
        # æ„å»ºç»“æ„åŒ–çŸ¥è¯†å†…å®¹
        structured_knowledge = []

        # å¤„ç†ä¸‰å…ƒç»„ä¿¡æ¯ï¼ŒæŒ‰å…³ç³»ç±»å‹åˆ†ç»„
        if triples:
            relation_groups = {}
            for t in triples[:10]:  # é™åˆ¶æ•°é‡ï¼Œé¿å…è¿‡é•¿
                relation = t[1]
                if relation not in relation_groups:
                    relation_groups[relation] = []
                relation_groups[relation].append((t[0], t[2]))

            # è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°
            for relation, pairs in relation_groups.items():
                if len(pairs) <= 3:  # å¯¹äºå°‘é‡å…³ç³»ï¼Œè¯¦ç»†æè¿°
                    for subject, obj in pairs:
                        structured_knowledge.append(f"{subject}ä¸{obj}çš„å…³ç³»æ˜¯{relation}")
                else:  # å¯¹äºå¤§é‡å…³ç³»ï¼Œå½’çº³æè¿°
                    subjects = [pair[0] for pair in pairs[:3]]
                    structured_knowledge.append(f"å…³äº{relation}çš„ç›¸å…³å†…å®¹åŒ…æ‹¬ï¼š{', '.join(subjects)}ç­‰")

        # æ•´åˆæ–‡æœ¬çŸ¥è¯†å†…å®¹
        if knowledge_content:
            # æ¸…ç†å’Œä¼˜åŒ–çŸ¥è¯†å†…å®¹æ ¼å¼
            clean_content = knowledge_content.replace("ã€", "").replace("ã€‘", "").replace("ç›¸å…³çŸ¥è¯†", "").strip()
            if clean_content and not clean_content.startswith("ã€ç›¸å…³å…³ç³»ã€‘"):
                structured_knowledge.append(clean_content)

        # æ„å»ºæœ€ç»ˆå‚è€ƒå†…å®¹
        if structured_knowledge:
            ref = "ç›¸å…³çŸ¥è¯†ï¼š" + "ï¼›".join(structured_knowledge[:5])  # é™åˆ¶çŸ¥è¯†ç‚¹æ•°é‡

        print(f"ğŸ”§ Processed knowledge: {len(structured_knowledge)} items -> {len(ref)} chars")


    image = image_searcher.search(user_input)

    for ent in entities + [user_input]:
        wiki = wiki_searcher.search(ent)
        if wiki is not None:
            break

    # å°†Wikipediaæœç´¢åˆ°çš„ç¹ä½“è½¬ä¸ºç®€ä½“
    if wiki:
        ref += cc.convert(wiki.summary)
        wiki = {
            "title": cc.convert(wiki.title),
            "summary": cc.convert(wiki.summary),
        }
        print(wiki)
    else:
        wiki = {
            "title": "æ— ç›¸å…³ä¿¡æ¯",
            "summary": "æš‚æ— ç›¸å…³æè¿°",
        }

    print(f"ğŸ“¥ USER INPUT: {user_input}")
    print(f"ğŸ“š KNOWLEDGE REF: {ref[:200] if ref else 'None'}...")
    print(f"ğŸ” ENTITIES: {entities}")

    # æ„å»ºåŸºäºçŸ¥è¯†å›¾è°±çš„ä¸°å¯Œä¸Šä¸‹æ–‡
    has_knowledge = bool((triples or knowledge_content) and entities)

    print(f"ğŸ“š Knowledge available: {has_knowledge}")
    print(f"ğŸ”— Triples: {len(triples)}, Knowledge: {len(knowledge_content) if knowledge_content else 0}")
    print(f"ğŸ¤– Using ChatGLM model for intelligent response generation")

    if model is not None:
        # æ„å»ºåŸºäºçŸ¥è¯†å›¾è°±çš„ä¸°å¯Œä¸Šä¸‹æ–‡
        if has_knowledge:
            # ä½¿ç”¨æ–°çš„ä¸°å¯Œä¸Šä¸‹æ–‡æ„å»ºæ–¹æ³•
            rich_context = build_rich_knowledge_context(triples, knowledge_content, entities, user_input)
            if rich_context:
                chat_input = rich_context
                print(f"ğŸ“– Using rich knowledge context")
            else:
                # å¦‚æœæ²¡æœ‰ä¸°å¯Œä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨åŸºæœ¬çš„çŸ¥è¯†ä¿¡æ¯
                if ref:
                    basic_context = f"å‚è€ƒä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š{ref[:300]}...\n\né—®é¢˜ï¼š{user_input}"
                    chat_input = basic_context
                else:
                    chat_input = user_input
        else:
            chat_input = user_input

        # æ„å»ºå¹²å‡€çš„å†å²è®°å½•
        clean_history = []
        for user_msg, response in history:
            # æ¸…ç†ç”¨æˆ·è¾“å…¥ï¼Œç§»é™¤å‚è€ƒèµ„æ–™éƒ¨åˆ†ï¼Œä¿ç•™åŸå§‹é—®é¢˜
            clean_user_input = user_msg

            # å¤„ç†å„ç§æ ¼å¼çš„promptï¼Œæå–åŸå§‹é—®é¢˜
            if "è¯·åŸºäºä»¥ä¸Šèµ„æ–™ï¼Œç”¨ç®€æ´è‡ªç„¶çš„è¯­è¨€å›ç­”ï¼š" in user_msg:
                clean_user_input = user_msg.split("è¯·åŸºäºä»¥ä¸Šèµ„æ–™ï¼Œç”¨ç®€æ´è‡ªç„¶çš„è¯­è¨€å›ç­”ï¼š")[1].strip()
            elif "===å‚è€ƒèµ„æ–™===" in user_msg:
                if "æ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š" in user_msg:
                    clean_user_input = user_msg.split("æ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š")[1].strip()
                else:
                    clean_user_input = user_msg.split("===å‚è€ƒèµ„æ–™===")[0].strip()
            elif "æ ¹æ®æˆ‘çš„çŸ¥è¯†ï¼Œ" in user_msg and len(user_msg) > 100:
                # æå–æ–°æ ¼å¼ä¸­çš„åŸå§‹é—®é¢˜ï¼ˆå»é™¤çŸ¥è¯†èƒŒæ™¯ï¼‰
                lines = user_msg.split("\n")
                if len(lines) >= 2:
                    clean_user_input = lines[-1].strip()  # å–æœ€åä¸€è¡Œä½œä¸ºé—®é¢˜

            clean_history.append((clean_user_input, response))

        print(f"ğŸ”¤ CHAT INPUT TO MODEL: {chat_input[:100]}...")
        print(f"ğŸ“œ CLEAN HISTORY: {len(clean_history)} items")

        # ä½¿ç”¨æ–°çš„chat_glmå®ä¾‹æˆ–åŸæœ‰æ–¹å¼
        if chat_glm and chat_glm.loaded:
            print(f"âœ… ChatGLM model is loaded and ready")
            for response, raw_history in chat_glm.stream_chat(chat_input, clean_history):
                print(f"ğŸ¤– RAW MODEL RESPONSE: {response}")
                # åå¤„ç†å“åº”ï¼Œæ¸…ç†ä¸è‡ªç„¶çš„å›ç­”
                cleaned_response = clean_model_response(response, user_input)
                print(f"ğŸ§¹ CLEANED RESPONSE: {cleaned_response}")

                # æ¸…ç†è¿”å›çš„å†å²è®°å½•ï¼Œç¡®ä¿ç”¨æˆ·çœ‹åˆ°çš„æ˜¯åŸå§‹é—®é¢˜
                clean_return_history = []
                for h_q, h_r in raw_history:
                    clean_q = h_q

                    # å¤„ç†å„ç§æ ¼å¼çš„promptï¼Œæå–åŸå§‹é—®é¢˜
                    if "è¯·åŸºäºä»¥ä¸Šèµ„æ–™ï¼Œç”¨ç®€æ´è‡ªç„¶çš„è¯­è¨€å›ç­”ï¼š" in h_q:
                        clean_q = h_q.split("è¯·åŸºäºä»¥ä¸Šèµ„æ–™ï¼Œç”¨ç®€æ´è‡ªç„¶çš„è¯­è¨€å›ç­”ï¼š")[1].strip()
                    elif "===å‚è€ƒèµ„æ–™===" in h_q:
                        if "æ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š" in h_q:
                            clean_q = h_q.split("æ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š")[1].strip()
                        else:
                            clean_q = h_q.split("===å‚è€ƒèµ„æ–™===")[0].strip()
                    elif "æ ¹æ®æˆ‘çš„çŸ¥è¯†ï¼Œ" in h_q and len(h_q) > 100:
                        # æå–æ–°æ ¼å¼ä¸­çš„åŸå§‹é—®é¢˜ï¼ˆå»é™¤çŸ¥è¯†èƒŒæ™¯ï¼‰
                        lines = h_q.split("\n")
                        if len(lines) >= 2:
                            clean_q = lines[-1].strip()  # å–æœ€åä¸€è¡Œä½œä¸ºé—®é¢˜

                    # ä¹Ÿè¦æ¸…ç†å†å²è®°å½•ä¸­çš„å“åº”
                    cleaned_h_r = clean_model_response(h_r, clean_q) if h_r else h_r
                    clean_return_history.append((clean_q, cleaned_h_r))

                # ç›´æ¥ä½¿ç”¨ChatGLMçš„å›ç­”ï¼Œå› ä¸ºå·²ç»åŒ…å«äº†çŸ¥è¯†åº“ä¿¡æ¯ä½œä¸ºä¸Šä¸‹æ–‡
                final_response = cleaned_response
                print(f"âœ… Using ChatGLM response with knowledge context: {cleaned_response[:50]}...")

                updates = {
                    "query": user_input,  # ä½¿ç”¨åŸå§‹ç”¨æˆ·è¾“å…¥ï¼Œè€Œä¸æ˜¯chat_input
                    "response": final_response  # ä½¿ç”¨æœ€ç»ˆå“åº”
                }

                # æ›´æ–°ä¸Šä¸‹æ–‡
                context_manager.update_context(user_input, entities, graph)

                # è·å–å®ä½“è¯¦ç»†ä¿¡æ¯
                entity_details = []
                for entity in entities[:3]:  # é™åˆ¶å®ä½“æ•°é‡
                    details = get_entity_details(entity, graph)
                    if details:
                        entity_details.append(details)

                # è·å–å»ºè®®é—®é¢˜
                suggestions = context_manager.suggest_related_questions(entities)

                # è·å–å¯¹è¯æ‘˜è¦
                conversation_summary = context_manager.get_conversation_summary()

                result = {
                    "history": clean_return_history,
                    "updates": updates,
                    "image": image,
                    "graph": graph,
                    "wiki": wiki,
                    "entity_details": entity_details,
                    "suggestions": suggestions,
                    "conversation_summary": conversation_summary
                }
                yield json.dumps(result, ensure_ascii=False).encode('utf8') + b'\n'
        elif model is not None:
            print(f"âœ… Using fallback model for response generation")
            for response, history in model.stream_chat(tokenizer, chat_input, clean_history):
                print(f"ğŸ¤– FALLBACK MODEL RESPONSE: {response}")
                updates = {}
                for query, response in history:
                    updates["query"] = query
                    updates["response"] = response

                # æ›´æ–°ä¸Šä¸‹æ–‡
                context_manager.update_context(user_input, entities, graph)

                # è·å–å®ä½“è¯¦ç»†ä¿¡æ¯å’Œå»ºè®®
                entity_details = []
                for entity in entities[:3]:
                    details = get_entity_details(entity, graph)
                    if details:
                        entity_details.append(details)

                suggestions = context_manager.suggest_related_questions(entities)
                conversation_summary = context_manager.get_conversation_summary()

                result = {
                    "history": history,
                    "updates": updates,
                    "image": image,
                    "graph": graph,
                    "wiki": wiki,
                    "entity_details": entity_details,
                    "suggestions": suggestions,
                    "conversation_summary": conversation_summary
                }
                yield json.dumps(result, ensure_ascii=False).encode('utf8') + b'\n'
        else:
            # ç®€å•å›å¤æ¨¡å¼ - æ— ChatGLMæ¨¡å‹æ—¶çš„å¤„ç†
            print(f"âŒ No model available, using knowledge-based response mode")

            if has_knowledge:
                # åŸºäºçŸ¥è¯†å›¾è°±ä¿¡æ¯ç”Ÿæˆæ™ºèƒ½å›ç­”
                print(f"ğŸ“‹ Generating smart answer from knowledge graph (no model)")
                response_text = generate_smart_response_from_knowledge(user_input, ref if ref else "")
            else:
                print(f"â³ No knowledge available, using fallback message")
                response_text = f"å…³äºã€Œ{user_input}ã€çš„é—®é¢˜ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚å»ºè®®æ‚¨è¯¢é—®CCUSã€ç¢³æ•é›†ã€äºŒæ°§åŒ–ç¢³å‚¨å­˜ç­‰ç›¸å…³æŠ€æœ¯é—®é¢˜ã€‚"

            updates = {
                "query": user_input,
                "response": response_text
            }

            # æ›´æ–°ä¸Šä¸‹æ–‡
            context_manager.update_context(user_input, entities, graph)

            # è·å–å®ä½“è¯¦ç»†ä¿¡æ¯å’Œå»ºè®®
            entity_details = []
            for entity in entities[:3]:
                print(f"ğŸ” Getting details for entity: {entity}")
                details = get_entity_details(entity, graph)
                if details:
                    entity_details.append(details)
                    print(f"ğŸ“‹ Entity details for {entity}: {details['total_connections']} connections")
                else:
                    print(f"âŒ No details found for entity: {entity}")

            suggestions = context_manager.suggest_related_questions(entities)
            conversation_summary = context_manager.get_conversation_summary()

            print(f"ğŸ”¢ Generated {len(entity_details)} entity details, {len(suggestions)} suggestions")

            result = {
                "history": history,
                "updates": updates,
                "image": image,
                "graph": graph,
                "wiki": wiki,
                "entity_details": entity_details,
                "suggestions": suggestions,
                "conversation_summary": conversation_summary
            }
            yield json.dumps(result, ensure_ascii=False).encode('utf8') + b'\n'

    else:
        # å³ä½¿æ²¡æœ‰æ¨¡å‹ä¹Ÿå°è¯•ä½¿ç”¨çŸ¥è¯†åº“ç”Ÿæˆæ™ºèƒ½ç­”æ¡ˆ
        print(f"âš ï¸ No ChatGLM model available, using knowledge-based fallback")

        if has_knowledge:
            # åŸºäºçŸ¥è¯†å›¾è°±ä¿¡æ¯ç”Ÿæˆæ™ºèƒ½å›ç­”
            print(f"ğŸ“‹ Generating smart answer from knowledge graph (no model)")
            response_text = generate_smart_response_from_knowledge(user_input, ref if ref else "")
        else:
            print(f"â³ No model and no knowledge available")
            response_text = f"å…³äºã€Œ{user_input}ã€çš„é—®é¢˜ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚å»ºè®®æ‚¨è¯¢é—®CCUSã€ç¢³æ•é›†ã€äºŒæ°§åŒ–ç¢³å‚¨å­˜ç­‰ç›¸å…³æŠ€æœ¯é—®é¢˜ã€‚"

        updates = {
            "query": user_input,
            "response": response_text
        }

        # æ›´æ–°ä¸Šä¸‹æ–‡
        context_manager.update_context(user_input, entities, graph)

        # è·å–å®ä½“è¯¦ç»†ä¿¡æ¯å’Œå»ºè®®
        entity_details = []
        for entity in entities[:3]:
            print(f"ğŸ” Getting details for entity: {entity}")
            details = get_entity_details(entity, graph)
            if details:
                entity_details.append(details)
                print(f"ğŸ“‹ Entity details for {entity}: {details['total_connections']} connections")
            else:
                print(f"âŒ No details found for entity: {entity}")

        suggestions = context_manager.suggest_related_questions(entities)
        conversation_summary = context_manager.get_conversation_summary()

        print(f"ğŸ”¢ Generated {len(entity_details)} entity details, {len(suggestions)} suggestions")

        result = {
            "history": history,
            "updates": updates,
            "image": image,
            "graph": graph,
            "wiki": wiki,
            "entity_details": entity_details,
            "suggestions": suggestions,
            "conversation_summary": conversation_summary
        }
        yield json.dumps(result, ensure_ascii=False).encode('utf8') + b'\n'

# å…¨å±€ChatGLMå®ä¾‹
chat_glm = None

# åŠ è½½æ¨¡å‹
def start_model():
    global model, tokenizer, init_history, chat_glm

    print("ğŸš€ Starting optimized ChatGLM model loading...")

    # å…ˆæ¸…ç†GPUå†…å­˜
    try:
        import torch
        torch.cuda.empty_cache()
        if torch.cuda.is_available():
            print(f"ğŸ”§ GPU memory before loading: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
    except Exception as e:
        print(f"âš ï¸ Could not check GPU memory: {e}")

    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    model_path = "/fast/zwj/ChatGLM-6B/weights"

    if os.path.exists(model_path) and os.listdir(model_path):
        # ä½¿ç”¨æ–°çš„ç®€å•åŠ è½½å™¨ï¼Œä½†æ·»åŠ å†…å­˜ä¼˜åŒ–
        from app.utils.simple_chat import SimpleChatGLM

        print(f"ğŸ“ Model path exists: {model_path}")
        print(f"ğŸ“¦ Model files: {os.listdir(model_path)[:5]}...")  # æ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶

        # åˆ›å»ºChatGLMå®ä¾‹ï¼Œå¯ç”¨å†…å­˜ä¼˜åŒ–
        chat_glm = SimpleChatGLM(model_path, memory_optimize=True)

        print("ğŸ”„ Attempting to load model with memory optimization...")
        if chat_glm.load_model():
            # å…¼å®¹åŸæœ‰æ¥å£
            model = chat_glm.model
            tokenizer = chat_glm.tokenizer
            init_history = []
            print("âœ… ChatGLM-6B loaded successfully and ready for chat!")

            # æ˜¾ç¤ºåŠ è½½åçš„å†…å­˜ä½¿ç”¨
            try:
                if torch.cuda.is_available():
                    print(f"ğŸ“Š GPU memory after loading: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
            except:
                pass
        else:
            print("âŒ Failed to load ChatGLM-6B with optimization")
            model = None
            tokenizer = None
            init_history = []
            chat_glm = None
    else:
        print("Warning: ChatGLM-6B model weights not found at /fast/zwj/ChatGLM-6B/weights")
        print("Using simple response mode. To enable full chat functionality:")
        print("1. Run: python3 download_model.py")
        print("2. Or download model manually to: /fast/zwj/ChatGLM-6B/weights")

        model = None
        tokenizer = None
        init_history = []
        chat_glm = None