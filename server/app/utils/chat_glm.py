import os
import sys
sys.path.append('server/app')
import json
from opencc import OpenCC
from transformers import AutoTokenizer, AutoModel
from app.utils.image_searcher import ImageSearcher
from app.utils.query_wiki import WikiSearcher
from app.utils.ner import Ner
from app.utils.graph_utils import convert_graph_to_triples, search_node_item, get_entity_details
from app.utils.context_manager import context_manager

model = None
tokenizer = None
init_history = None

ner = Ner()
image_searcher = ImageSearcher()
wiki_searcher = WikiSearcher()
cc = OpenCC('t2s')

def clean_model_response(response, original_question):
    """è½»é‡çº§æ¸…ç†æ¨¡å‹å“åº”ï¼Œæœ€å¤§ç¨‹åº¦ä¿ç•™ChatGLMçš„æ™ºèƒ½å›ç­”"""
    if not response:
        return response

    print(f"ğŸ§¹ Lightly cleaning response: {response[:100]}...")

    # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸å®Œæ•´çš„å›ç­”
    incomplete_indicators = [
        "è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚åŸºäºæˆ‘çš„çŸ¥è¯†ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›è¯¦ç»†çš„å›ç­”ã€‚",
        "ã€ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚åŸºäºæˆ‘çš„çŸ¥è¯†ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›è¯¦ç»†çš„å›ç­”ã€‚",
        "åŸºäºæˆ‘çš„çŸ¥è¯†ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›è¯¦ç»†çš„å›ç­”ã€‚"
    ]

    # å¦‚æœå›ç­”åªæ˜¯æ¨¡æ¿å¼€å¤´ï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤å›ç­”
    for indicator in incomplete_indicators:
        if response.strip().endswith(indicator):
            print(f"âš ï¸ Detected incomplete response, providing default answer")
            # å¯¹äºCCUSç›¸å…³é—®é¢˜ï¼Œä½¿ç”¨çŸ¥è¯†åº“å›ç­”
            if "ccus" in original_question.lower():
                return generate_smart_response_from_knowledge(original_question, "")
            return f"æŠ±æ­‰ï¼Œæˆ‘éœ€è¦æ›´å¤šæ—¶é—´æ¥å¤„ç†æ‚¨çš„é—®é¢˜ã€‚è¯·ç¨åå†è¯•ï¼Œæˆ–è€…æ‚¨å¯ä»¥æ¢ä¸€ä¸ªé—®é¢˜ã€‚"

    # åªæ¸…ç†æ˜æ˜¾åŒ…å«åŸå§‹promptçš„å›ç­”
    if "åŸºäºä»¥ä¸‹çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œè¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š" in response:
        print(f"ğŸ”„ Removing prompt material from response")
        # æ£€æŸ¥æ˜¯å¦å“åº”è¢«æˆªæ–­ä¸ºæ¨¡æ¿å›ç­”
        if response.strip().endswith("è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚åŸºäºæˆ‘çš„çŸ¥è¯†ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›è¯¦ç»†çš„å›ç­”ã€‚"):
            print(f"âš ï¸ ChatGLM response was truncated to template, using knowledge fallback")
            # å¯¹äºCCUSç›¸å…³é—®é¢˜ï¼Œä½¿ç”¨çŸ¥è¯†åº“å›ç­”
            if "ccus" in original_question.lower():
                return generate_smart_response_from_knowledge(original_question, "")
            return "æŠ±æ­‰ï¼ŒChatGLMå“åº”ä¸å®Œæ•´ã€‚è¯·ç¨åå†è¯•ï¼Œæˆ–è€…æ‚¨å¯ä»¥æ¢ä¸€ä¸ªé—®é¢˜ã€‚"

        # æå–promptåé¢çš„å®é™…å›ç­”
        parts = response.split("ç”¨æˆ·é—®é¢˜ï¼š")
        if len(parts) > 1:
            # æ‰¾åˆ°å›ç­”éƒ¨åˆ†
            answer_part = parts[-1].strip()

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ— æ•ˆçš„æ¨¡æ¿å›ç­”
            if answer_part.endswith("ã€çš„é—®é¢˜ã€‚è®©æˆ‘åŸºäºæˆ‘çš„çŸ¥è¯†ä¸ºæ‚¨æä¾›åˆé€‚çš„å›ç­”ã€‚"):
                print(f"âš ï¸ Found template response in answer part, using knowledge fallback")
                # å¯¹äºCCUSç›¸å…³é—®é¢˜ï¼Œä½¿ç”¨çŸ¥è¯†åº“å›ç­”
                if any(word in original_question.lower() for word in ["ccus", "ç¢³æ•é›†", "ç¢³å‚¨å­˜", "ç¢³åˆ©ç”¨", "äºŒæ°§åŒ–ç¢³"]):
                    return generate_smart_response_from_knowledge(original_question, "")
                return "æŠ±æ­‰ï¼Œæˆ‘éœ€è¦æ›´å¤šæ—¶é—´æ¥å¤„ç†æ‚¨çš„é—®é¢˜ã€‚è¯·ç¨åå†è¯•ï¼Œæˆ–è€…æ‚¨å¯ä»¥æ¢ä¸€ä¸ªé—®é¢˜ã€‚"

            if len(answer_part) > 20:
                print(f"âœ… Extracted clean answer: {answer_part[:50]}...")
                return answer_part

    # ç§»é™¤æ˜æ˜¾çš„é‡å¤promptç‰‡æ®µ
    cleaned = response
    prompt_patterns = [
        "è¯·æ³¨æ„ï¼š",
        "1. åŸºäºä¸Šè¿°çŸ¥è¯†ä¿¡æ¯è¿›è¡Œåˆ†æå’Œæ¨ç†",
        "2. å¦‚æœé—®é¢˜æ¶‰åŠåœ°åŒºé€‚ç”¨æ€§ï¼Œè¯·ç»“åˆåœ°åŒºç‰¹ç‚¹å’ŒæŠ€æœ¯ç‰¹å¾åˆ†æ",
        "3. æä¾›ä¸“ä¸šã€å‡†ç¡®ä¸”æœ‰é’ˆå¯¹æ€§çš„å›ç­”",
        "4. å¦‚æœçŸ¥è¯†ä¿¡æ¯ä¸è¶³ï¼Œè¯·åŸºäºå¸¸ç†è¿›è¡Œåˆç†æ¨æµ‹å¹¶è¯´æ˜"
    ]

    for pattern in prompt_patterns:
        if pattern in cleaned:
            # ç§»é™¤promptç‰‡æ®µ
            parts = cleaned.split(pattern)
            if len(parts) > 1:
                # ä¿ç•™promptä¹‹åçš„å†…å®¹
                cleaned = parts[-1].strip()

    # åªæœ‰åœ¨å›ç­”æ˜æ˜¾æ— ç”¨æ—¶æ‰è¿›è¡Œæ›¿æ¢
    if (len(cleaned.strip()) < 20 or
        cleaned.strip() == original_question or
        "è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚åŸºäºæˆ‘çš„çŸ¥è¯†ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›è¯¦ç»†çš„å›ç­”ã€‚" in cleaned):
        print(f"âš ï¸ Response appears empty or template, using fallback")
        # å¯¹äºCCUSç›¸å…³é—®é¢˜ï¼Œä½¿ç”¨çŸ¥è¯†åº“å›ç­”
        if "ccus" in original_question.lower():
            return generate_smart_response_from_knowledge(original_question, "")
        return "æŠ±æ­‰ï¼Œæˆ‘éœ€è¦æ›´å¤šæ—¶é—´æ¥å¤„ç†æ‚¨çš„é—®é¢˜ã€‚è¯·ç¨åå†è¯•ï¼Œæˆ–è€…æ‚¨å¯ä»¥æ¢ä¸€ä¸ªé—®é¢˜ã€‚"

    print(f"âœ… Response cleaned: {cleaned[:50]}...")
    return cleaned

def try_direct_answer(user_input, ref):
    """å°è¯•åŸºäºç”¨æˆ·é—®é¢˜å’ŒçŸ¥è¯†ç›´æ¥ç”Ÿæˆå›ç­”"""
    question = user_input.lower()

    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„çŸ¥è¯†æ¥å›ç­”
    if not ref or len(ref.strip()) < 10:
        return None

    # CCUSæŠ€æœ¯åŸºç¡€é—®é¢˜
    if "ccus" in question.lower() or "ç¢³æ•é›†" in question or "äºŒæ°§åŒ–ç¢³å‚¨å­˜" in question:
        if "ä»€ä¹ˆæ˜¯" in question or "å®šä¹‰" in question:
            return "CCUSï¼ˆCarbon Capture, Utilization and Storageï¼‰æ˜¯ç¢³æ•é›†ã€åˆ©ç”¨ä¸å‚¨å­˜æŠ€æœ¯çš„ç®€ç§°ã€‚è¯¥æŠ€æœ¯é€šè¿‡æ•é›†å·¥ä¸šæ’æ”¾çš„äºŒæ°§åŒ–ç¢³ï¼Œç»è¿‡å¤„ç†åè¿›è¡Œèµ„æºåŒ–åˆ©ç”¨æˆ–å®‰å…¨å‚¨å­˜ï¼Œæ˜¯åº”å¯¹æ°”å€™å˜åŒ–çš„é‡è¦æŠ€æœ¯æ‰‹æ®µã€‚"
        elif "æŠ€æœ¯" in question:
            return "CCUSæŠ€æœ¯ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªç¯èŠ‚ï¼šç¢³æ•é›†ï¼ˆä»æ’æ”¾æºæ•é›†CO2ï¼‰ã€ç¢³åˆ©ç”¨ï¼ˆå°†CO2è½¬åŒ–ä¸ºæœ‰ç”¨äº§å“ï¼‰ã€ç¢³å‚¨å­˜ï¼ˆå°†CO2é•¿æœŸå®‰å…¨å‚¨å­˜ï¼‰ã€‚æŠ€æœ¯è·¯çº¿åŒ…æ‹¬åç‡ƒçƒ§æ•é›†ã€é¢„ç‡ƒçƒ§æ•é›†ã€å¯Œæ°§ç‡ƒçƒ§ç­‰å¤šç§æ–¹å¼ã€‚"
        elif "åº”ç”¨" in question:
            return "CCUSæŠ€æœ¯å¹¿æ³›åº”ç”¨äºç”µåŠ›ã€é’¢é“ã€æ°´æ³¥ã€çŸ³åŒ–ç­‰é«˜æ’æ”¾è¡Œä¸šã€‚ä¸»è¦åº”ç”¨åœºæ™¯åŒ…æ‹¬ç‡ƒç…¤ç”µå‚ç¢³æ•é›†ã€å·¥ä¸šè¿‡ç¨‹ç¢³åˆ©ç”¨ã€åœ°è´¨å‚¨å­˜ç­‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—å‡å°‘å·¥ä¸šæ¸©å®¤æ°”ä½“æ’æ”¾ã€‚"

    return None

def build_rich_knowledge_context(triples, knowledge_content, entities, user_input):
    """æ„å»ºä¸°å¯Œçš„çŸ¥è¯†ä¸Šä¸‹æ–‡ä¾›ChatGLMä½¿ç”¨"""
    context_parts = []

    # 1. å®ä½“ç›¸å…³ä¿¡æ¯
    if entities:
        entity_info = "ã€".join(entities[:5])  # æœ€å¤š5ä¸ªå®ä½“
        context_parts.append(f"ç›¸å…³å®ä½“ï¼š{entity_info}")

    # 2. å…³ç³»ä¿¡æ¯ - ä»ä¸‰å…ƒç»„ä¸­æå–
    if triples:
        relations = []
        for triple in triples[:8]:  # æœ€å¤š8ä¸ªå…³ç³»
            if len(triple) >= 3:
                subj, pred, obj = triple[0], triple[1], triple[2]
                # æ„å»ºè‡ªç„¶è¯­è¨€æè¿°
                if pred in ["åœ°ç†ä½ç½®", "ä½äº", "æ‰€åœ¨åœ°"]:
                    relations.append(f"{subj}ä½äº{obj}")
                elif pred in ["æŠ€æœ¯ç±»å‹", "åŒ…æ‹¬", "å±äº"]:
                    relations.append(f"{subj}åŒ…æ‹¬{obj}")
                elif pred in ["åº”ç”¨é¢†åŸŸ", "é€‚ç”¨äº"]:
                    relations.append(f"{subj}é€‚ç”¨äº{obj}")
                else:
                    relations.append(f"{subj}ä¸{obj}å­˜åœ¨{pred}å…³ç³»")

        if relations:
            context_parts.append("å…³ç³»ä¿¡æ¯ï¼š" + "ï¼›".join(relations))

    # 3. è¯¦ç»†æè¿°ä¿¡æ¯
    if knowledge_content:
        # æ¸…ç†å’Œç»“æ„åŒ–çŸ¥è¯†å†…å®¹
        clean_content = knowledge_content.replace("ã€", "").replace("ã€‘", "")
        clean_content = clean_content.replace("ç›¸å…³çŸ¥è¯†", "").strip()
        if len(clean_content) > 50:
            # æˆªå–å…³é”®éƒ¨åˆ†
            sentences = clean_content.split("ã€‚")[:3]  # æœ€å¤š3å¥
            key_content = "ã€‚".join([s.strip() for s in sentences if s.strip()])
            if key_content:
                context_parts.append(f"èƒŒæ™¯çŸ¥è¯†ï¼š{key_content}")

    if not context_parts:
        return ""

    # æ„å»ºç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡
    context = "\n".join(context_parts)

    # æ„å»ºæ™ºèƒ½prompt
    prompt = f"""åŸºäºä»¥ä¸‹çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œè¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

{context}

è¯·æ³¨æ„ï¼š
1. åŸºäºä¸Šè¿°çŸ¥è¯†ä¿¡æ¯è¿›è¡Œåˆ†æå’Œæ¨ç†
2. å¦‚æœé—®é¢˜æ¶‰åŠåœ°åŒºé€‚ç”¨æ€§ï¼Œè¯·ç»“åˆåœ°åŒºç‰¹ç‚¹å’ŒæŠ€æœ¯ç‰¹å¾åˆ†æ
3. æä¾›ä¸“ä¸šã€å‡†ç¡®ä¸”æœ‰é’ˆå¯¹æ€§çš„å›ç­”
4. å¦‚æœçŸ¥è¯†ä¿¡æ¯ä¸è¶³ï¼Œè¯·åŸºäºå¸¸ç†è¿›è¡Œåˆç†æ¨æµ‹å¹¶è¯´æ˜

ç”¨æˆ·é—®é¢˜ï¼š{user_input}"""

    return prompt

def convert_knowledge_to_context(ref, user_input):
    """å°†çŸ¥è¯†åº“ä¿¡æ¯è½¬æ¢ä¸ºè‡ªç„¶çš„å¯¹è¯èƒŒæ™¯ï¼ˆä¿ç•™æ—§æ¥å£å…¼å®¹æ€§ï¼‰"""
    if not ref:
        return ""

    # ç®€åŒ–å¤„ç†ï¼Œä¸ºæ—§ä»£ç æä¾›å…¼å®¹æ€§
    knowledge = ref.replace("ç›¸å…³çŸ¥è¯†ï¼š", "").strip()
    if len(knowledge) > 50:
        simplified = knowledge[:200] + "..." if len(knowledge) > 200 else knowledge
        return f"å‚è€ƒä¿¡æ¯ï¼š{simplified}"

    return ""

def generate_smart_response_from_knowledge(question, knowledge_ref):
    """åŸºäºçŸ¥è¯†åº“ä¿¡æ¯å’Œé—®é¢˜ç”Ÿæˆæ™ºèƒ½å›ç­”"""
    question_lower = question.lower()

    print(f"ğŸ¤– Generating smart response for: {question}")

    # åŸºäºé—®é¢˜ç±»å‹ç”Ÿæˆä¸“ä¸šå›ç­” - CCUSç›¸å…³
    if any(word in question_lower for word in ["ccus", "ç¢³æ•é›†", "ç¢³å‚¨å­˜", "ç¢³åˆ©ç”¨", "äºŒæ°§åŒ–ç¢³"]):
        if any(word in question_lower for word in ["å“ªäº›", "æœ‰ä»€ä¹ˆ", "åŒ…æ‹¬", "ç§ç±»", "ç±»å‹"]):
            if "æŠ€æœ¯" in question_lower:
                return """ç¢³æ•é›†æŠ€æœ¯ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ç±»ï¼š

**1. ç‡ƒçƒ§åæ•é›†æŠ€æœ¯**
â€¢ åŒ–å­¦å¸æ”¶æ³•ï¼šä½¿ç”¨MEAã€MDEAç­‰èƒºç±»æº¶å‰‚
â€¢ ç‰©ç†å¸é™„æ³•ï¼šä½¿ç”¨æ´»æ€§ç‚­ã€åˆ†å­ç­›ç­‰
â€¢ è†œåˆ†ç¦»æŠ€æœ¯ï¼šæ°”ä½“åˆ†ç¦»è†œæŠ€æœ¯

**2. ç‡ƒçƒ§å‰æ•é›†æŠ€æœ¯**
â€¢ æ•´ä½“ç…¤æ°”åŒ–è”åˆå¾ªç¯ï¼ˆIGCCï¼‰
â€¢ åˆ¶æ°¢å·¥è‰ºä¸­çš„CO2åˆ†ç¦»

**3. å¯Œæ°§ç‡ƒçƒ§æŠ€æœ¯**
â€¢ æ°§æ°”ç‡ƒçƒ§æ•é›†æŠ€æœ¯
â€¢ åŒ–å­¦é“¾ç‡ƒçƒ§æŠ€æœ¯

**4. æ–°å…´æŠ€æœ¯**
â€¢ ç›´æ¥ç©ºæ°”æ•é›†ï¼ˆDACï¼‰
â€¢ ç”Ÿç‰©è´¨èƒ½ç¢³æ•é›†ï¼ˆBECCSï¼‰
â€¢ ç”µåŒ–å­¦æ•é›†æŠ€æœ¯

ç›®å‰åœ¨æˆ‘å›½ï¼Œç‡ƒçƒ§åæ•é›†æŠ€æœ¯åº”ç”¨æœ€ä¸ºå¹¿æ³›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‡ƒç…¤ç”µå‚å’Œå·¥ä¸šçƒŸæ°”å¤„ç†æ–¹é¢ã€‚"""

        if "ä»€ä¹ˆæ˜¯" in question_lower or "å®šä¹‰" in question_lower:
            return """CCUSï¼ˆCarbon Capture, Utilization and Storageï¼‰æ˜¯ç¢³æ•é›†ã€åˆ©ç”¨ä¸å‚¨å­˜æŠ€æœ¯çš„ç®€ç§°ï¼Œæ˜¯åº”å¯¹æ°”å€™å˜åŒ–çš„é‡è¦æŠ€æœ¯æ‰‹æ®µã€‚

**ä¸‰ä¸ªæ ¸å¿ƒç¯èŠ‚ï¼š**

1. **ç¢³æ•é›†ï¼ˆCaptureï¼‰**ï¼šä»å·¥ä¸šæ’æ”¾æºæˆ–å¤§æ°”ä¸­æ•è·CO2
2. **ç¢³åˆ©ç”¨ï¼ˆUtilizationï¼‰**ï¼šå°†æ•é›†çš„CO2è½¬åŒ–ä¸ºæœ‰ä»·å€¼çš„äº§å“
3. **ç¢³å‚¨å­˜ï¼ˆStorageï¼‰**ï¼šå°†CO2å®‰å…¨å°å­˜åœ¨åœ°è´¨ç»“æ„ä¸­

**ä¸»è¦åº”ç”¨é¢†åŸŸï¼š**
â€¢ ç”µåŠ›è¡Œä¸šï¼šç‡ƒç…¤ç”µå‚ã€å¤©ç„¶æ°”ç”µå‚
â€¢ å·¥ä¸šé¢†åŸŸï¼šé’¢é“ã€æ°´æ³¥ã€çŸ³æ²¹åŒ–å·¥
â€¢ æ–°å…´åº”ç”¨ï¼šç›´æ¥ç©ºæ°”æ•é›†ã€ç”Ÿç‰©è´¨èƒ½

CCUSæŠ€æœ¯æ˜¯å®ç°ç¢³ä¸­å’Œç›®æ ‡çš„å…³é”®æŠ€æœ¯è·¯å¾„ï¼Œåœ¨æˆ‘å›½"åŒç¢³"æˆ˜ç•¥ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚"""

        if "åº”ç”¨" in question_lower or "æ¡ˆä¾‹" in question_lower:
            return """æˆ‘å›½CCUSæŠ€æœ¯åº”ç”¨æ¡ˆä¾‹ä¸°å¯Œï¼š

**ç”µåŠ›è¡Œä¸šï¼š**
â€¢ å›½å®¶èƒ½æºé›†å›¢æ³°å·ç”µå‚ï¼š50ä¸‡å¨/å¹´ï¼Œäºšæ´²æœ€å¤§ç‡ƒç…¤ç”µå‚CCUSé¡¹ç›®
â€¢ åèƒ½é™‡ä¸œèƒ½æºåŸºåœ°ï¼šè§„åˆ’150ä¸‡å¨/å¹´æ•é›†è§„æ¨¡

**çŸ³æ²¹è¡Œä¸šï¼š**
â€¢ é½é²çŸ³åŒ–-èƒœåˆ©æ²¹ç”°ï¼š100ä¸‡å¨çº§å…¨äº§ä¸šé“¾ç¤ºèŒƒå·¥ç¨‹
â€¢ ä¸­çŸ³æ²¹å‰æ—æ²¹ç”°ï¼š20ä¸‡å¨/å¹´CCUSé¡¹ç›®

**é’¢é“è¡Œä¸šï¼š**
â€¢ å®é’¢æ¹›æ±Ÿå·¥å‚ï¼šé’¢é“è¡Œä¸šç¢³æ•é›†ç¤ºèŒƒ
â€¢ æ²³é’¢é›†å›¢ï¼šä½ç¢³å†¶é‡‘æŠ€æœ¯ç ”å‘

**åŒ–å·¥è¡Œä¸šï¼š**
â€¢ ç…¤åŒ–å·¥ä¼ä¸šï¼šé«˜æµ“åº¦CO2æ•é›†åˆ©ç”¨
â€¢ çŸ³æ²¹åŒ–å·¥ï¼šå‚¬åŒ–è£‚åŒ–çƒŸæ°”CO2å›æ”¶

è¿™äº›é¡¹ç›®å±•ç¤ºäº†CCUSæŠ€æœ¯åœ¨ä¸åŒè¡Œä¸šçš„åº”ç”¨æ½œåŠ›å’ŒæŠ€æœ¯å¯è¡Œæ€§ã€‚"""

    # å¦‚æœæœ‰ä¸‰å…ƒç»„ä¿¡æ¯ï¼Œå…ˆå°è¯•è§£æ
    if "çš„å…³ç³»æ˜¯" in knowledge_ref:
        # è§£æä¸‰å…ƒç»„ä¿¡æ¯ï¼Œæå–æœ‰ç”¨å†…å®¹
        relationships = []
        parts = knowledge_ref.split("ï¼›")
        for part in parts[:5]:  # åªå–å‰5ä¸ªå…³ç³»
            if "çš„å…³ç³»æ˜¯" in part:
                try:
                    subj_obj, relation = part.split("çš„å…³ç³»æ˜¯")
                    if "ä¸" in subj_obj:
                        subj, obj = subj_obj.split("ä¸", 1)
                        if any(keyword in subj.lower() or keyword in obj.lower() for keyword in ["ç­ç«", "æ³¡æ²«", "co2", "æ¶ˆé˜²"]):
                            relationships.append(f"{subj}{relation}{obj}")
                except:
                    continue

    if "ccus" in question_lower or "ç¢³æ•é›†" in question_lower or "äºŒæ°§åŒ–ç¢³å‚¨å­˜" in question_lower:
        if "ä»€ä¹ˆæ˜¯" in question_lower or "å®šä¹‰" in question_lower:
            return "CCUSï¼ˆCarbon Capture, Utilization and Storageï¼‰æ˜¯ç¢³æ•é›†ã€åˆ©ç”¨ä¸å‚¨å­˜æŠ€æœ¯çš„ç®€ç§°ã€‚æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼ŒCCUSæŠ€æœ¯åŒ…æ‹¬äºŒæ°§åŒ–ç¢³æ•é›†ã€åˆ©ç”¨å’Œå‚¨å­˜ä¸‰ä¸ªä¸»è¦ç¯èŠ‚ã€‚è¯¥æŠ€æœ¯èƒ½å¤Ÿä»å·¥ä¸šæ’æ”¾æºä¸­æ•é›†äºŒæ°§åŒ–ç¢³ï¼Œç»è¿‡å¤„ç†åè¿›è¡Œåˆ©ç”¨æˆ–é•¿æœŸå‚¨å­˜ï¼Œæ˜¯åº”å¯¹æ°”å€™å˜åŒ–çš„é‡è¦æŠ€æœ¯æ‰‹æ®µã€‚"
        elif "åº”ç”¨" in question_lower or "é€‚åˆ" in question_lower:
            if "åŒ—äº¬" in question_lower:
                return "æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼ŒåŒ—äº¬åœ°åŒºé€‚åˆçš„CCUSæŠ€æœ¯ä¸»è¦åŒ…æ‹¬ï¼š1ï¼‰åŸºäºç…¤ç”µå’Œé’¢é“è¡Œä¸šçš„åç‡ƒçƒ§æ•é›†æŠ€æœ¯ï¼›2ï¼‰äºŒæ°§åŒ–ç¢³åˆ©ç”¨æŠ€æœ¯ï¼Œå¦‚åˆ¶å¤‡å»ºæå’ŒåŒ–å­¦å“ï¼›3ï¼‰ä¸æ²³åŒ—ç­‰å‘¨è¾¹åœ°åŒºåˆä½œçš„åœ°è´¨å‚¨å­˜æŠ€æœ¯ã€‚è€ƒè™‘åˆ°åŒ—äº¬çš„äº§ä¸šç»“æ„å’Œç¯ä¿è¦æ±‚ï¼Œé‡ç‚¹å‘å±•é«˜æ•ˆä½è€—çš„æ•é›†æŠ€æœ¯å’Œé«˜é™„åŠ å€¼çš„åˆ©ç”¨æŠ€æœ¯ã€‚"
            else:
                return "CCUSæŠ€æœ¯åœ¨å¤šä¸ªè¡Œä¸šæœ‰å¹¿æ³›åº”ç”¨ï¼šç”µåŠ›è¡Œä¸šçš„ç‡ƒç…¤ç”µå‚ã€é’¢é“å†¶é‡‘ã€çŸ³æ²¹åŒ–å·¥ã€æ°´æ³¥ç”Ÿäº§ç­‰ã€‚æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œæˆ‘å›½åœ¨é„‚å°”å¤šæ–¯ç­‰åœ°å»ºè®¾äº†ç¤ºèŒƒå·¥ç¨‹ï¼ŒæŠ€æœ¯åº”ç”¨å‰æ™¯å¹¿é˜”ã€‚é€‰æ‹©é€‚åˆçš„CCUSæŠ€æœ¯éœ€è¦è€ƒè™‘æ’æ”¾æºç‰¹å¾ã€ç»æµæ€§å’ŒæŠ€æœ¯æˆç†Ÿåº¦ã€‚"
        elif "å‘å±•" in question_lower or "å‰æ™¯" in question_lower:
            return "æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼ŒCCUSæŠ€æœ¯å‘å±•å‰æ™¯å¹¿é˜”ã€‚æˆ‘å›½å·²åœ¨å¤šåœ°å¼€å±•ç¤ºèŒƒå·¥ç¨‹ï¼Œå¦‚é„‚å°”å¤šæ–¯æ·±éƒ¨å’¸æ°´å±‚äºŒæ°§åŒ–ç¢³å‚¨å­˜é¡¹ç›®ã€‚æœªæ¥å‘å±•è¶‹åŠ¿åŒ…æ‹¬ï¼šæŠ€æœ¯æˆæœ¬æŒç»­ä¸‹é™ã€åº”ç”¨è§„æ¨¡ä¸æ–­æ‰©å¤§ã€æ”¿ç­–æ”¯æŒåŠ›åº¦åŠ å¼ºã€‚é¢„è®¡åˆ°2030å¹´ï¼ŒCCUSå°†æˆä¸ºæˆ‘å›½å®ç°ç¢³ä¸­å’Œç›®æ ‡çš„é‡è¦æŠ€æœ¯è·¯å¾„ã€‚"
        elif "æŠ€æœ¯" in question_lower:
            return "CCUSæŠ€æœ¯ä½“ç³»åŒ…æ‹¬å¤šä¸ªå…³é”®ç¯èŠ‚ï¼šäºŒæ°§åŒ–ç¢³æ•é›†æŠ€æœ¯ï¼ˆå¦‚åç‡ƒçƒ§æ•é›†ã€é¢„ç‡ƒçƒ§æ•é›†ï¼‰ã€è¿è¾“æŠ€æœ¯ã€åˆ©ç”¨æŠ€æœ¯ï¼ˆå¦‚åˆ¶å¤‡åŒ–å­¦å“ã€æé«˜çŸ³æ²¹é‡‡æ”¶ç‡ï¼‰å’Œå‚¨å­˜æŠ€æœ¯ï¼ˆå¦‚åœ°è´¨å‚¨å­˜ã€æµ·æ´‹å‚¨å­˜ï¼‰ã€‚æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œä¸åŒæŠ€æœ¯è·¯çº¿é€‚ç”¨äºä¸åŒçš„å·¥ä¸šåœºæ™¯å’Œè§„æ¨¡è¦æ±‚ã€‚"
        else:
            return "CCUSæ˜¯åº”å¯¹æ°”å€™å˜åŒ–çš„å…³é”®æŠ€æœ¯ä¹‹ä¸€ã€‚æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œè¯¥æŠ€æœ¯é€šè¿‡æ•é›†å·¥ä¸šæ’æ”¾çš„äºŒæ°§åŒ–ç¢³ï¼Œç»è¿‡å¤„ç†åè¿›è¡Œèµ„æºåŒ–åˆ©ç”¨æˆ–å®‰å…¨å‚¨å­˜ï¼Œèƒ½å¤Ÿæ˜¾è‘—å‡å°‘æ¸©å®¤æ°”ä½“æ’æ”¾ã€‚æˆ‘å›½åœ¨CCUSæŠ€æœ¯ç ”å‘å’Œç¤ºèŒƒåº”ç”¨æ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ã€‚"


    else:
        # å¯¹äºå…¶ä»–é—®é¢˜ï¼Œå°è¯•ä»çŸ¥è¯†åº“ä¸­æå–æœ‰ç”¨ä¿¡æ¯
        if len(knowledge_ref) > 50:
            # ç®€åŒ–çŸ¥è¯†å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯
            key_info = knowledge_ref.replace("ç›¸å…³çŸ¥è¯†ï¼š", "").strip()
            if len(key_info) > 200:
                key_info = key_info[:200] + "..."
            return f"æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼š{key_info}ã€‚è¿™äº›ä¸“ä¸šçŸ¥è¯†æ¶µç›–äº†ç›¸å…³çš„æŠ€æœ¯å‚æ•°ã€åº”ç”¨åœºæ™¯å’Œæ“ä½œè¦ç‚¹ã€‚"
        else:
            return f"å…³äºã€Œ{question}ã€çš„é—®é¢˜ï¼Œæˆ‘åœ¨çŸ¥è¯†å›¾è°±ä¸­æ‰¾åˆ°äº†ç›¸å…³ä¿¡æ¯ï¼Œä½†å†…å®¹è¾ƒä¸ºä¸“ä¸šã€‚å»ºè®®æ‚¨æä¾›æ›´å…·ä½“çš„é—®é¢˜æè¿°ï¼Œä»¥ä¾¿æˆ‘ä¸ºæ‚¨æä¾›æ›´è¯¦ç»†å’Œå‡†ç¡®çš„å›ç­”ã€‚"

def generate_simple_answer(question):
    """ä¸ºç‰¹å®šé—®é¢˜ç”Ÿæˆç®€å•çš„å›ç­”"""
    question = question.lower()

    # CCUSç›¸å…³é—®é¢˜
    if "ccus" in question or "ç¢³æ•é›†" in question or "äºŒæ°§åŒ–ç¢³å‚¨å­˜" in question:
        if "ä»€ä¹ˆæ˜¯" in question or "å®šä¹‰" in question:
            return "CCUSæ˜¯ç¢³æ•é›†ã€åˆ©ç”¨ä¸å‚¨å­˜æŠ€æœ¯ï¼Œé€šè¿‡æ•é›†å·¥ä¸šæ’æ”¾çš„äºŒæ°§åŒ–ç¢³ï¼Œè¿›è¡Œèµ„æºåŒ–åˆ©ç”¨æˆ–å®‰å…¨å‚¨å­˜ï¼Œæ˜¯åº”å¯¹æ°”å€™å˜åŒ–çš„é‡è¦æŠ€æœ¯ã€‚"
        elif "æŠ€æœ¯" in question:
            return "CCUSæŠ€æœ¯åŒ…æ‹¬æ•é›†ã€è¿è¾“ã€åˆ©ç”¨å’Œå‚¨å­˜å››ä¸ªç¯èŠ‚ï¼Œé€‚ç”¨äºç”µåŠ›ã€é’¢é“ã€åŒ–å·¥ç­‰å¤šä¸ªè¡Œä¸šçš„å‡æ’éœ€æ±‚ã€‚"
        elif "åº”ç”¨" in question or "é€‚ç”¨" in question:
            return "CCUSæŠ€æœ¯å¹¿æ³›åº”ç”¨äºç‡ƒç…¤ç”µå‚ã€é’¢é“å†¶é‡‘ã€çŸ³æ²¹åŒ–å·¥ã€æ°´æ³¥ç”Ÿäº§ç­‰é«˜æ’æ”¾è¡Œä¸šï¼Œé€šè¿‡æŠ€æœ¯æ”¹é€ å®ç°ç¢³å‡æ’ç›®æ ‡ã€‚"
        elif "å‘å±•" in question or "å‰æ™¯" in question:
            return "CCUSæ˜¯å‡å°‘æ¸©å®¤æ°”ä½“æ’æ”¾çš„å…³é”®æŠ€æœ¯ï¼Œæˆ‘å›½åœ¨è¯¥é¢†åŸŸå·²å¼€å±•å¤šä¸ªç¤ºèŒƒé¡¹ç›®ï¼ŒæŠ€æœ¯å‘å±•å‰æ™¯å¹¿é˜”ï¼Œæ˜¯å®ç°ç¢³ä¸­å’Œç›®æ ‡çš„é‡è¦é€”å¾„ã€‚"
        else:
            return "CCUSæŠ€æœ¯é€šè¿‡æ•é›†ã€åˆ©ç”¨å’Œå‚¨å­˜å·¥ä¸šæ’æ”¾çš„äºŒæ°§åŒ–ç¢³ï¼Œä¸ºå®ç°ç¢³å‡æ’å’Œæ°”å€™ç›®æ ‡æä¾›é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚"

    # ç¢³å‡æ’ç›¸å…³é—®é¢˜
    if "ç¢³å‡æ’" in question or "æ¸©å®¤æ°”ä½“" in question or "æ°”å€™å˜åŒ–" in question:
        return "ç¢³å‡æ’æ˜¯åº”å¯¹æ°”å€™å˜åŒ–çš„å…³é”®æªæ–½ï¼Œä¸»è¦é€šè¿‡æé«˜èƒ½æºæ•ˆç‡ã€å‘å±•å¯å†ç”Ÿèƒ½æºã€ç¢³æ•é›†åˆ©ç”¨ä¸å‚¨å­˜ç­‰æŠ€æœ¯æ‰‹æ®µæ¥å‡å°‘æ¸©å®¤æ°”ä½“æ’æ”¾ã€‚"

    # æ¸…æ´èƒ½æºç›¸å…³é—®é¢˜
    if "æ¸…æ´èƒ½æº" in question or "å¯å†ç”Ÿèƒ½æº" in question:
        return "æ¸…æ´èƒ½æºåŒ…æ‹¬å¤ªé˜³èƒ½ã€é£èƒ½ã€æ°´èƒ½ã€æ ¸èƒ½ç­‰ï¼Œæ˜¯å‡å°‘ç¢³æ’æ”¾ã€å®ç°å¯æŒç»­å‘å±•çš„é‡è¦é€”å¾„ï¼Œä¸CCUSæŠ€æœ¯å½¢æˆäº’è¡¥çš„å‡æ’ä½“ç³»ã€‚"

    return f"å…³äºã€Œ{question}ã€çš„é—®é¢˜ï¼Œæˆ‘ä¼šåŸºäºCCUSé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ä¸ºæ‚¨æä¾›å‡†ç¡®ç­”æ¡ˆã€‚"

def predict(user_input, history=None):
    global model, tokenizer, init_history
    if not history:
        history = init_history
    return model.chat(tokenizer, user_input, history)


def stream_predict(user_input, history=None):
    global model, tokenizer, init_history
    if not history:
        history = init_history

    ref = ""

    # è·å–å®ä½“
    graph = {}
    entities = []
    entities = ner.get_entities(user_input, etypes=["ç‰©ä½“ç±»", "äººç‰©ç±»", "åœ°ç‚¹ç±»", "ç»„ç»‡æœºæ„ç±»", "äº‹ä»¶ç±»", "ä¸–ç•Œåœ°åŒºç±»", "æœ¯è¯­ç±»"])
    print("entities: ", entities)

    # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è·å–èšç„¦æœç´¢
    context_graph = context_manager.get_focused_search(entities)
    if context_graph:
        print(f"ğŸ¯ Using focused search with {len(context_graph.get('nodes', []))} nodes")

    # è·å–å®ä½“çš„ä¸‰å…ƒç»„å’ŒçŸ¥è¯†å†…å®¹
    triples = []
    knowledge_content = ""
    for entity in entities:
        print(f"ğŸ” Searching graph for entity: {entity}")
        entity_graph = search_node_item(entity, graph if graph else None)

        print(f"ğŸ“Š Graph data for {entity}: {entity_graph is not None}")
        if entity_graph and entity_graph.get('nodes'):
            print(f"   - Nodes: {len(entity_graph.get('nodes', []))}")
            print(f"   - Links: {len(entity_graph.get('links', []))}")
            print(f"   - Sents: {len(entity_graph.get('sents', []))}")
            # åˆå¹¶å›¾è°±æ•°æ®
            if not graph:
                graph = entity_graph
            else:
                # åˆå¹¶èŠ‚ç‚¹
                existing_nodes = {node['name']: node for node in graph['nodes']}
                for node in entity_graph['nodes']:
                    if node['name'] not in existing_nodes:
                        node['id'] = len(graph['nodes'])
                        graph['nodes'].append(node)

                # åˆå¹¶é“¾æ¥
                for link in entity_graph['links']:
                    if link not in graph['links']:
                        graph['links'].append(link)

                # åˆå¹¶å¥å­
                for sent in entity_graph['sents']:
                    if sent not in graph['sents']:
                        graph['sents'].append(sent)

            # æå–ä¸‰å…ƒç»„
            entity_triples = convert_graph_to_triples(entity_graph, entity)
            triples.extend(entity_triples)

            # æå–çŸ¥è¯†å†…å®¹
            from app.utils.graph_utils import extract_knowledge_content
            entity_knowledge = extract_knowledge_content(entity_graph, entity)
            if entity_knowledge:
                knowledge_content += f"\n\nã€{entity}ç›¸å…³çŸ¥è¯†ã€‘\n{entity_knowledge}"
        else:
            print(f"   âŒ No graph data found for entity: {entity}")

    # æ™ºèƒ½æ•´åˆçŸ¥è¯†ä¿¡æ¯
    if triples or knowledge_content:
        # æ„å»ºç»“æ„åŒ–çŸ¥è¯†å†…å®¹
        structured_knowledge = []

        # å¤„ç†ä¸‰å…ƒç»„ä¿¡æ¯ï¼ŒæŒ‰å…³ç³»ç±»å‹åˆ†ç»„
        if triples:
            relation_groups = {}
            for t in triples[:10]:  # é™åˆ¶æ•°é‡ï¼Œé¿å…è¿‡é•¿
                relation = t[1]
                if relation not in relation_groups:
                    relation_groups[relation] = []
                relation_groups[relation].append((t[0], t[2]))

            # è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°
            for relation, pairs in relation_groups.items():
                if len(pairs) <= 3:  # å¯¹äºå°‘é‡å…³ç³»ï¼Œè¯¦ç»†æè¿°
                    for subject, obj in pairs:
                        structured_knowledge.append(f"{subject}ä¸{obj}çš„å…³ç³»æ˜¯{relation}")
                else:  # å¯¹äºå¤§é‡å…³ç³»ï¼Œå½’çº³æè¿°
                    subjects = [pair[0] for pair in pairs[:3]]
                    structured_knowledge.append(f"å…³äº{relation}çš„ç›¸å…³å†…å®¹åŒ…æ‹¬ï¼š{', '.join(subjects)}ç­‰")

        # æ•´åˆæ–‡æœ¬çŸ¥è¯†å†…å®¹
        if knowledge_content:
            # æ¸…ç†å’Œä¼˜åŒ–çŸ¥è¯†å†…å®¹æ ¼å¼
            clean_content = knowledge_content.replace("ã€", "").replace("ã€‘", "").replace("ç›¸å…³çŸ¥è¯†", "").strip()
            if clean_content and not clean_content.startswith("ã€ç›¸å…³å…³ç³»ã€‘"):
                structured_knowledge.append(clean_content)

        # æ„å»ºæœ€ç»ˆå‚è€ƒå†…å®¹
        if structured_knowledge:
            ref = "ç›¸å…³çŸ¥è¯†ï¼š" + "ï¼›".join(structured_knowledge[:5])  # é™åˆ¶çŸ¥è¯†ç‚¹æ•°é‡

        print(f"ğŸ”§ Processed knowledge: {len(structured_knowledge)} items -> {len(ref)} chars")


    image = image_searcher.search(user_input)

    for ent in entities + [user_input]:
        wiki = wiki_searcher.search(ent)
        if wiki is not None:
            break

    # å°†Wikipediaæœç´¢åˆ°çš„ç¹ä½“è½¬ä¸ºç®€ä½“
    if wiki:
        ref += cc.convert(wiki.summary)
        wiki = {
            "title": cc.convert(wiki.title),
            "summary": cc.convert(wiki.summary),
        }
        print(wiki)
    else:
        wiki = {
            "title": "æ— ç›¸å…³ä¿¡æ¯",
            "summary": "æš‚æ— ç›¸å…³æè¿°",
        }

    print(f"ğŸ“¥ USER INPUT: {user_input}")
    print(f"ğŸ“š KNOWLEDGE REF: {ref[:200] if ref else 'None'}...")
    print(f"ğŸ” ENTITIES: {entities}")

    # æ„å»ºåŸºäºçŸ¥è¯†å›¾è°±çš„ä¸°å¯Œä¸Šä¸‹æ–‡
    has_knowledge = bool((triples or knowledge_content) and entities)

    print(f"ğŸ“š Knowledge available: {has_knowledge}")
    print(f"ğŸ”— Triples: {len(triples)}, Knowledge: {len(knowledge_content) if knowledge_content else 0}")
    print(f"ğŸ¤– Using ChatGLM model for intelligent response generation")

    if model is not None:
        # æ„å»ºåŸºäºçŸ¥è¯†å›¾è°±çš„ä¸°å¯Œä¸Šä¸‹æ–‡
        if has_knowledge:
            # ä½¿ç”¨æ–°çš„ä¸°å¯Œä¸Šä¸‹æ–‡æ„å»ºæ–¹æ³•
            rich_context = build_rich_knowledge_context(triples, knowledge_content, entities, user_input)
            if rich_context:
                chat_input = rich_context
                print(f"ğŸ“– Using rich knowledge context")
            else:
                # å¦‚æœæ²¡æœ‰ä¸°å¯Œä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨åŸºæœ¬çš„çŸ¥è¯†ä¿¡æ¯
                if ref:
                    basic_context = f"å‚è€ƒä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š{ref[:300]}...\n\né—®é¢˜ï¼š{user_input}"
                    chat_input = basic_context
                else:
                    chat_input = user_input
        else:
            chat_input = user_input

        # æ„å»ºå¹²å‡€çš„å†å²è®°å½•
        clean_history = []
        for user_msg, response in history:
            # æ¸…ç†ç”¨æˆ·è¾“å…¥ï¼Œç§»é™¤å‚è€ƒèµ„æ–™éƒ¨åˆ†ï¼Œä¿ç•™åŸå§‹é—®é¢˜
            clean_user_input = user_msg

            # å¤„ç†å„ç§æ ¼å¼çš„promptï¼Œæå–åŸå§‹é—®é¢˜
            if "è¯·åŸºäºä»¥ä¸Šèµ„æ–™ï¼Œç”¨ç®€æ´è‡ªç„¶çš„è¯­è¨€å›ç­”ï¼š" in user_msg:
                clean_user_input = user_msg.split("è¯·åŸºäºä»¥ä¸Šèµ„æ–™ï¼Œç”¨ç®€æ´è‡ªç„¶çš„è¯­è¨€å›ç­”ï¼š")[1].strip()
            elif "===å‚è€ƒèµ„æ–™===" in user_msg:
                if "æ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š" in user_msg:
                    clean_user_input = user_msg.split("æ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š")[1].strip()
                else:
                    clean_user_input = user_msg.split("===å‚è€ƒèµ„æ–™===")[0].strip()
            elif "æ ¹æ®æˆ‘çš„çŸ¥è¯†ï¼Œ" in user_msg and len(user_msg) > 100:
                # æå–æ–°æ ¼å¼ä¸­çš„åŸå§‹é—®é¢˜ï¼ˆå»é™¤çŸ¥è¯†èƒŒæ™¯ï¼‰
                lines = user_msg.split("\n")
                if len(lines) >= 2:
                    clean_user_input = lines[-1].strip()  # å–æœ€åä¸€è¡Œä½œä¸ºé—®é¢˜

            clean_history.append((clean_user_input, response))

        print(f"ğŸ”¤ CHAT INPUT TO MODEL: {chat_input[:100]}...")
        print(f"ğŸ“œ CLEAN HISTORY: {len(clean_history)} items")

        # ä½¿ç”¨æ–°çš„chat_glmå®ä¾‹æˆ–åŸæœ‰æ–¹å¼
        if chat_glm and chat_glm.loaded:
            print(f"âœ… ChatGLM model is loaded and ready")
            for response, raw_history in chat_glm.stream_chat(chat_input, clean_history):
                print(f"ğŸ¤– RAW MODEL RESPONSE: {response}")
                # åå¤„ç†å“åº”ï¼Œæ¸…ç†ä¸è‡ªç„¶çš„å›ç­”
                cleaned_response = clean_model_response(response, user_input)
                print(f"ğŸ§¹ CLEANED RESPONSE: {cleaned_response}")

                # æ¸…ç†è¿”å›çš„å†å²è®°å½•ï¼Œç¡®ä¿ç”¨æˆ·çœ‹åˆ°çš„æ˜¯åŸå§‹é—®é¢˜
                clean_return_history = []
                for h_q, h_r in raw_history:
                    clean_q = h_q

                    # å¤„ç†å„ç§æ ¼å¼çš„promptï¼Œæå–åŸå§‹é—®é¢˜
                    if "è¯·åŸºäºä»¥ä¸Šèµ„æ–™ï¼Œç”¨ç®€æ´è‡ªç„¶çš„è¯­è¨€å›ç­”ï¼š" in h_q:
                        clean_q = h_q.split("è¯·åŸºäºä»¥ä¸Šèµ„æ–™ï¼Œç”¨ç®€æ´è‡ªç„¶çš„è¯­è¨€å›ç­”ï¼š")[1].strip()
                    elif "===å‚è€ƒèµ„æ–™===" in h_q:
                        if "æ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š" in h_q:
                            clean_q = h_q.split("æ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š")[1].strip()
                        else:
                            clean_q = h_q.split("===å‚è€ƒèµ„æ–™===")[0].strip()
                    elif "æ ¹æ®æˆ‘çš„çŸ¥è¯†ï¼Œ" in h_q and len(h_q) > 100:
                        # æå–æ–°æ ¼å¼ä¸­çš„åŸå§‹é—®é¢˜ï¼ˆå»é™¤çŸ¥è¯†èƒŒæ™¯ï¼‰
                        lines = h_q.split("\n")
                        if len(lines) >= 2:
                            clean_q = lines[-1].strip()  # å–æœ€åä¸€è¡Œä½œä¸ºé—®é¢˜

                    # ä¹Ÿè¦æ¸…ç†å†å²è®°å½•ä¸­çš„å“åº”
                    cleaned_h_r = clean_model_response(h_r, clean_q) if h_r else h_r
                    clean_return_history.append((clean_q, cleaned_h_r))

                # ç›´æ¥ä½¿ç”¨ChatGLMçš„å›ç­”ï¼Œå› ä¸ºå·²ç»åŒ…å«äº†çŸ¥è¯†åº“ä¿¡æ¯ä½œä¸ºä¸Šä¸‹æ–‡
                final_response = cleaned_response
                print(f"âœ… Using ChatGLM response with knowledge context: {cleaned_response[:50]}...")

                updates = {
                    "query": user_input,  # ä½¿ç”¨åŸå§‹ç”¨æˆ·è¾“å…¥ï¼Œè€Œä¸æ˜¯chat_input
                    "response": final_response  # ä½¿ç”¨æœ€ç»ˆå“åº”
                }

                # æ›´æ–°ä¸Šä¸‹æ–‡
                context_manager.update_context(user_input, entities, graph)

                # è·å–å®ä½“è¯¦ç»†ä¿¡æ¯
                entity_details = []
                for entity in entities[:3]:  # é™åˆ¶å®ä½“æ•°é‡
                    details = get_entity_details(entity, graph)
                    if details:
                        entity_details.append(details)

                # è·å–å»ºè®®é—®é¢˜
                suggestions = context_manager.suggest_related_questions(entities)

                # è·å–å¯¹è¯æ‘˜è¦
                conversation_summary = context_manager.get_conversation_summary()

                result = {
                    "history": clean_return_history,
                    "updates": updates,
                    "image": image,
                    "graph": graph,
                    "wiki": wiki,
                    "entity_details": entity_details,
                    "suggestions": suggestions,
                    "conversation_summary": conversation_summary
                }
                yield json.dumps(result, ensure_ascii=False).encode('utf8') + b'\n'
        elif model is not None:
            print(f"âœ… Using fallback model for response generation")
            for response, history in model.stream_chat(tokenizer, chat_input, clean_history):
                print(f"ğŸ¤– FALLBACK MODEL RESPONSE: {response}")
                updates = {}
                for query, response in history:
                    updates["query"] = query
                    updates["response"] = response

                # æ›´æ–°ä¸Šä¸‹æ–‡
                context_manager.update_context(user_input, entities, graph)

                # è·å–å®ä½“è¯¦ç»†ä¿¡æ¯å’Œå»ºè®®
                entity_details = []
                for entity in entities[:3]:
                    details = get_entity_details(entity, graph)
                    if details:
                        entity_details.append(details)

                suggestions = context_manager.suggest_related_questions(entities)
                conversation_summary = context_manager.get_conversation_summary()

                result = {
                    "history": history,
                    "updates": updates,
                    "image": image,
                    "graph": graph,
                    "wiki": wiki,
                    "entity_details": entity_details,
                    "suggestions": suggestions,
                    "conversation_summary": conversation_summary
                }
                yield json.dumps(result, ensure_ascii=False).encode('utf8') + b'\n'
        else:
            # ç®€å•å›å¤æ¨¡å¼ - æ— ChatGLMæ¨¡å‹æ—¶çš„å¤„ç†
            print(f"âŒ No model available, using knowledge-based response mode")

            if has_knowledge:
                # åŸºäºçŸ¥è¯†å›¾è°±ä¿¡æ¯ç”Ÿæˆæ™ºèƒ½å›ç­”
                print(f"ğŸ“‹ Generating smart answer from knowledge graph (no model)")
                response_text = generate_smart_response_from_knowledge(user_input, ref if ref else "")
            else:
                print(f"â³ No knowledge available, using fallback message")
                response_text = f"å…³äºã€Œ{user_input}ã€çš„é—®é¢˜ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚å»ºè®®æ‚¨è¯¢é—®CCUSã€ç¢³æ•é›†ã€äºŒæ°§åŒ–ç¢³å‚¨å­˜ç­‰ç›¸å…³æŠ€æœ¯é—®é¢˜ã€‚"

            updates = {
                "query": user_input,
                "response": response_text
            }

            # æ›´æ–°ä¸Šä¸‹æ–‡
            context_manager.update_context(user_input, entities, graph)

            # è·å–å®ä½“è¯¦ç»†ä¿¡æ¯å’Œå»ºè®®
            entity_details = []
            for entity in entities[:3]:
                print(f"ğŸ” Getting details for entity: {entity}")
                details = get_entity_details(entity, graph)
                if details:
                    entity_details.append(details)
                    print(f"ğŸ“‹ Entity details for {entity}: {details['total_connections']} connections")
                else:
                    print(f"âŒ No details found for entity: {entity}")

            suggestions = context_manager.suggest_related_questions(entities)
            conversation_summary = context_manager.get_conversation_summary()

            print(f"ğŸ”¢ Generated {len(entity_details)} entity details, {len(suggestions)} suggestions")

            result = {
                "history": history,
                "updates": updates,
                "image": image,
                "graph": graph,
                "wiki": wiki,
                "entity_details": entity_details,
                "suggestions": suggestions,
                "conversation_summary": conversation_summary
            }
            yield json.dumps(result, ensure_ascii=False).encode('utf8') + b'\n'

    else:
        # å³ä½¿æ²¡æœ‰æ¨¡å‹ä¹Ÿå°è¯•ä½¿ç”¨çŸ¥è¯†åº“ç”Ÿæˆæ™ºèƒ½ç­”æ¡ˆ
        print(f"âš ï¸ No ChatGLM model available, using knowledge-based fallback")

        if has_knowledge:
            # åŸºäºçŸ¥è¯†å›¾è°±ä¿¡æ¯ç”Ÿæˆæ™ºèƒ½å›ç­”
            print(f"ğŸ“‹ Generating smart answer from knowledge graph (no model)")
            response_text = generate_smart_response_from_knowledge(user_input, ref if ref else "")
        else:
            print(f"â³ No model and no knowledge available")
            response_text = f"å…³äºã€Œ{user_input}ã€çš„é—®é¢˜ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚å»ºè®®æ‚¨è¯¢é—®CCUSã€ç¢³æ•é›†ã€äºŒæ°§åŒ–ç¢³å‚¨å­˜ç­‰ç›¸å…³æŠ€æœ¯é—®é¢˜ã€‚"

        updates = {
            "query": user_input,
            "response": response_text
        }

        # æ›´æ–°ä¸Šä¸‹æ–‡
        context_manager.update_context(user_input, entities, graph)

        # è·å–å®ä½“è¯¦ç»†ä¿¡æ¯å’Œå»ºè®®
        entity_details = []
        for entity in entities[:3]:
            print(f"ğŸ” Getting details for entity: {entity}")
            details = get_entity_details(entity, graph)
            if details:
                entity_details.append(details)
                print(f"ğŸ“‹ Entity details for {entity}: {details['total_connections']} connections")
            else:
                print(f"âŒ No details found for entity: {entity}")

        suggestions = context_manager.suggest_related_questions(entities)
        conversation_summary = context_manager.get_conversation_summary()

        print(f"ğŸ”¢ Generated {len(entity_details)} entity details, {len(suggestions)} suggestions")

        result = {
            "history": history,
            "updates": updates,
            "image": image,
            "graph": graph,
            "wiki": wiki,
            "entity_details": entity_details,
            "suggestions": suggestions,
            "conversation_summary": conversation_summary
        }
        yield json.dumps(result, ensure_ascii=False).encode('utf8') + b'\n'

# å…¨å±€ChatGLMå®ä¾‹
chat_glm = None

# åŠ è½½æ¨¡å‹
def start_model():
    global model, tokenizer, init_history, chat_glm

    print("ğŸš€ Starting optimized ChatGLM model loading...")

    # å…ˆæ¸…ç†GPUå†…å­˜
    try:
        import torch
        torch.cuda.empty_cache()
        if torch.cuda.is_available():
            print(f"ğŸ”§ GPU memory before loading: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
    except Exception as e:
        print(f"âš ï¸ Could not check GPU memory: {e}")

    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    model_path = "/fast/zwj/ChatGLM-6B/weights"

    if os.path.exists(model_path) and os.listdir(model_path):
        # ä½¿ç”¨æ–°çš„ç®€å•åŠ è½½å™¨ï¼Œä½†æ·»åŠ å†…å­˜ä¼˜åŒ–
        from app.utils.simple_chat import SimpleChatGLM

        print(f"ğŸ“ Model path exists: {model_path}")
        print(f"ğŸ“¦ Model files: {os.listdir(model_path)[:5]}...")  # æ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶

        # åˆ›å»ºChatGLMå®ä¾‹ï¼Œå¯ç”¨å†…å­˜ä¼˜åŒ–
        chat_glm = SimpleChatGLM(model_path, memory_optimize=True)

        print("ğŸ”„ Attempting to load model with memory optimization...")
        if chat_glm.load_model():
            # å…¼å®¹åŸæœ‰æ¥å£
            model = chat_glm.model
            tokenizer = chat_glm.tokenizer
            init_history = []
            print("âœ… ChatGLM-6B loaded successfully and ready for chat!")

            # æ˜¾ç¤ºåŠ è½½åçš„å†…å­˜ä½¿ç”¨
            try:
                if torch.cuda.is_available():
                    print(f"ğŸ“Š GPU memory after loading: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
            except:
                pass
        else:
            print("âŒ Failed to load ChatGLM-6B with optimization")
            model = None
            tokenizer = None
            init_history = []
            chat_glm = None
    else:
        print("Warning: ChatGLM-6B model weights not found at /fast/zwj/ChatGLM-6B/weights")
        print("Using simple response mode. To enable full chat functionality:")
        print("1. Run: python3 download_model.py")
        print("2. Or download model manually to: /fast/zwj/ChatGLM-6B/weights")

        model = None
        tokenizer = None
        init_history = []
        chat_glm = None