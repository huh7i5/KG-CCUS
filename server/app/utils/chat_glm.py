import os
import sys
sys.path.append('server/app')
import json
from opencc import OpenCC
from transformers import AutoTokenizer, AutoModel
from app.utils.image_searcher import ImageSearcher
from app.utils.query_wiki import WikiSearcher
from app.utils.ner import Ner
from app.utils.graph_utils import convert_graph_to_triples, search_node_item, get_entity_details
from app.utils.context_manager import context_manager

model = None
tokenizer = None
init_history = None

ner = Ner()
image_searcher = ImageSearcher()
wiki_searcher = WikiSearcher()
cc = OpenCC('t2s')

def predict(user_input, history=None):
    global model, tokenizer, init_history
    if not history:
        history = init_history
    return model.chat(tokenizer, user_input, history)


def stream_predict(user_input, history=None):
    global model, tokenizer, init_history
    if not history:
        history = init_history

    ref = ""

    # è·å–å®ä½“
    graph = {}
    entities = []
    entities = ner.get_entities(user_input, etypes=["ç‰©ä½“ç±»", "äººç‰©ç±»", "åœ°ç‚¹ç±»", "ç»„ç»‡æœºæ„ç±»", "äº‹ä»¶ç±»", "ä¸–ç•Œåœ°åŒºç±»", "æœ¯è¯­ç±»"])
    print("entities: ", entities)

    # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è·å–èšç„¦æœç´¢
    context_graph = context_manager.get_focused_search(entities)
    if context_graph:
        print(f"ğŸ¯ Using focused search with {len(context_graph.get('nodes', []))} nodes")

    # è·å–å®ä½“çš„ä¸‰å…ƒç»„å’ŒçŸ¥è¯†å†…å®¹
    triples = []
    knowledge_content = ""
    for entity in entities:
        entity_graph = search_node_item(entity, graph if graph else None)

        if entity_graph:
            # åˆå¹¶å›¾è°±æ•°æ®
            if not graph:
                graph = entity_graph
            else:
                # åˆå¹¶èŠ‚ç‚¹
                existing_nodes = {node['name']: node for node in graph['nodes']}
                for node in entity_graph['nodes']:
                    if node['name'] not in existing_nodes:
                        node['id'] = len(graph['nodes'])
                        graph['nodes'].append(node)

                # åˆå¹¶é“¾æ¥
                for link in entity_graph['links']:
                    if link not in graph['links']:
                        graph['links'].append(link)

                # åˆå¹¶å¥å­
                for sent in entity_graph['sents']:
                    if sent not in graph['sents']:
                        graph['sents'].append(sent)

            # æå–ä¸‰å…ƒç»„
            entity_triples = convert_graph_to_triples(entity_graph, entity)
            triples.extend(entity_triples)

            # æå–çŸ¥è¯†å†…å®¹
            from app.utils.graph_utils import extract_knowledge_content
            entity_knowledge = extract_knowledge_content(entity_graph, entity)
            if entity_knowledge:
                knowledge_content += f"\n\nã€{entity}ç›¸å…³çŸ¥è¯†ã€‘\n{entity_knowledge}"

    # ä½¿ç”¨SimpleChatGLMçš„è‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•
    if chat_glm and hasattr(chat_glm, '_extract_meaningful_info'):
        # æ„å»ºåŸå§‹å‚è€ƒå†…å®¹ç”¨äºå¤„ç†
        triples_str = ""
        for t in triples[:15]:  # é™åˆ¶ä¸‰å…ƒç»„æ•°é‡
            triples_str += f"({t[0]} {t[1]} {t[2]})ï¼›"

        raw_ref = ""
        if triples_str:
            raw_ref += f"ä¸‰å…ƒç»„ä¿¡æ¯ï¼š{triples_str}ï¼›"
        if knowledge_content:
            raw_ref += knowledge_content

        # ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†è½¬æ¢
        meaningful_info = chat_glm._extract_meaningful_info(raw_ref, user_input)
        if meaningful_info:
            ref += meaningful_info
        elif knowledge_content:
            ref += knowledge_content
    else:
        # å›é€€åˆ°åŸæœ‰æ–¹å¼
        triples_str = ""
        for t in triples[:15]:  # é™åˆ¶ä¸‰å…ƒç»„æ•°é‡
            triples_str += f"({t[0]} {t[1]} {t[2]})ï¼›"

        if triples_str:
            ref += f"ä¸‰å…ƒç»„ä¿¡æ¯ï¼š{triples_str}ï¼›"

        if knowledge_content:
            ref += knowledge_content


    image = image_searcher.search(user_input)

    for ent in entities + [user_input]:
        wiki = wiki_searcher.search(ent)
        if wiki is not None:
            break

    # å°†Wikipediaæœç´¢åˆ°çš„ç¹ä½“è½¬ä¸ºç®€ä½“
    if wiki:
        ref += cc.convert(wiki.summary)
        wiki = {
            "title": cc.convert(wiki.title),
            "summary": cc.convert(wiki.summary),
        }
        print(wiki)
    else:
        wiki = {
            "title": "æ— ç›¸å…³ä¿¡æ¯",
            "summary": "æš‚æ— ç›¸å…³æè¿°",
        }

    if model is not None:
        if ref:
            chat_input = f"\n===å‚è€ƒèµ„æ–™===ï¼š\n{ref}ï¼›\n\næ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š\n{user_input}"
        else:
            chat_input = user_input

        clean_history = []
        for user_msg, response in history:
            # æ¸…ç†ç”¨æˆ·è¾“å…¥ï¼Œç§»é™¤å‚è€ƒèµ„æ–™éƒ¨åˆ†
            if "===å‚è€ƒèµ„æ–™===" in user_msg:
                clean_user_input = user_msg.split("===å‚è€ƒèµ„æ–™===")[0].strip()
                if "æ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š" in user_msg:
                    clean_user_input = user_msg.split("æ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š")[1].strip()
            else:
                clean_user_input = user_msg
            clean_history.append((clean_user_input, response))

        print("chat_input: ", chat_input)

        # ä½¿ç”¨æ–°çš„chat_glmå®ä¾‹æˆ–åŸæœ‰æ–¹å¼
        if chat_glm and chat_glm.loaded:
            for response, raw_history in chat_glm.stream_chat(chat_input, clean_history):
                # æ¸…ç†è¿”å›çš„å†å²è®°å½•ï¼Œç¡®ä¿ç”¨æˆ·çœ‹åˆ°çš„æ˜¯åŸå§‹é—®é¢˜è€Œä¸æ˜¯å¸¦å‚è€ƒèµ„æ–™çš„query
                clean_return_history = []
                for h_q, h_r in raw_history:
                    if "===å‚è€ƒèµ„æ–™===" in h_q:
                        if "æ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š" in h_q:
                            clean_q = h_q.split("æ ¹æ®ä¸Šé¢èµ„æ–™ï¼Œç”¨ç®€æ´ä¸”å‡†ç¡®çš„è¯å›ç­”ä¸‹é¢é—®é¢˜ï¼š")[1].strip()
                        else:
                            clean_q = h_q.split("===å‚è€ƒèµ„æ–™===")[0].strip()
                    else:
                        clean_q = h_q
                    clean_return_history.append((clean_q, h_r))

                updates = {
                    "query": user_input,  # ä½¿ç”¨åŸå§‹ç”¨æˆ·è¾“å…¥ï¼Œè€Œä¸æ˜¯chat_input
                    "response": response
                }

                # æ›´æ–°ä¸Šä¸‹æ–‡
                context_manager.update_context(user_input, entities, graph)

                # è·å–å®ä½“è¯¦ç»†ä¿¡æ¯
                entity_details = []
                for entity in entities[:3]:  # é™åˆ¶å®ä½“æ•°é‡
                    details = get_entity_details(entity, graph)
                    if details:
                        entity_details.append(details)

                # è·å–å»ºè®®é—®é¢˜
                suggestions = context_manager.suggest_related_questions(entities)

                # è·å–å¯¹è¯æ‘˜è¦
                conversation_summary = context_manager.get_conversation_summary()

                result = {
                    "history": clean_return_history,
                    "updates": updates,
                    "image": image,
                    "graph": graph,
                    "wiki": wiki,
                    "entity_details": entity_details,
                    "suggestions": suggestions,
                    "conversation_summary": conversation_summary
                }
                yield json.dumps(result, ensure_ascii=False).encode('utf8') + b'\n'
        elif model is not None:
            for response, history in model.stream_chat(tokenizer, chat_input, clean_history):
                updates = {}
                for query, response in history:
                    updates["query"] = query
                    updates["response"] = response

                # æ›´æ–°ä¸Šä¸‹æ–‡
                context_manager.update_context(user_input, entities, graph)

                # è·å–å®ä½“è¯¦ç»†ä¿¡æ¯å’Œå»ºè®®
                entity_details = []
                for entity in entities[:3]:
                    details = get_entity_details(entity, graph)
                    if details:
                        entity_details.append(details)

                suggestions = context_manager.suggest_related_questions(entities)
                conversation_summary = context_manager.get_conversation_summary()

                result = {
                    "history": history,
                    "updates": updates,
                    "image": image,
                    "graph": graph,
                    "wiki": wiki,
                    "entity_details": entity_details,
                    "suggestions": suggestions,
                    "conversation_summary": conversation_summary
                }
                yield json.dumps(result, ensure_ascii=False).encode('utf8') + b'\n'
        else:
            # ç®€å•å›å¤æ¨¡å¼
            updates = {
                "query": user_input,
                "response": "æ¨¡å‹åŠ è½½ä¸­ï¼Œè¯·ç¨åå†è¯•"
            }

            # æ›´æ–°ä¸Šä¸‹æ–‡
            context_manager.update_context(user_input, entities, graph)

            # è·å–å®ä½“è¯¦ç»†ä¿¡æ¯å’Œå»ºè®®
            entity_details = []
            for entity in entities[:3]:
                print(f"ğŸ” Getting details for entity: {entity}")
                details = get_entity_details(entity, graph)
                if details:
                    entity_details.append(details)
                    print(f"ğŸ“‹ Entity details for {entity}: {details['total_connections']} connections")
                else:
                    print(f"âŒ No details found for entity: {entity}")

            suggestions = context_manager.suggest_related_questions(entities)
            conversation_summary = context_manager.get_conversation_summary()

            print(f"ğŸ”¢ Generated {len(entity_details)} entity details, {len(suggestions)} suggestions")

            result = {
                "history": history,
                "updates": updates,
                "image": image,
                "graph": graph,
                "wiki": wiki,
                "entity_details": entity_details,
                "suggestions": suggestions,
                "conversation_summary": conversation_summary
            }
            yield json.dumps(result, ensure_ascii=False).encode('utf8') + b'\n'

    else:
        updates = {
            "query": user_input,
            "response": "æ¨¡å‹åŠ è½½ä¸­ï¼Œè¯·ç¨åå†è¯•"
        }

        # æ›´æ–°ä¸Šä¸‹æ–‡
        context_manager.update_context(user_input, entities, graph)

        # è·å–å®ä½“è¯¦ç»†ä¿¡æ¯å’Œå»ºè®®
        entity_details = []
        for entity in entities[:3]:
            print(f"ğŸ” Getting details for entity: {entity}")
            details = get_entity_details(entity, graph)
            if details:
                entity_details.append(details)
                print(f"ğŸ“‹ Entity details for {entity}: {details['total_connections']} connections")
            else:
                print(f"âŒ No details found for entity: {entity}")

        suggestions = context_manager.suggest_related_questions(entities)
        conversation_summary = context_manager.get_conversation_summary()

        print(f"ğŸ”¢ Generated {len(entity_details)} entity details, {len(suggestions)} suggestions")

        result = {
            "history": history,
            "updates": updates,
            "image": image,
            "graph": graph,
            "wiki": wiki,
            "entity_details": entity_details,
            "suggestions": suggestions,
            "conversation_summary": conversation_summary
        }
        yield json.dumps(result, ensure_ascii=False).encode('utf8') + b'\n'

# å…¨å±€ChatGLMå®ä¾‹
chat_glm = None

# åŠ è½½æ¨¡å‹
def start_model():
    global model, tokenizer, init_history, chat_glm

    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    model_path = "/fast/zwj/ChatGLM-6B/weights"

    if os.path.exists(model_path) and os.listdir(model_path):
        # ä½¿ç”¨æ–°çš„ç®€å•åŠ è½½å™¨
        from app.utils.simple_chat import SimpleChatGLM
        chat_glm = SimpleChatGLM(model_path)

        if chat_glm.load_model():
            # å…¼å®¹åŸæœ‰æ¥å£
            model = chat_glm.model
            tokenizer = chat_glm.tokenizer
            init_history = []
            print("âœ… ChatGLM-6B ready for chat!")
        else:
            print("âŒ Failed to load ChatGLM-6B")
            model = None
            tokenizer = None
            init_history = []
            chat_glm = None
    else:
        print("Warning: ChatGLM-6B model weights not found at /fast/zwj/ChatGLM-6B/weights")
        print("Using simple response mode. To enable full chat functionality:")
        print("1. Run: python3 download_model.py")
        print("2. Or download model manually to: /fast/zwj/ChatGLM-6B/weights")

        model = None
        tokenizer = None
        init_history = []
        chat_glm = None