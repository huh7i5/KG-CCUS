import os
import sys
sys.path.append('server/app')
import json
from opencc import OpenCC
from transformers import AutoTokenizer, AutoModel
from app.utils.image_searcher import ImageSearcher
from app.utils.query_wiki import WikiSearcher
from app.utils.ner import Ner
from app.utils.graph_utils import convert_graph_to_triples, search_node_item, get_entity_details
from app.utils.context_manager import context_manager

model = None
tokenizer = None
init_history = None

ner = Ner()
image_searcher = ImageSearcher()
wiki_searcher = WikiSearcher()
cc = OpenCC('t2s')

class KnowledgeGraphQA:
    """CCUSçŸ¥è¯†å›¾è°±æ™ºèƒ½é—®ç­”ç³»ç»Ÿ - æŒ‰ç…§æµç¨‹å›¾å®ç°"""

    def __init__(self):
        self.ner = Ner()
        self.image_searcher = ImageSearcher()
        self.wiki_searcher = WikiSearcher()
        self.cc = OpenCC('t2s')

    def named_entity_recognition(self, user_input):
        """æ­¥éª¤1: å‘½åå®ä½“è¯†åˆ«æ¨¡å‹"""
        entities = self.ner.get_entities(
            user_input,
            etypes=["ç‰©ä½“ç±»", "äººç‰©ç±»", "åœ°ç‚¹ç±»", "ç»„ç»‡æœºæ„ç±»", "äº‹ä»¶ç±»", "ä¸–ç•Œåœ°åŒºç±»", "æœ¯è¯­ç±»"]
        )
        return entities

    def graph_search(self, entities):
        """æ­¥éª¤2: å›¾è°±æ£€ç´¢ - åœ¨é¢†åŸŸçŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢ç›¸å…³å®ä½“"""
        graph_data = {}
        subgraph_data = {}
        triples = []

        for entity in entities:
            entity_graph = search_node_item(entity, graph_data if graph_data else None)

            if entity_graph and entity_graph.get('nodes'):
                # åˆå¹¶å›¾è°±æ•°æ®
                if not graph_data:
                    graph_data = entity_graph
                else:
                    # åˆå¹¶èŠ‚ç‚¹
                    existing_nodes = {node['name']: node for node in graph_data['nodes']}
                    for node in entity_graph['nodes']:
                        if node['name'] not in existing_nodes:
                            node['id'] = len(graph_data['nodes'])
                            graph_data['nodes'].append(node)

                    # åˆå¹¶é“¾æ¥
                    for link in entity_graph['links']:
                        if link not in graph_data['links']:
                            graph_data['links'].append(link)

                    # åˆå¹¶å¥å­
                    for sent in entity_graph['sents']:
                        if sent not in graph_data['sents']:
                            graph_data['sents'].append(sent)

                # æå–ä¸‰å…ƒç»„
                entity_triples = convert_graph_to_triples(entity_graph, entity)
                triples.extend(entity_triples)

                # æ„å»ºå­å›¾è°±
                subgraph_data[entity] = {
                    'graph': entity_graph,
                    'triples': entity_triples
                }

        return {
            'full_graph': graph_data,
            'subgraphs': subgraph_data,
            'triples': triples
        }

    def external_knowledge_search(self, entities, user_input):
        """æ­¥éª¤3: å¤–éƒ¨çŸ¥è¯†æ£€ç´¢ - ä»Wikipediaç­‰å¤–éƒ¨æ•°æ®åº“æ£€ç´¢"""
        external_knowledge = {}

        # å›¾ç‰‡æœç´¢
        image_result = self.image_searcher.search(user_input)
        if image_result:
            external_knowledge['image'] = image_result

        # Wikipediaæœç´¢
        wiki_result = None
        for entity in entities + [user_input]:
            wiki = self.wiki_searcher.search(entity)
            if wiki is not None:
                wiki_result = {
                    "title": self.cc.convert(wiki.title),
                    "summary": self.cc.convert(wiki.summary),
                }
                break

        if not wiki_result:
            wiki_result = {
                "title": "æ— ç›¸å…³ä¿¡æ¯",
                "summary": "æš‚æ— ç›¸å…³æè¿°",
            }

        external_knowledge['wiki'] = wiki_result
        return external_knowledge

    def structured_processing(self, graph_results, external_knowledge, entities):
        """æ­¥éª¤4: ç»“æ„åŒ–å¤„ç† - æ•´åˆå¤šæºä¿¡æ¯"""
        structured_info = {
            'entities': entities,
            'relations': [],
            'knowledge_text': "",
            'context_info': {}
        }

        # å¤„ç†å›¾è°±ä¸‰å…ƒç»„ä¿¡æ¯
        if graph_results['triples']:
            relation_groups = {}
            for triple in graph_results['triples'][:10]:  # é™åˆ¶æ•°é‡
                if len(triple) >= 3:
                    relation = triple[1]
                    if relation not in relation_groups:
                        relation_groups[relation] = []
                    relation_groups[relation].append((triple[0], triple[2]))

            # è½¬æ¢ä¸ºç»“æ„åŒ–å…³ç³»æè¿°
            for relation, pairs in relation_groups.items():
                for subject, obj in pairs[:3]:  # æ¯ç§å…³ç³»æœ€å¤š3ä¸ªä¾‹å­
                    structured_info['relations'].append({
                        'subject': subject,
                        'predicate': relation,
                        'object': obj,
                        'description': f"{subject}ä¸{obj}çš„å…³ç³»æ˜¯{relation}"
                    })

        # å¤„ç†å›¾è°±çŸ¥è¯†å†…å®¹
        if graph_results.get('subgraphs'):
            knowledge_parts = []
            for entity, subgraph in graph_results['subgraphs'].items():
                from app.utils.graph_utils import extract_knowledge_content
                entity_knowledge = extract_knowledge_content(subgraph['graph'], entity)
                if entity_knowledge:
                    knowledge_parts.append(f"{entity}: {entity_knowledge}")

            if knowledge_parts:
                structured_info['knowledge_text'] = "; ".join(knowledge_parts)

        # æ•´åˆå¤–éƒ¨çŸ¥è¯†
        if external_knowledge.get('wiki') and external_knowledge['wiki']['summary'] != "æš‚æ— ç›¸å…³æè¿°":
            wiki_summary = external_knowledge['wiki']['summary'][:200]  # é™åˆ¶é•¿åº¦
            structured_info['context_info']['wikipedia'] = wiki_summary

        return structured_info

    def build_prompt(self, user_input, structured_info):
        """æ­¥éª¤5: æ„å»ºprompt - ä¸ºå¯¹è¯è¯­è¨€æ¨¡å‹æ„å»ºä¸Šä¸‹æ–‡"""
        prompt_parts = []

        # æ·»åŠ å®ä½“ä¿¡æ¯
        if structured_info['entities']:
            entities_str = "ã€".join(structured_info['entities'][:5])
            prompt_parts.append(f"ç›¸å…³å®ä½“: {entities_str}")

        # æ·»åŠ å…³ç³»ä¿¡æ¯
        if structured_info['relations']:
            relations_str = "; ".join([rel['description'] for rel in structured_info['relations'][:5]])
            prompt_parts.append(f"çŸ¥è¯†å…³ç³»: {relations_str}")

        # æ·»åŠ çŸ¥è¯†å†…å®¹
        if structured_info['knowledge_text']:
            knowledge_str = structured_info['knowledge_text'][:300]  # é™åˆ¶é•¿åº¦
            prompt_parts.append(f"èƒŒæ™¯çŸ¥è¯†: {knowledge_str}")

        # æ·»åŠ å¤–éƒ¨ä¸Šä¸‹æ–‡
        if structured_info['context_info'].get('wikipedia'):
            wiki_str = structured_info['context_info']['wikipedia']
            prompt_parts.append(f"å‚è€ƒä¿¡æ¯: {wiki_str}")

        if prompt_parts:
            context = "\n".join(prompt_parts)
            prompt = f"""åŸºäºä»¥ä¸‹çŸ¥è¯†ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜:

{context}

ç”¨æˆ·é—®é¢˜: {user_input}

è¯·åŸºäºä¸Šè¿°ä¿¡æ¯æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·åŸºäºCCUSé¢†åŸŸå¸¸è¯†è¿›è¡Œåˆç†å›ç­”ã€‚"""
        else:
            prompt = user_input

        return prompt

    def generate_response(self, prompt, history, chat_glm_model=None):
        """æ­¥éª¤6: å¯¹è¯è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­” - ä½¿ç”¨SimpleChatGLM"""
        global chat_glm

        print("ğŸ¤– [CHATGLM] === å¼€å§‹ç”Ÿæˆå›ç­” ===")
        print(f"ğŸ¤– [CHATGLM] æ˜¯å¦æœ‰SimpleChatGLM: {chat_glm is not None}")

        # ä½¿ç”¨SimpleChatGLMå®ä¾‹
        if chat_glm is not None and chat_glm.loaded:
            print("ğŸ¤– [CHATGLM] ä½¿ç”¨SimpleChatGLMè°ƒç”¨æ–¹å¼")
            print(f"ğŸ¤– [CHATGLM] Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
            print(f"ğŸ¤– [CHATGLM] Prompté¢„è§ˆ: {prompt[:200]}...")
            print(f"ğŸ¤– [CHATGLM] Historyé•¿åº¦: {len(history)}")

            try:
                # å°†çŸ¥è¯†å›¾è°±æ ¼å¼çš„promptè½¬æ¢ä¸ºç”¨æˆ·é—®é¢˜
                if prompt.startswith("åŸºäºä»¥ä¸‹çŸ¥è¯†ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜"):
                    # æå–ç”¨æˆ·é—®é¢˜
                    lines = prompt.split('\n')
                    user_question = ""
                    for line in lines:
                        if line.startswith("ç”¨æˆ·é—®é¢˜:"):
                            user_question = line.replace("ç”¨æˆ·é—®é¢˜:", "").strip()
                            break

                    if user_question:
                        chat_input = user_question
                    else:
                        chat_input = prompt
                else:
                    chat_input = prompt

                print(f"ğŸ¤– [CHATGLM] è½¬æ¢åçš„chat_input: {chat_input[:200]}...")

                response_count = 0
                # ä½¿ç”¨SimpleChatGLMçš„stream_chatæ–¹æ³•
                for response, updated_history in chat_glm.stream_chat(chat_input, history):
                    response_count += 1
                    print(f"ğŸ¤– [CHATGLM] ç¬¬{response_count}ä¸ªSimpleChatGLMå“åº”:")
                    print(f"ğŸ¤– [CHATGLM] å“åº”é•¿åº¦: {len(response)}")
                    print(f"ğŸ¤– [CHATGLM] å“åº”é¢„è§ˆ: {response[:150]}...")

                    # æ£€æŸ¥æ˜¯å¦æ˜¯çœŸæ­£çš„ChatGLMæ™ºèƒ½å›ç­”
                    if response and len(response.strip()) > 20:
                        print(f"âœ… [CHATGLM] ç¡®è®¤æ”¶åˆ°ChatGLMæ™ºèƒ½å›å¤!")

                    yield response, updated_history

                if response_count == 0:
                    print("âŒ [CHATGLM] SimpleChatGLMæœªäº§ç”Ÿä»»ä½•å“åº”")

            except Exception as e:
                print(f"âŒ [CHATGLM] SimpleChatGLMè°ƒç”¨å¼‚å¸¸: {e}")
                import traceback
                traceback.print_exc()
                # é™çº§åˆ°ç®€å•æ¨¡å¼
                response = self._generate_simple_response(prompt)
                updated_history = history + [(prompt, response)]
                yield response, updated_history
        else:
            print("âš ï¸ [CHATGLM] SimpleChatGLMæœªåŠ è½½ï¼Œä½¿ç”¨ç®€å•æ¨¡å¼å›ç­”")
            # ç®€å•æ¨¡å¼å›ç­”
            response = self._generate_simple_response(prompt)
            updated_history = history + [(prompt, response)]
            yield response, updated_history

    def _clean_response(self, response):
        """æ¸…ç†æ¨¡å‹å“åº”ä¸­çš„promptå†…å®¹"""
        if not response:
            return response

        # ç§»é™¤æ˜æ˜¾çš„promptç‰‡æ®µ
        clean_patterns = [
            "åŸºäºä»¥ä¸‹çŸ¥è¯†ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜:",
            "ç”¨æˆ·é—®é¢˜:",
            "è¯·åŸºäºä¸Šè¿°ä¿¡æ¯æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”",
            "å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·åŸºäºCCUSé¢†åŸŸå¸¸è¯†è¿›è¡Œåˆç†å›ç­”"
        ]

        cleaned = response
        for pattern in clean_patterns:
            if pattern in cleaned:
                parts = cleaned.split(pattern)
                if len(parts) > 1:
                    cleaned = parts[-1].strip()

        return cleaned if len(cleaned.strip()) > 10 else response

    def _is_chatglm_response(self, response):
        """æ£€æŸ¥æ˜¯å¦æ˜¯çœŸæ­£çš„ChatGLMæ™ºèƒ½å“åº”"""
        if not response or len(response.strip()) < 10:
            return False

        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡æ¿å“åº”æŒ‡æ ‡
        template_indicators = [
            "CCUSæ˜¯ç¢³æ•é›†ã€åˆ©ç”¨ä¸å‚¨å­˜æŠ€æœ¯",
            "æ˜¯åº”å¯¹æ°”å€™å˜åŒ–çš„é‡è¦æŠ€æœ¯æ‰‹æ®µ",
            "æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯",
            "æˆ‘ä¸ºæ‚¨æä¾›ç›¸å…³çš„CCUSæŠ€æœ¯é¢†åŸŸå›ç­”"
        ]

        is_template = any(indicator in response for indicator in template_indicators)
        is_longer_than_template = len(response) > 100
        has_detailed_content = any(keyword in response for keyword in ["å…·ä½“", "è¯¦ç»†", "ä¾‹å¦‚", "åŒ…æ‹¬", "ä¸»è¦"])

        return not is_template and (is_longer_than_template or has_detailed_content)

    def _generate_simple_response(self, prompt):
        """ç”Ÿæˆç®€å•å›ç­”ï¼ˆå½“æ²¡æœ‰ChatGLMæ¨¡å‹æ—¶ï¼‰"""
        if "ccus" in prompt.lower() or "ç¢³æ•é›†" in prompt or "äºŒæ°§åŒ–ç¢³" in prompt:
            return "CCUSæ˜¯ç¢³æ•é›†ã€åˆ©ç”¨ä¸å‚¨å­˜æŠ€æœ¯ï¼Œæ˜¯åº”å¯¹æ°”å€™å˜åŒ–çš„é‡è¦æŠ€æœ¯æ‰‹æ®µã€‚è¯¥æŠ€æœ¯é€šè¿‡æ•é›†å·¥ä¸šæ’æ”¾çš„äºŒæ°§åŒ–ç¢³ï¼Œè¿›è¡Œèµ„æºåŒ–åˆ©ç”¨æˆ–å®‰å…¨å‚¨å­˜ã€‚"
        else:
            return "æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œæˆ‘ä¸ºæ‚¨æä¾›ç›¸å…³çš„CCUSæŠ€æœ¯é¢†åŸŸå›ç­”ã€‚"

# å…¨å±€å®ä¾‹
kg_qa_system = KnowledgeGraphQA()
chat_glm = None

def predict(user_input, history=None):
    """å…¼å®¹åŸæœ‰æ¥å£"""
    global model, tokenizer, init_history
    if not history:
        history = init_history
    return model.chat(tokenizer, user_input, history)

def stream_predict(user_input, history=None):
    """ä¸»è¦çš„æµå¼é¢„æµ‹å‡½æ•° - æŒ‰ç…§æµç¨‹å›¾å®ç°"""
    global model, tokenizer, init_history, chat_glm

    print("ğŸš€ [STREAM_PREDICT] === å¼€å§‹æµå¼é¢„æµ‹ ===")
    print(f"ğŸš€ [STREAM_PREDICT] ç”¨æˆ·è¾“å…¥: {user_input}")
    print(f"ğŸš€ [STREAM_PREDICT] å…¨å±€ChatGLMå®ä¾‹: {chat_glm is not None}")

    if not history:
        history = init_history or []

    print(f"ğŸš€ [STREAM_PREDICT] å†å²è®°å½•é•¿åº¦: {len(history)}")

    # æ­¥éª¤1: å‘½åå®ä½“è¯†åˆ«
    print("ğŸ“ [STREAM_PREDICT] æ­¥éª¤1: å‘½åå®ä½“è¯†åˆ«")
    entities = kg_qa_system.named_entity_recognition(user_input)
    print(f"ğŸ“ [STREAM_PREDICT] è¯†åˆ«å®ä½“: {entities}")

    # æ­¥éª¤2: å›¾è°±æ£€ç´¢
    print("ğŸ” [STREAM_PREDICT] æ­¥éª¤2: å›¾è°±æ£€ç´¢")
    graph_results = kg_qa_system.graph_search(entities)
    print(f"ğŸ” [STREAM_PREDICT] å›¾è°±ç»“æœ: èŠ‚ç‚¹æ•°={len(graph_results['full_graph'].get('nodes', []))} ä¸‰å…ƒç»„æ•°={len(graph_results['triples'])}")

    # æ­¥éª¤3: å¤–éƒ¨çŸ¥è¯†æ£€ç´¢
    print("ğŸŒ [STREAM_PREDICT] æ­¥éª¤3: å¤–éƒ¨çŸ¥è¯†æ£€ç´¢")
    external_knowledge = kg_qa_system.external_knowledge_search(entities, user_input)
    print(f"ğŸŒ [STREAM_PREDICT] Wikiæ ‡é¢˜: {external_knowledge['wiki']['title']}")

    # æ­¥éª¤4: ç»“æ„åŒ–å¤„ç†
    print("ğŸ”§ [STREAM_PREDICT] æ­¥éª¤4: ç»“æ„åŒ–å¤„ç†")
    structured_info = kg_qa_system.structured_processing(graph_results, external_knowledge, entities)
    print(f"ğŸ”§ [STREAM_PREDICT] å…³ç³»æ•°: {len(structured_info['relations'])} çŸ¥è¯†æ–‡æœ¬é•¿åº¦: {len(structured_info['knowledge_text'])}")

    # æ­¥éª¤5: æ„å»ºprompt
    print("ğŸ“‹ [STREAM_PREDICT] æ­¥éª¤5: æ„å»ºprompt")
    prompt = kg_qa_system.build_prompt(user_input, structured_info)
    print(f"ğŸ“‹ [STREAM_PREDICT] Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")

    # æ›´æ–°ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    context_manager.update_context(user_input, entities, graph_results['full_graph'])

    # æ­¥éª¤6: å¯¹è¯è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”
    print("ğŸ¤– [STREAM_PREDICT] æ­¥éª¤6: è°ƒç”¨ChatGLMç”Ÿæˆå›ç­”")
    response_count = 0
    for response, updated_history in kg_qa_system.generate_response(prompt, history, None):
        response_count += 1
        print(f"ğŸ“¤ [STREAM_PREDICT] ç”Ÿæˆç¬¬{response_count}ä¸ªå“åº”")

        # æ„å»ºè¿”å›ç»“æœ
        result = {
            "history": updated_history,
            "updates": {
                "query": user_input,
                "response": response
            },
            "image": external_knowledge.get('image'),
            "graph": graph_results['full_graph'] if graph_results['full_graph'] and len(graph_results['full_graph'].get('nodes', [])) <= 50 else None,
            "wiki": external_knowledge['wiki']
        }

        json_result = json.dumps(result, ensure_ascii=False).encode('utf8') + b'\n'
        print(f"ğŸ“¤ [STREAM_PREDICT] è¿”å›ç»“æœå¤§å°: {len(json_result)} bytes")
        yield json_result

    print(f"âœ… [STREAM_PREDICT] æµå¼é¢„æµ‹å®Œæˆï¼Œæ€»å…±ç”Ÿæˆ{response_count}ä¸ªå“åº”")

def start_model():
    """åŠ è½½æ¨¡å‹ - ä½¿ç”¨SimpleChatGLMå®ç°"""
    global model, tokenizer, init_history, chat_glm

    print("ğŸš€ [START_MODEL] === å¼€å§‹åŠ è½½ChatGLMæ¨¡å‹ï¼ˆä½¿ç”¨SimpleChatGLMï¼‰===")

    try:
        from app.utils.simple_chat import SimpleChatGLM

        model_path = "/fast/zwj/ChatGLM-6B/weights"
        print(f"ğŸ“ [START_MODEL] æ¨¡å‹è·¯å¾„: {model_path}")

        # åˆ›å»ºSimpleChatGLMå®ä¾‹
        print("ğŸ”„ [START_MODEL] åˆ›å»ºSimpleChatGLMå®ä¾‹...")
        chat_glm = SimpleChatGLM(model_path, memory_optimize=True)

        # åŠ è½½æ¨¡å‹
        print("ğŸ”„ [START_MODEL] åŠ è½½ChatGLMæ¨¡å‹...")
        if chat_glm.load_model():
            print("âœ… [START_MODEL] SimpleChatGLMåŠ è½½æˆåŠŸ!")

            # è·å–å†…éƒ¨æ¨¡å‹å’Œåˆ†è¯å™¨ç”¨äºå…¼å®¹æ€§
            model = chat_glm.model
            tokenizer = chat_glm.tokenizer

            # åˆå§‹åŒ–å†å²è®°å½•
            print("ğŸ”„ [START_MODEL] åˆå§‹åŒ–å†å²è®°å½•...")
            pre_prompt = "ä½ å« ChatKGï¼Œæ˜¯ä¸€ä¸ªå›¾è°±é—®ç­”æœºå™¨äººï¼Œæ­¤ä¸ºèƒŒæ™¯ã€‚ä¸‹é¢å¼€å§‹èŠå¤©å§ï¼"
            try:
                # ä½¿ç”¨SimpleChatGLMçš„stream_chatæ¥åˆå§‹åŒ–
                for response, history in chat_glm.stream_chat(pre_prompt, []):
                    init_history = history
                    break
                print("âœ… [START_MODEL] å†å²è®°å½•åˆå§‹åŒ–å®Œæˆ!")
            except Exception as e:
                print(f"âš ï¸ [START_MODEL] å†å²è®°å½•åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨ç©ºå†å²: {e}")
                init_history = []

            print(f"ğŸ¯ [START_MODEL] ChatGLM-6BåŠ è½½å®Œæˆ!")
            print(f"ğŸ¯ [START_MODEL] æ¨¡å‹ç±»å‹: {type(model)}")
            print(f"ğŸ¯ [START_MODEL] åˆ†è¯å™¨ç±»å‹: {type(tokenizer)}")
            print(f"ğŸ¯ [START_MODEL] åˆå§‹å†å²é•¿åº¦: {len(init_history)}")

        else:
            print("âŒ [START_MODEL] SimpleChatGLMåŠ è½½å¤±è´¥")
            model = None
            tokenizer = None
            init_history = []
            chat_glm = None

    except Exception as e:
        print(f"âŒ [START_MODEL] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        model = None
        tokenizer = None
        init_history = []
        chat_glm = None

    print("ğŸ¯ [START_MODEL] æ¨¡å‹åŠ è½½æµç¨‹å®Œæˆ!")