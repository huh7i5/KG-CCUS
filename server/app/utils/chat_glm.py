import os
import sys
sys.path.append('server/app')
import json
from opencc import OpenCC
from transformers import AutoTokenizer, AutoModel
from app.utils.image_searcher import ImageSearcher
from app.utils.query_wiki import WikiSearcher
from app.utils.ner import Ner
from app.utils.graph_utils import convert_graph_to_triples, search_node_item, get_entity_details
from app.utils.context_manager import context_manager

model = None
tokenizer = None
init_history = None

ner = Ner()
image_searcher = ImageSearcher()
wiki_searcher = WikiSearcher()
cc = OpenCC('t2s')

class KnowledgeGraphQA:
    """CCUSçŸ¥è¯†å›¾è°±æ™ºèƒ½é—®ç­”ç³»ç»Ÿ - æŒ‰ç…§æµç¨‹å›¾å®ç°"""

    def __init__(self):
        self.ner = Ner()
        self.image_searcher = ImageSearcher()
        self.wiki_searcher = WikiSearcher()
        self.cc = OpenCC('t2s')

    def named_entity_recognition(self, user_input):
        """æ­¥éª¤1: å‘½åå®ä½“è¯†åˆ«æ¨¡å‹"""
        entities = self.ner.get_entities(
            user_input,
            etypes=["ç‰©ä½“ç±»", "äººç‰©ç±»", "åœ°ç‚¹ç±»", "ç»„ç»‡æœºæ„ç±»", "äº‹ä»¶ç±»", "ä¸–ç•Œåœ°åŒºç±»", "æœ¯è¯­ç±»"]
        )
        return entities

    def graph_search(self, entities):
        """æ­¥éª¤2: å›¾è°±æ£€ç´¢ - åœ¨é¢†åŸŸçŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢ç›¸å…³å®ä½“"""
        graph_data = {}
        subgraph_data = {}
        triples = []

        for entity in entities:
            entity_graph = search_node_item(entity, graph_data if graph_data else None)

            if entity_graph and entity_graph.get('nodes'):
                # åˆå¹¶å›¾è°±æ•°æ®
                if not graph_data:
                    graph_data = entity_graph
                else:
                    # åˆå¹¶èŠ‚ç‚¹
                    existing_nodes = {node['name']: node for node in graph_data['nodes']}
                    for node in entity_graph['nodes']:
                        if node['name'] not in existing_nodes:
                            node['id'] = len(graph_data['nodes'])
                            graph_data['nodes'].append(node)

                    # åˆå¹¶é“¾æ¥
                    for link in entity_graph['links']:
                        if link not in graph_data['links']:
                            graph_data['links'].append(link)

                    # åˆå¹¶å¥å­
                    for sent in entity_graph['sents']:
                        if sent not in graph_data['sents']:
                            graph_data['sents'].append(sent)

                # æå–ä¸‰å…ƒç»„
                entity_triples = convert_graph_to_triples(entity_graph, entity)
                triples.extend(entity_triples)

                # æ„å»ºå­å›¾è°±
                subgraph_data[entity] = {
                    'graph': entity_graph,
                    'triples': entity_triples
                }

        return {
            'full_graph': graph_data,
            'subgraphs': subgraph_data,
            'triples': triples
        }

    def external_knowledge_search(self, entities, user_input):
        """æ­¥éª¤3: å¤–éƒ¨çŸ¥è¯†æ£€ç´¢ - ä»Wikipediaç­‰å¤–éƒ¨æ•°æ®åº“æ£€ç´¢"""
        external_knowledge = {}

        # å›¾ç‰‡æœç´¢
        image_result = self.image_searcher.search(user_input)
        if image_result:
            external_knowledge['image'] = image_result

        # Wikipediaæœç´¢
        wiki_result = None
        for entity in entities + [user_input]:
            wiki = self.wiki_searcher.search(entity)
            if wiki is not None:
                wiki_result = {
                    "title": self.cc.convert(wiki.title),
                    "summary": self.cc.convert(wiki.summary),
                }
                break

        if not wiki_result:
            wiki_result = {
                "title": "æ— ç›¸å…³ä¿¡æ¯",
                "summary": "æš‚æ— ç›¸å…³æè¿°",
            }

        external_knowledge['wiki'] = wiki_result
        return external_knowledge

    def structured_processing(self, graph_results, external_knowledge, entities):
        """æ­¥éª¤4: ç»“æ„åŒ–å¤„ç† - æ•´åˆå¤šæºä¿¡æ¯"""
        structured_info = {
            'entities': entities,
            'relations': [],
            'knowledge_text': "",
            'context_info': {}
        }

        # å¤„ç†å›¾è°±ä¸‰å…ƒç»„ä¿¡æ¯
        if graph_results['triples']:
            relation_groups = {}
            for triple in graph_results['triples'][:10]:  # é™åˆ¶æ•°é‡
                if len(triple) >= 3:
                    relation = triple[1]
                    if relation not in relation_groups:
                        relation_groups[relation] = []
                    relation_groups[relation].append((triple[0], triple[2]))

            # è½¬æ¢ä¸ºç»“æ„åŒ–å…³ç³»æè¿°
            for relation, pairs in relation_groups.items():
                for subject, obj in pairs[:3]:  # æ¯ç§å…³ç³»æœ€å¤š3ä¸ªä¾‹å­
                    structured_info['relations'].append({
                        'subject': subject,
                        'predicate': relation,
                        'object': obj,
                        'description': f"{subject}ä¸{obj}çš„å…³ç³»æ˜¯{relation}"
                    })

        # å¤„ç†å›¾è°±çŸ¥è¯†å†…å®¹
        if graph_results.get('subgraphs'):
            knowledge_parts = []
            for entity, subgraph in graph_results['subgraphs'].items():
                from app.utils.graph_utils import extract_knowledge_content
                entity_knowledge = extract_knowledge_content(subgraph['graph'], entity)
                if entity_knowledge:
                    knowledge_parts.append(f"{entity}: {entity_knowledge}")

            if knowledge_parts:
                structured_info['knowledge_text'] = "; ".join(knowledge_parts)

        # æ•´åˆå¤–éƒ¨çŸ¥è¯†
        if external_knowledge.get('wiki') and external_knowledge['wiki']['summary'] != "æš‚æ— ç›¸å…³æè¿°":
            wiki_summary = external_knowledge['wiki']['summary'][:200]  # é™åˆ¶é•¿åº¦
            structured_info['context_info']['wikipedia'] = wiki_summary

        return structured_info

    def build_prompt(self, user_input, structured_info):
        """æ­¥éª¤5: æ„å»ºprompt - ä¸ºå¯¹è¯è¯­è¨€æ¨¡å‹æ„å»ºä¸Šä¸‹æ–‡"""
        prompt_parts = []

        # æ·»åŠ å®ä½“ä¿¡æ¯
        if structured_info['entities']:
            entities_str = "ã€".join(structured_info['entities'][:5])
            prompt_parts.append(f"ç›¸å…³å®ä½“: {entities_str}")

        # æ·»åŠ å…³ç³»ä¿¡æ¯
        if structured_info['relations']:
            relations_str = "; ".join([rel['description'] for rel in structured_info['relations'][:5]])
            prompt_parts.append(f"çŸ¥è¯†å…³ç³»: {relations_str}")

        # æ·»åŠ çŸ¥è¯†å†…å®¹
        if structured_info['knowledge_text']:
            knowledge_str = structured_info['knowledge_text'][:300]  # é™åˆ¶é•¿åº¦
            prompt_parts.append(f"èƒŒæ™¯çŸ¥è¯†: {knowledge_str}")

        # æ·»åŠ å¤–éƒ¨ä¸Šä¸‹æ–‡
        if structured_info['context_info'].get('wikipedia'):
            wiki_str = structured_info['context_info']['wikipedia']
            prompt_parts.append(f"å‚è€ƒä¿¡æ¯: {wiki_str}")

        if prompt_parts:
            context = "\n".join(prompt_parts)
            prompt = f"""åŸºäºä»¥ä¸‹çŸ¥è¯†ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜:

{context}

ç”¨æˆ·é—®é¢˜: {user_input}

è¯·åŸºäºä¸Šè¿°ä¿¡æ¯æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·åŸºäºCCUSé¢†åŸŸå¸¸è¯†è¿›è¡Œåˆç†å›ç­”ã€‚"""
        else:
            prompt = user_input

        return prompt

    def generate_response(self, prompt, history, chat_glm_model=None):
        """æ­¥éª¤6: å¯¹è¯è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”"""
        if chat_glm_model and chat_glm_model.loaded:
            # ä½¿ç”¨ChatGLMæ¨¡å‹
            for response, updated_history in chat_glm_model.stream_chat(prompt, history):
                # æ¸…ç†å“åº”ä¸­çš„promptå†…å®¹
                cleaned_response = self._clean_response(response)
                yield cleaned_response, updated_history
        else:
            # ç®€å•æ¨¡å¼å›ç­”
            response = self._generate_simple_response(prompt)
            updated_history = history + [(prompt, response)]
            yield response, updated_history

    def _clean_response(self, response):
        """æ¸…ç†æ¨¡å‹å“åº”ä¸­çš„promptå†…å®¹"""
        if not response:
            return response

        # ç§»é™¤æ˜æ˜¾çš„promptç‰‡æ®µ
        clean_patterns = [
            "åŸºäºä»¥ä¸‹çŸ¥è¯†ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜:",
            "ç”¨æˆ·é—®é¢˜:",
            "è¯·åŸºäºä¸Šè¿°ä¿¡æ¯æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”",
            "å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·åŸºäºCCUSé¢†åŸŸå¸¸è¯†è¿›è¡Œåˆç†å›ç­”"
        ]

        cleaned = response
        for pattern in clean_patterns:
            if pattern in cleaned:
                parts = cleaned.split(pattern)
                if len(parts) > 1:
                    cleaned = parts[-1].strip()

        return cleaned if len(cleaned.strip()) > 10 else response

    def _generate_simple_response(self, prompt):
        """ç”Ÿæˆç®€å•å›ç­”ï¼ˆå½“æ²¡æœ‰ChatGLMæ¨¡å‹æ—¶ï¼‰"""
        if "ccus" in prompt.lower() or "ç¢³æ•é›†" in prompt or "äºŒæ°§åŒ–ç¢³" in prompt:
            return "CCUSæ˜¯ç¢³æ•é›†ã€åˆ©ç”¨ä¸å‚¨å­˜æŠ€æœ¯ï¼Œæ˜¯åº”å¯¹æ°”å€™å˜åŒ–çš„é‡è¦æŠ€æœ¯æ‰‹æ®µã€‚è¯¥æŠ€æœ¯é€šè¿‡æ•é›†å·¥ä¸šæ’æ”¾çš„äºŒæ°§åŒ–ç¢³ï¼Œè¿›è¡Œèµ„æºåŒ–åˆ©ç”¨æˆ–å®‰å…¨å‚¨å­˜ã€‚"
        else:
            return "æ ¹æ®çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œæˆ‘ä¸ºæ‚¨æä¾›ç›¸å…³çš„CCUSæŠ€æœ¯é¢†åŸŸå›ç­”ã€‚"

# å…¨å±€å®ä¾‹
kg_qa_system = KnowledgeGraphQA()
chat_glm = None

def predict(user_input, history=None):
    """å…¼å®¹åŸæœ‰æ¥å£"""
    global model, tokenizer, init_history
    if not history:
        history = init_history
    return model.chat(tokenizer, user_input, history)

def stream_predict(user_input, history=None):
    """ä¸»è¦çš„æµå¼é¢„æµ‹å‡½æ•° - æŒ‰ç…§æµç¨‹å›¾å®ç°"""
    global model, tokenizer, init_history, chat_glm
    if not history:
        history = init_history or []

    # æ­¥éª¤1: å‘½åå®ä½“è¯†åˆ«
    entities = kg_qa_system.named_entity_recognition(user_input)

    # æ­¥éª¤2: å›¾è°±æ£€ç´¢
    graph_results = kg_qa_system.graph_search(entities)

    # æ­¥éª¤3: å¤–éƒ¨çŸ¥è¯†æ£€ç´¢
    external_knowledge = kg_qa_system.external_knowledge_search(entities, user_input)

    # æ­¥éª¤4: ç»“æ„åŒ–å¤„ç†
    structured_info = kg_qa_system.structured_processing(graph_results, external_knowledge, entities)

    # æ­¥éª¤5: æ„å»ºprompt
    prompt = kg_qa_system.build_prompt(user_input, structured_info)

    # æ›´æ–°ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    context_manager.update_context(user_input, entities, graph_results['full_graph'])

    # æ­¥éª¤6: å¯¹è¯è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”
    for response, updated_history in kg_qa_system.generate_response(prompt, history, chat_glm):
        # æ„å»ºè¿”å›ç»“æœ
        result = {
            "history": updated_history,
            "updates": {
                "query": user_input,
                "response": response
            },
            "image": external_knowledge.get('image'),
            "graph": graph_results['full_graph'] if graph_results['full_graph'] and len(graph_results['full_graph'].get('nodes', [])) <= 50 else None,
            "wiki": external_knowledge['wiki']
        }
        yield json.dumps(result, ensure_ascii=False).encode('utf8') + b'\n'

def start_model():
    """åŠ è½½æ¨¡å‹"""
    global model, tokenizer, init_history, chat_glm

    print("ğŸš€ Starting CCUS Knowledge Graph QA System...")

    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    model_path = "/fast/zwj/ChatGLM-6B/weights"

    if os.path.exists(model_path) and os.listdir(model_path):
        try:
            from app.utils.simple_chat import SimpleChatGLM

            print(f"ğŸ“ Loading ChatGLM model from: {model_path}")
            chat_glm = SimpleChatGLM(model_path, memory_optimize=True)

            if chat_glm.load_model():
                model = chat_glm.model
                tokenizer = chat_glm.tokenizer
                init_history = []
                print("âœ… ChatGLM-6B loaded successfully!")
            else:
                print("âŒ Failed to load ChatGLM-6B")
                model = None
                tokenizer = None
                init_history = []
                chat_glm = None
        except Exception as e:
            print(f"âŒ Error loading ChatGLM: {e}")
            model = None
            tokenizer = None
            init_history = []
            chat_glm = None
    else:
        print("âš ï¸  ChatGLM model not found, using simple response mode")
        model = None
        tokenizer = None
        init_history = []
        chat_glm = None

    print("ğŸ¯ CCUS Knowledge Graph QA System ready!")